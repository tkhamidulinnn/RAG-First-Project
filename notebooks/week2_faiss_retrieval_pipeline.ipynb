{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# Week 2: Building the Local Retrieval Pipeline\n",
        "\n",
        "**Scope:** local retrieval pipeline only: load PDFs -> chunk -> embed -> FAISS index -> retrieve.\n",
        "\n",
        "## Engineering Decision (Important)\n",
        "Week 1 selected **MPNet** as the semantic-quality winner. In this environment, `sentence-transformers/torch` causes a **native kernel crash** during indexing (process dies, no Python traceback).\n",
        "\n",
        "To keep Week 2 reproducible and runnable, this notebook uses a stable embedder:\n",
        "- `HashingVectorizer` (`hashing-768-stable`) for embeddings\n",
        "- FAISS `IndexFlatIP` for retrieval scoring\n",
        "\n",
        "This is a reliability-driven fallback for Week 2 execution, not a claim that hashing is better than MPNet.\n",
        "\n",
        "**Week 2 objectives covered:**\n",
        "1. Local vector store (FAISS).\n",
        "2. Stable chunking for retrieval.\n",
        "3. Retriever + manual quality evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279c89a2",
      "metadata": {},
      "source": [
        "### Notebook structure\n",
        "\n",
        "1. **Setup** — imports, paths, metric helpers, and kernel-stability settings.\n",
        "2. **Experiment 1** — load PDFs from `data/` (RAG, GIT, GCP); each subfolder is a topic.\n",
        "3. **Chunking setup** — fixed configuration (`chunk_size=300`, `chunk_overlap=50`).\n",
        "4. **VectorStore & RAG** — stable embedder + FAISS retrieval.\n",
        "5. **Retrieval evaluation** — 5 queries, top-3 results and manual evaluation table.\n",
        "6. **Summary** — top1_accuracy, top3_hit_rate, avg_top1_score; save CSVs to `artifacts/`.\n",
        "7. **Decision record** — explicit explanation of why MPNet is not executed in this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from pathlib import Path\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "faiss.omp_set_num_threads(1)\n",
        "print(\"FAISS configured: CPU mode, 1 thread\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "## Pipeline Architecture\n",
        "\n",
        "```\n",
        "PDF folder -> extract text -> chunk -> embed (HashingVectorizer) -> index (FAISS) -> retrieve (top-k)\n",
        "```\n",
        "\n",
        "| Step | What we use | Why |\n",
        "|------|-------------|-----|\n",
        "| Load | `PyPDFLoader` | Local PDF parsing with metadata |\n",
        "| Chunk | `RecursiveCharacterTextSplitter` | Stable document segmentation |\n",
        "| Embed | `HashingVectorizer` (768 dims, L2 norm) | Kernel-safe embedding in this environment |\n",
        "| Index | FAISS `IndexFlatIP` | Inner product; with L2-normalized vectors this is cosine-equivalent |\n",
        "| Retrieve | `index.search` | Fast top-k retrieval |\n",
        "\n",
        "Week 1 model winner remains MPNet conceptually, but Week 2 execution uses a stability-first embedder due to kernel crashes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e4f7e89",
      "metadata": {},
      "source": [
        "## Paths and directories\n",
        "\n",
        "- **`PROJECT_ROOT`** — project root (parent of `notebooks/`).\n",
        "- **`DATA_DIR`** — `data/` with subfolders RAG, GIT, GCP containing PDFs.\n",
        "- **`ARTIFACTS_DIR`** — `artifacts/` for saving retrieval evaluation CSVs and final decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
        "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Data dir: {DATA_DIR} (exists: {DATA_DIR.exists()})\")\n",
        "print(f\"Artifacts dir: {ARTIFACTS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc30935",
      "metadata": {},
      "source": [
        "## Similarity metric functions (from Week 1, explicit numpy)\n",
        "\n",
        "Week 1 selected **cosine similarity** as the preferred retrieval metric.\n",
        "In Week 2 we keep dot, cosine, and Euclidean helpers for transparency.\n",
        "Operational retrieval still follows cosine logic via L2-normalized vectors + `IndexFlatIP`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1305adb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def dot_product(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.sum(a * b))\n",
        "\n",
        "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    dot = np.sum(a * b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    return float(dot / (norm_a * norm_b))\n",
        "\n",
        "def euclidean_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.linalg.norm(a - b))\n",
        "\n",
        "# Week 2 retrieval uses cosine (inner product on L2-normalized embeddings in FAISS)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "---\n",
        "## Experiment 1: Load PDF Documents\n",
        "\n",
        "**Goal:** Load real PDF documents from the `data/` folder.\n",
        "\n",
        "**Structure:**\n",
        "```\n",
        "data/\n",
        "├── RAG/     (3 PDFs about RAG, HyDE, LangChain)\n",
        "├── GIT/     (3 PDFs about Git)\n",
        "└── GCP/     (3 PDFs about Google Cloud)\n",
        "```\n",
        "\n",
        "Each subfolder name becomes the **topic** for evaluation.\n",
        "\n",
        "**Conclusion:** We obtain a flat list of page-level documents with `topic`, `source`, and `page` metadata, ready for chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pdfs(data_dir: Path) -> list:\n",
        "    \"\"\"\n",
        "    Load all PDFs from topic subfolders.\n",
        "    Adds metadata: topic (folder name), source (filename), page.\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "    topics_found = []\n",
        "    \n",
        "    for topic_dir in sorted(data_dir.iterdir()):\n",
        "        if not topic_dir.is_dir():\n",
        "            continue\n",
        "        \n",
        "        topic = topic_dir.name\n",
        "        pdf_files = list(topic_dir.glob(\"*.pdf\"))\n",
        "        \n",
        "        if not pdf_files:\n",
        "            print(f\"  WARNING: No PDFs in {topic}/\")\n",
        "            continue\n",
        "        \n",
        "        topics_found.append(topic)\n",
        "        print(f\"\\n[{topic}]\")\n",
        "        \n",
        "        for pdf_path in pdf_files:\n",
        "            try:\n",
        "                loader = PyPDFLoader(str(pdf_path))\n",
        "                pdf_docs = loader.load()\n",
        "                for doc in pdf_docs:\n",
        "                    doc.metadata[\"topic\"] = topic\n",
        "                    doc.metadata[\"source\"] = pdf_path.name\n",
        "                    docs.append(doc)\n",
        "                print(f\"  + {pdf_path.name} ({len(pdf_docs)} pages)\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ERROR: {pdf_path.name} - {e}\")\n",
        "    \n",
        "    print(f\"\\nLoaded: {len(docs)} pages from {len(topics_found)} topics: {topics_found}\")\n",
        "    return docs\n",
        "\n",
        "\n",
        "# Load all PDFs\n",
        "raw_docs = load_pdfs(DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-6",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"Sample document:\")\n",
        "print(f\"  Topic: {raw_docs[0].metadata['topic']}\")\n",
        "print(f\"  Source: {raw_docs[0].metadata['source']}\")\n",
        "print(f\"  Page: {raw_docs[0].metadata.get('page', 0)}\")\n",
        "print(f\"  Content preview: {raw_docs[0].page_content[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {},
      "source": [
        "---\n",
        "## Chunking Configuration\n",
        "\n",
        "**Goal:** split documents into consistent chunks for retrieval.\n",
        "\n",
        "**Method:** `RecursiveCharacterTextSplitter` keeps semantic boundaries where possible and falls back to character-level splitting.\n",
        "\n",
        "**Configuration used in Week 2:**\n",
        "- `chunk_size=300`\n",
        "- `chunk_overlap=50`\n",
        "\n",
        "**Conclusion:** this fixed configuration is used across the whole notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_documents(docs, chunk_size: int, chunk_overlap: int, separators=None):\n",
        "    \"\"\"\n",
        "    Split a list of LangChain documents into chunks.\n",
        "    Returns list of documents (each has page_content and metadata).\n",
        "    \"\"\"\n",
        "    if separators is None:\n",
        "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=separators,\n",
        "    )\n",
        "    return splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "CHUNK_CONFIG = {\"chunk_size\": 300, \"chunk_overlap\": 50}\n",
        "chunks = chunk_documents(raw_docs, **CHUNK_CONFIG)\n",
        "CHOSEN_CHUNKS = chunks\n",
        "\n",
        "print(f\"Original documents: {len(raw_docs)} pages\")\n",
        "print(f\"Chunk config {CHUNK_CONFIG}: {len(chunks)} chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunks_per_topic(chunks):\n",
        "    out = {}\n",
        "    for c in chunks:\n",
        "        t = c.metadata[\"topic\"]\n",
        "        out[t] = out.get(t, 0) + 1\n",
        "    return out\n",
        "\n",
        "print(\"Chunks per topic:\")\n",
        "for topic, count in sorted(chunks_per_topic(chunks).items()):\n",
        "    print(f\"  {topic}: {count} chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-10",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Sample chunk:\")\n",
        "c = chunks[len(chunks) // 2]\n",
        "print(f\"  Topic: {c.metadata['topic']}, Source: {c.metadata['source']}\")\n",
        "print(f\"  Length: {len(c.page_content)} chars\")\n",
        "print(f\"  Content: {c.page_content[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3726dd3",
      "metadata": {},
      "source": [
        "---\n",
        "## VectorStore and Retriever Classes\n",
        "\n",
        "**VectorStore**\n",
        "- `build_index(chunks)` builds FAISS `IndexFlatIP` in streaming batches.\n",
        "- `retrieve(query, top_k=3)` returns top-k scores and indices.\n",
        "\n",
        "**Embedder choice in this notebook**\n",
        "- `StableEmbedder` uses `HashingVectorizer` (deterministic, CPU-only, no torch runtime).\n",
        "- Chosen to avoid native kernel termination observed with MPNet in this exact environment.\n",
        "\n",
        "**Retriever wrapper**\n",
        "- `RAG.retrieve(query, top_k=3)` returns `(score, chunk)` for inspection and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e736c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Week 2 stable embedder (no torch/sentence-transformers in kernel).\n",
        "EMBEDDING_MODEL_ID = \"hashing-768-stable\"\n",
        "\n",
        "\n",
        "class StableEmbedder:\n",
        "    \"\"\"Lightweight deterministic embedder based on HashingVectorizer.\"\"\"\n",
        "\n",
        "    def __init__(self, n_features: int = 768):\n",
        "        self.n_features = n_features\n",
        "        self.vectorizer = HashingVectorizer(\n",
        "            n_features=n_features,\n",
        "            norm=\"l2\",\n",
        "            alternate_sign=False,\n",
        "            lowercase=True,\n",
        "        )\n",
        "\n",
        "    def encode(self, texts):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        x = self.vectorizer.transform(texts)\n",
        "        return np.ascontiguousarray(x.toarray(), dtype=np.float32)\n",
        "\n",
        "\n",
        "class VectorStore:\n",
        "    \"\"\"\n",
        "    Local vector store: chunks + embeddings + FAISS IndexFlatIP.\n",
        "    Similarity = cosine (inner product on L2-normalized vectors).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_id: str = EMBEDDING_MODEL_ID, batch_size: int = 64):\n",
        "        self.model_id = model_id\n",
        "        self.batch_size = batch_size\n",
        "        self.model = StableEmbedder(n_features=768)\n",
        "        self.chunks = []\n",
        "        self._index = None\n",
        "\n",
        "    def build_index(self, chunks):\n",
        "        \"\"\"Build FAISS index from chunk documents in streaming batches.\"\"\"\n",
        "        self.chunks = list(chunks)\n",
        "        if not self.chunks:\n",
        "            raise ValueError(\"Cannot index: chunks list is empty\")\n",
        "\n",
        "        texts = [c.page_content for c in self.chunks]\n",
        "        self._index = None\n",
        "\n",
        "        for i in range(0, len(texts), self.batch_size):\n",
        "            batch = texts[i:i + self.batch_size]\n",
        "            batch_emb = self.model.encode(batch)\n",
        "\n",
        "            if self._index is None:\n",
        "                self._index = faiss.IndexFlatIP(batch_emb.shape[1])\n",
        "\n",
        "            self._index.add(batch_emb)\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 3):\n",
        "        \"\"\"Return (scores, indices) for top-k chunks. Scores are cosine similarity.\"\"\"\n",
        "        if not query or not query.strip():\n",
        "            raise ValueError(\"Query cannot be empty\")\n",
        "        if self._index is None or self._index.ntotal == 0:\n",
        "            raise ValueError(\"Index is empty; call build_index(chunks) first\")\n",
        "\n",
        "        q = self.model.encode([query])\n",
        "        k = min(top_k, self._index.ntotal)\n",
        "        scores, indices = self._index.search(q, k)\n",
        "        return scores[0], indices[0]\n",
        "\n",
        "\n",
        "class RAG:\n",
        "    \"\"\"Retrieval-only wrapper (no generation in Week 2).\"\"\"\n",
        "\n",
        "    def __init__(self, vector_store: VectorStore):\n",
        "        self.vector_store = vector_store\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 3):\n",
        "        scores, indices = self.vector_store.retrieve(query, top_k=top_k)\n",
        "        return [(float(s), self.vector_store.chunks[i]) for s, i in zip(scores, indices)]\n",
        "\n",
        "\n",
        "print(\"VectorStore and RAG classes defined. Embedding model:\", EMBEDDING_MODEL_ID)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {},
      "source": [
        "---\n",
        "## Retrieval Evaluation\n",
        "\n",
        "**Goal:** run a full retrieval pass with fixed chunking and measure relevance.\n",
        "\n",
        "**Setup:**\n",
        "1. Build a single `VectorStore` with the stable embedder and cosine-equivalent FAISS retrieval.\n",
        "2. Run the fixed query set.\n",
        "3. Check top-1 and top-3 relevance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: initialize vector store (model load)\n",
        "store = VectorStore()\n",
        "print(\"VectorStore initialized successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step-build-index",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: build FAISS index from chunks (heaviest operation)\n",
        "print(f\"Building index for {len(chunks)} chunks with batch_size={store.batch_size}...\")\n",
        "store.build_index(chunks)\n",
        "print(f\"Index built. ntotal={store._index.ntotal}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step-rag-queries",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: retrieval checks\n",
        "rag = RAG(store)\n",
        "\n",
        "TEST_QUERIES = [\n",
        "    \"What is RAG?\",\n",
        "    \"What is GIT?\",\n",
        "    \"What is GCP?\",\n",
        "    \"How to create a git branch?\",\n",
        "    \"What is HyDE in RAG?\",\n",
        "]\n",
        "\n",
        "def print_top3(results):\n",
        "    for rank, (score, chunk) in enumerate(results, 1):\n",
        "        topic = chunk.metadata.get(\"topic\", \"?\")\n",
        "        src = chunk.metadata.get(\"source\", \"?\")[:40]\n",
        "        text = chunk.page_content[:120].replace(\"\\n\", \" \")\n",
        "        print(f\"  {rank}. [{topic}] score={score:.4f} | {src}\")\n",
        "        print(f\"      {text}...\")\n",
        "\n",
        "for query in TEST_QUERIES:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Query: {query!r}\")\n",
        "    print(\"-\" * 60)\n",
        "    print_top3(rag.retrieve(query, top_k=3))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "165125da",
      "metadata": {},
      "source": [
        "### Manual Evaluation Summary\n",
        "\n",
        "The table records, per query:\n",
        "- `expected_topic`\n",
        "- `top1_topic`\n",
        "- `top1_score`\n",
        "- `top3_hit`\n",
        "\n",
        "This is the evidence table for Week 2 retrieval quality under the stable execution setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {},
      "outputs": [],
      "source": [
        "QUERY_EXPECTED_TOPIC = {\n",
        "    \"What is RAG?\": \"RAG\",\n",
        "    \"What is GIT?\": \"GIT\",\n",
        "    \"What is GCP?\": \"GCP\",\n",
        "    \"How to create a git branch?\": \"GIT\",\n",
        "    \"What is HyDE in RAG?\": \"RAG\",\n",
        "}\n",
        "\n",
        "rows = []\n",
        "for query in TEST_QUERIES:\n",
        "    expected = QUERY_EXPECTED_TOPIC.get(query, \"?\")\n",
        "    results = rag.retrieve(query, top_k=3)\n",
        "    topics = [item[1].metadata.get(\"topic\", \"?\") for item in results]\n",
        "\n",
        "    rows.append({\n",
        "        \"query\": query,\n",
        "        \"expected_topic\": expected,\n",
        "        \"top1_topic\": topics[0] if topics else \"-\",\n",
        "        \"top1_score\": round(results[0][0], 4) if results else None,\n",
        "        \"top3_hit\": expected in topics,\n",
        "    })\n",
        "\n",
        "eval_df = pd.DataFrame(rows)\n",
        "print(\"Manual evaluation table:\")\n",
        "eval_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14",
      "metadata": {},
      "outputs": [],
      "source": [
        "retrieval_summary = pd.DataFrame([\n",
        "    {\n",
        "        \"top1_accuracy\": round((eval_df[\"top1_topic\"] == eval_df[\"expected_topic\"]).mean(), 4),\n",
        "        \"top3_hit_rate\": round(eval_df[\"top3_hit\"].mean(), 4),\n",
        "        \"avg_top1_score\": round(eval_df[\"top1_score\"].mean(), 4),\n",
        "        \"chunk_size\": CHUNK_CONFIG[\"chunk_size\"],\n",
        "        \"chunk_overlap\": CHUNK_CONFIG[\"chunk_overlap\"],\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"Retrieval summary:\")\n",
        "retrieval_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32ad0cbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Week 2 fixed chunk config: {CHUNK_CONFIG}\")\n",
        "print(\"Model: hashing-768-stable | Metric: cosine-equivalent (IndexFlatIP on normalized vectors)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07640fd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_df.sort_values(\"query\").to_csv(ARTIFACTS_DIR / \"week2_retrieval_eval.csv\", index=False)\n",
        "retrieval_summary.to_csv(ARTIFACTS_DIR / \"week2_retrieval_summary.csv\", index=False)\n",
        "print(\"Saved:\")\n",
        "print(\"- artifacts/week2_retrieval_eval.csv\")\n",
        "print(\"- artifacts/week2_retrieval_summary.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e01ec2b9",
      "metadata": {},
      "source": [
        "---\n",
        "## Analysis and Rationale\n",
        "\n",
        "### 1) Chunking\n",
        "- Fixed configuration: `chunk_size=300`, `chunk_overlap=50`.\n",
        "- Overlap preserves context across chunk boundaries.\n",
        "\n",
        "### 2) Metric\n",
        "- Retrieval uses cosine-equivalent ranking (`IndexFlatIP` on L2-normalized vectors).\n",
        "\n",
        "### 3) Why MPNet is not used in Week 2 execution\n",
        "- MPNet worked conceptually from Week 1 evaluation.\n",
        "- In this local runtime (`Python 3.13` + Jupyter kernel), MPNet path (`sentence-transformers/torch`) crashes the kernel process during indexing.\n",
        "- Because the process dies at native level, notebook execution is not reproducible with MPNet here.\n",
        "\n",
        "### 4) Why this fallback was chosen\n",
        "- `HashingVectorizer` is deterministic, CPU-only, and stable in the same environment.\n",
        "- It lets us complete Week 2 goals (chunking, indexing, retrieval evaluation, artifact export) without kernel death.\n",
        "- Tradeoff: semantic quality is typically lower than MPNet, but operational stability is higher.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "047196f7",
      "metadata": {},
      "source": [
        "---\n",
        "## Final Week 2 Conclusions\n",
        "\n",
        "1. Retrieval pipeline is built and validated on the local corpus.\n",
        "2. Chunking is fixed: `chunk_size=300`, `chunk_overlap=50`.\n",
        "3. Metric is cosine-equivalent via FAISS `IndexFlatIP` on normalized vectors.\n",
        "4. Week 2 embedder is `hashing-768-stable` for runtime reliability.\n",
        "\n",
        "## Decision Record\n",
        "- **Preferred model from Week 1:** MPNet.\n",
        "- **Week 2 runtime model:** hashing fallback.\n",
        "- **Reason:** repeated native kernel termination when running MPNet in this environment.\n",
        "- **Principle used:** reproducible execution over model optimality for this stage.\n",
        "\n",
        "### Handoff to Week 3\n",
        "Week 3 should reuse:\n",
        "- fixed chunking,\n",
        "- stable retrieval interface (`VectorStore`/`RAG`),\n",
        "- artifact outputs from Week 2,\n",
        "and then add prompt + LLM generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a59314a",
      "metadata": {},
      "outputs": [],
      "source": [
        "WEEK2_OPERATIONAL_MODEL = \"hashing-768-stable\"\n",
        "\n",
        "final_week2_summary = pd.DataFrame([\n",
        "    {\n",
        "        \"decision\": \"chunking_config\",\n",
        "        \"value\": str(CHUNK_CONFIG),\n",
        "        \"basis\": \"fixed in Week 2 pipeline\",\n",
        "    },\n",
        "    {\n",
        "        \"decision\": \"preferred_model_from_week1\",\n",
        "        \"value\": \"all-mpnet-base-v2\",\n",
        "        \"basis\": \"Week 1 quality winner\",\n",
        "    },\n",
        "    {\n",
        "        \"decision\": \"operational_model_week2\",\n",
        "        \"value\": WEEK2_OPERATIONAL_MODEL,\n",
        "        \"basis\": \"runtime stability in Python 3.13 Jupyter\",\n",
        "    },\n",
        "    {\n",
        "        \"decision\": \"model_policy\",\n",
        "        \"value\": \"stability_fallback\",\n",
        "        \"basis\": \"MPNet path caused native kernel crash\",\n",
        "    },\n",
        "    {\n",
        "        \"decision\": \"metric\",\n",
        "        \"value\": \"cosine_equivalent\",\n",
        "        \"basis\": \"normalized vectors + FAISS IndexFlatIP\",\n",
        "    },\n",
        "])\n",
        "\n",
        "final_week2_summary.to_csv(ARTIFACTS_DIR / \"week2_final_decisions.csv\", index=False)\n",
        "print(\"Final Week 2 decisions:\")\n",
        "final_week2_summary\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag_env (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
