{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61464f1c",
   "metadata": {},
   "source": [
    "# Week 3: LLM Integration and Prompts\n",
    "\n",
    "**Pipeline continuity:**\n",
    "- **Week 1 output:** semantic-quality winner was `MPNet` + cosine metric.\n",
    "- **Week 2 output:** production-stable retriever in this environment uses `hashing-768-stable` + FAISS `IndexFlatIP`.\n",
    "- **Week 3 focus:** keep Week 2 runtime retriever unchanged and add generation (prompting + local LLM).\n",
    "\n",
    "**Goal:** end-to-end RAG flow:\n",
    "`query -> retrieve context (Week 2 runtime) -> build prompt -> LLM answer`.\n",
    "\n",
    "**Important:** Week 3 is aligned with Week 2 execution constraints (Python 3.13 Jupyter stability), so retrieval here uses the same stable fallback embedder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf2c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "faiss.omp_set_num_threads(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d17d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/tkhamidulin/Desktop/First Project - RAG\n",
      "Data dir: /Users/tkhamidulin/Desktop/First Project - RAG/data (exists: True)\n",
      "Artifacts: /Users/tkhamidulin/Desktop/First Project - RAG/artifacts\n"
     ]
    }
   ],
   "source": [
    "# Same paths as Week 2 (notebook is in notebooks/)\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "W3_EXPERIMENTS_CSV = ARTIFACTS_DIR / \"week3_prompt_experiments.csv\"\n",
    "W3_GENERATIONS_JSONL = ARTIFACTS_DIR / \"week3_generations.jsonl\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data dir: {DATA_DIR} (exists: {DATA_DIR.exists()})\")\n",
    "print(f\"Artifacts: {ARTIFACTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5d37a9",
   "metadata": {},
   "source": [
    "---\n",
    "## From Week 1 and Week 2: Retrieval Pipeline\n",
    "\n",
    "**From Week 1:** MPNet + cosine was the quality winner during embedding-model evaluation.\n",
    "\n",
    "**From Week 2 (runtime decision):** due to native kernel crashes with `sentence-transformers/torch` in this local environment, retrieval switched to a stable fallback embedder (`hashing-768-stable`) while keeping FAISS and cosine-equivalent ranking.\n",
    "\n",
    "Week 3 reuses that **exact runtime retriever** so experiments with prompts/LLM stay reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f2cb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: load_pdfs, chunk_documents, VectorStore, RAG. Embedding model: hashing-768-stable\n"
     ]
    }
   ],
   "source": [
    "# --- From Week 2: load PDFs and chunking ---\n",
    "def load_pdfs(data_dir: Path):\n",
    "    docs = []\n",
    "    for topic_dir in sorted(data_dir.iterdir()):\n",
    "        if not topic_dir.is_dir():\n",
    "            continue\n",
    "        topic = topic_dir.name\n",
    "        for pdf_path in topic_dir.glob(\"*.pdf\"):\n",
    "            try:\n",
    "                loader = PyPDFLoader(str(pdf_path))\n",
    "                for doc in loader.load():\n",
    "                    doc.metadata[\"topic\"] = topic\n",
    "                    doc.metadata[\"source\"] = pdf_path.name\n",
    "                    docs.append(doc)\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: {pdf_path.name} - {e}\")\n",
    "    return docs\n",
    "\n",
    "def chunk_documents(docs, chunk_size: int, chunk_overlap: int, separators=None):\n",
    "    if separators is None:\n",
    "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=separators)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "# --- From Week 2: stable embedder + VectorStore + RAG ---\n",
    "EMBEDDING_MODEL_ID = \"hashing-768-stable\"\n",
    "\n",
    "class StableEmbedder:\n",
    "    def __init__(self, n_features: int = 768):\n",
    "        self.vectorizer = HashingVectorizer(\n",
    "            n_features=n_features,\n",
    "            norm=\"l2\",\n",
    "            alternate_sign=False,\n",
    "            lowercase=True,\n",
    "        )\n",
    "\n",
    "    def encode(self, texts):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        arr = self.vectorizer.transform(texts).toarray()\n",
    "        return np.ascontiguousarray(arr, dtype=np.float32)\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, model_id: str = EMBEDDING_MODEL_ID, batch_size: int = 64):\n",
    "        self.model_id = model_id\n",
    "        self.batch_size = batch_size\n",
    "        self.model = StableEmbedder(n_features=768)\n",
    "        self.chunks = []\n",
    "        self._index = None\n",
    "\n",
    "    def build_index(self, chunks):\n",
    "        self.chunks = list(chunks)\n",
    "        if not self.chunks:\n",
    "            raise ValueError(\"Cannot index: chunks list is empty\")\n",
    "        texts = [c.page_content for c in self.chunks]\n",
    "        self._index = None\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            emb = self.model.encode(batch)\n",
    "            if self._index is None:\n",
    "                self._index = faiss.IndexFlatIP(emb.shape[1])\n",
    "            self._index.add(emb)\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 3):\n",
    "        if self._index is None or self._index.ntotal == 0:\n",
    "            raise ValueError(\"Index is empty; call build_index(chunks) first\")\n",
    "        q = self.model.encode([query])\n",
    "        k = min(top_k, self._index.ntotal)\n",
    "        scores, indices = self._index.search(q, k)\n",
    "        return scores[0], indices[0]\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, vector_store: VectorStore):\n",
    "        self.vector_store = vector_store\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 3):\n",
    "        scores, indices = self.vector_store.retrieve(query, top_k=top_k)\n",
    "        return [(float(s), self.vector_store.chunks[i]) for s, i in zip(scores, indices)]\n",
    "\n",
    "print(\"Loaded: load_pdfs, chunk_documents, VectorStore, RAG. Embedding model:\", EMBEDDING_MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe71939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n",
      "Error -3 while decompressing data: incorrect header check\n",
      "found 0 objects within Object(775,0) whereas 200 expected\n",
      "Error -3 while decompressing data: incorrect header check\n",
      "found 0 objects within Object(776,0) whereas 20 expected\n",
      "Cannot find \"/Root\" key in trailer\n",
      "Searching object with \"/Catalog\" key\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ERROR: A-Complete-Guide-to-the-Google-Cloud-Platform.pdf - Cannot find Root object in pdf\n",
      "Documents: 101 pages -> 833 chunks ({'chunk_size': 300, 'chunk_overlap': 50})\n",
      "Retriever ready. Embedding model: hashing-768-stable\n"
     ]
    }
   ],
   "source": [
    "# Run Week 2 runtime pipeline: load PDFs -> chunk -> build index -> RAG\n",
    "raw_docs = load_pdfs(DATA_DIR)\n",
    "CHUNK_CONFIG = {\"chunk_size\": 300, \"chunk_overlap\": 50}\n",
    "chunks = chunk_documents(raw_docs, **CHUNK_CONFIG)\n",
    "\n",
    "store = VectorStore()\n",
    "store.build_index(chunks)\n",
    "rag = RAG(store)\n",
    "\n",
    "print(f\"Documents: {len(raw_docs)} pages -> {len(chunks)} chunks ({CHUNK_CONFIG})\")\n",
    "print(f\"Retriever ready. Embedding model: {EMBEDDING_MODEL_ID}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dff120",
   "metadata": {},
   "source": [
    "## Local LLM (Ollama) via LangChain\n",
    "\n",
    "This notebook uses a local model through Ollama to keep experiments reproducible and offline-friendly.\n",
    "\n",
    "Example setup (outside notebook):\n",
    "- `ollama pull llama3.1` (or another model)\n",
    "- Ensure Ollama is running (default: `http://localhost:11434`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628a7d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"gemma3:4b\"   # exactly as in `ollama list`\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "TEMPERATURE_DEFAULT = 0.2\n",
    "\n",
    "def build_llm(temperature: float) -> OllamaLLM:\n",
    "    \"\"\"\n",
    "    Build a LangChain-compatible Ollama LLM.\n",
    "    Using a factory function avoids duplication across experiments.\n",
    "    \"\"\"\n",
    "    return OllamaLLM(\n",
    "        model=MODEL_NAME,\n",
    "        temperature=temperature,\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        validate_model_on_init=True,  # fail fast if model is missing\n",
    "    )\n",
    "\n",
    "# Smoke test\n",
    "build_llm(TEMPERATURE_DEFAULT).invoke(\"Reply: OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb8ed6",
   "metadata": {},
   "source": [
    "## Retrieval: Week 2 Runtime Pipeline Inside Week 3\n",
    "\n",
    "Контекст для LLM получаем локально через `rag.retrieve(query, top_k=3)`.\n",
    "Это тот же runtime-пайплайн, который зафиксирован в Week 2 (stable hashing embedder + FAISS).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c0a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example retrieval: query=What is RAG?...\n",
      "  Top-k chunks: 3, scores: [0.4629, 0.3825, 0.379]\n",
      "  First chunk preview: RAG makes LLMs better and\n",
      "equal\n",
      "Amnon, Roy, Ilai, Nathan, Amir\n",
      "Products\n",
      "Vector D...\n"
     ]
    }
   ],
   "source": [
    "def get_retrieval_for_query(query: str, top_k: int = 3) -> dict:\n",
    "    \"\"\"Use Week 2 pipeline: rag.retrieve() returns (score, chunk). Build row for prompt.\"\"\"\n",
    "    retrieved = rag.retrieve(query, top_k=top_k)\n",
    "    chunks = [c.page_content for _, c in retrieved]\n",
    "    scores = [s for s, _ in retrieved]\n",
    "    return {\"query\": query, \"chunks\": chunks, \"scores\": scores, \"meta\": {}}\n",
    "\n",
    "# Example: one query to verify retrieval works\n",
    "example = get_retrieval_for_query(\"What is RAG?\")\n",
    "print(f\"Example retrieval: query={example['query'][:40]}...\")\n",
    "print(f\"  Top-k chunks: {len(example['chunks'])}, scores: {[round(s, 4) for s in example['scores']]}\")\n",
    "print(f\"  First chunk preview: {example['chunks'][0][:80]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7317ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(chunks: list[str], scores: list[float] | None = None, max_chars: int = 7000) -> str:\n",
    "    parts = []\n",
    "    for i, ch in enumerate(chunks):\n",
    "        score_str = f\" (score={scores[i]:.4f})\" if scores is not None else \"\"\n",
    "        parts.append(f\"[Chunk {i+1}{score_str}]\\n{ch}\")\n",
    "    ctx = \"\\n\\n\".join(parts)\n",
    "    return ctx[:max_chars]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7afb77",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "I define three task-specific prompt templates:\n",
    "\n",
    "1) **Strict Q&A**  \n",
    "   - Answer using ONLY context  \n",
    "   - If missing: exact refusal message  \n",
    "   - Require chunk citations\n",
    "\n",
    "2) **Structured Summary**  \n",
    "   - Extract key points, definitions, practical notes, and gaps\n",
    "\n",
    "3) **Grounded Reasoning**  \n",
    "   - Provide short reasoning steps tied to evidence  \n",
    "   - Avoid outside knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51cd4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_STRICT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are a RAG assistant.\\n\"\n",
    "        \"You MUST answer using ONLY the provided CONTEXT.\\n\"\n",
    "        \"If the answer is not in the context, reply exactly:\\n\"\n",
    "        \"\\\"I don't know based on the provided context.\\\"\\n\\n\"\n",
    "        \"CONTEXT:\\n{context}\\n\\n\"\n",
    "        \"QUESTION:\\n{question}\\n\\n\"\n",
    "        \"RULES:\\n\"\n",
    "        \"- No outside knowledge.\\n\"\n",
    "        \"- Be concise.\\n\"\n",
    "        \"- Cite chunks like [Chunk 2].\\n\\n\"\n",
    "        \"ANSWER:\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "SUMMARY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"topic\"],\n",
    "    template=(\n",
    "        \"Summarize the provided context about: {topic}\\n\\n\"\n",
    "        \"CONTEXT:\\n{context}\\n\\n\"\n",
    "        \"OUTPUT FORMAT:\\n\"\n",
    "        \"- Key points (3–7 bullets)\\n\"\n",
    "        \"- Definitions (if any)\\n\"\n",
    "        \"- Practical notes (if any)\\n\"\n",
    "        \"- Missing information / open questions (if any)\\n\\n\"\n",
    "        \"SUMMARY:\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "REASONING_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are an analyst. Use ONLY the provided context.\\n\"\n",
    "        \"Do not add outside knowledge.\\n\\n\"\n",
    "        \"CONTEXT:\\n{context}\\n\\n\"\n",
    "        \"QUESTION:\\n{question}\\n\\n\"\n",
    "        \"OUTPUT FORMAT:\\n\"\n",
    "        \"1) Answer (1–3 sentences)\\n\"\n",
    "        \"2) Evidence: cite chunks like [Chunk 1]\\n\"\n",
    "        \"3) Reasoning: 3–6 short bullet steps tied to evidence\\n\\n\"\n",
    "        \"RESPONSE:\\n\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a964e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval is done by get_retrieval_for_query(question) — no find_retrieval_row needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbc2cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_id(*parts) -> str:\n",
    "    s = \"||\".join(map(str, parts))\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def run_generation(\n",
    "    question: str,\n",
    "    retrieval_row: dict,\n",
    "    prompt: PromptTemplate,\n",
    "    template_name: str,\n",
    "    temperature: float,\n",
    "    topic: str = \"RAG notes\"\n",
    ") -> dict:\n",
    "    llm = build_llm(temperature)\n",
    "\n",
    "    context = format_context(retrieval_row[\"chunks\"], retrieval_row[\"scores\"])\n",
    "    kwargs = {\"context\": context}\n",
    "\n",
    "    if \"question\" in prompt.input_variables:\n",
    "        kwargs[\"question\"] = question\n",
    "    if \"topic\" in prompt.input_variables:\n",
    "        kwargs[\"topic\"] = topic\n",
    "\n",
    "    prompt_text = prompt.format(**kwargs)\n",
    "\n",
    "    t0 = time.time()\n",
    "    answer = llm.invoke(prompt_text)\n",
    "    latency = round(time.time() - t0, 3)\n",
    "\n",
    "    answer = (answer or \"\").strip()\n",
    "\n",
    "    return {\n",
    "        \"run_id\": stable_id(question, template_name, MODEL_NAME, temperature),\n",
    "        \"question\": question,\n",
    "        \"template\": template_name,\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"temperature\": temperature,\n",
    "        \"latency_s\": latency,\n",
    "        \"prompt_chars\": len(prompt_text),\n",
    "        \"context_chars\": len(context),\n",
    "        \"n_chunks\": len(retrieval_row[\"chunks\"]),\n",
    "        \"retrieval_query_used\": retrieval_row[\"query\"],\n",
    "        \"answer\": answer,\n",
    "        \"idk_flag\": (\"i don't know based on the provided context\" in answer.lower()),\n",
    "        \"has_citation_flag\": (\"[chunk\" in answer.lower()),  # lightweight heuristic\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e57dc",
   "metadata": {},
   "source": [
    "## Test Set\n",
    "\n",
    "A small, diverse set of questions is used to compare prompt behavior:\n",
    "- definitions (low risk)\n",
    "- “why” questions (higher hallucination risk)\n",
    "- comparison questions\n",
    "- pipeline-level questions\n",
    "\n",
    "This is a controlled prompt experiment, not a full benchmark yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9dc1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"What is Retrieval-Augmented Generation (RAG)?\",\n",
    "    \"Why do we use chunk overlap in retrieval systems?\",\n",
    "    \"Explain cosine similarity vs dot product similarity for embeddings.\",\n",
    "    \"What is the role of FAISS in a RAG pipeline?\",\n",
    "    \"When would you increase chunk size and why?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e98a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>question</th>\n",
       "      <th>template</th>\n",
       "      <th>model</th>\n",
       "      <th>temperature</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>prompt_chars</th>\n",
       "      <th>context_chars</th>\n",
       "      <th>n_chunks</th>\n",
       "      <th>retrieval_query_used</th>\n",
       "      <th>answer</th>\n",
       "      <th>idk_flag</th>\n",
       "      <th>has_citation_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>773148cfcd0d205f</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>qa_strict</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.931</td>\n",
       "      <td>1160</td>\n",
       "      <td>835</td>\n",
       "      <td>3</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>Retrieval-augmented generation, or RAG, is a t...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ed2bea5911a9d13d</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>qa_strict</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.386</td>\n",
       "      <td>1160</td>\n",
       "      <td>835</td>\n",
       "      <td>3</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>Retrieval-augmented generation, or RAG, is a t...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>223dbe53cb99f7e7</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>summary</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.650</td>\n",
       "      <td>1064</td>\n",
       "      <td>835</td>\n",
       "      <td>3</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>Here’s a summary of the provided context about...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ec596e6f77b05533</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>summary</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9.511</td>\n",
       "      <td>1064</td>\n",
       "      <td>835</td>\n",
       "      <td>3</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>Here’s a summary of the provided context about...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83b5662850e2bfeb</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.752</td>\n",
       "      <td>1131</td>\n",
       "      <td>835</td>\n",
       "      <td>3</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>1) Answer: Retrieval-augmented generation (RAG...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             run_id                                       question   template  \\\n",
       "0  773148cfcd0d205f  What is Retrieval-Augmented Generation (RAG)?  qa_strict   \n",
       "1  ed2bea5911a9d13d  What is Retrieval-Augmented Generation (RAG)?  qa_strict   \n",
       "2  223dbe53cb99f7e7  What is Retrieval-Augmented Generation (RAG)?    summary   \n",
       "3  ec596e6f77b05533  What is Retrieval-Augmented Generation (RAG)?    summary   \n",
       "4  83b5662850e2bfeb  What is Retrieval-Augmented Generation (RAG)?  reasoning   \n",
       "\n",
       "       model  temperature  latency_s  prompt_chars  context_chars  n_chunks  \\\n",
       "0  gemma3:4b          0.0      1.931          1160            835         3   \n",
       "1  gemma3:4b          0.2      1.386          1160            835         3   \n",
       "2  gemma3:4b          0.0      8.650          1064            835         3   \n",
       "3  gemma3:4b          0.2      9.511          1064            835         3   \n",
       "4  gemma3:4b          0.0      5.752          1131            835         3   \n",
       "\n",
       "                            retrieval_query_used  \\\n",
       "0  What is Retrieval-Augmented Generation (RAG)?   \n",
       "1  What is Retrieval-Augmented Generation (RAG)?   \n",
       "2  What is Retrieval-Augmented Generation (RAG)?   \n",
       "3  What is Retrieval-Augmented Generation (RAG)?   \n",
       "4  What is Retrieval-Augmented Generation (RAG)?   \n",
       "\n",
       "                                              answer  idk_flag  \\\n",
       "0  Retrieval-augmented generation, or RAG, is a t...     False   \n",
       "1  Retrieval-augmented generation, or RAG, is a t...     False   \n",
       "2  Here’s a summary of the provided context about...     False   \n",
       "3  Here’s a summary of the provided context about...     False   \n",
       "4  1) Answer: Retrieval-augmented generation (RAG...     False   \n",
       "\n",
       "   has_citation_flag  \n",
       "0               True  \n",
       "1               True  \n",
       "2              False  \n",
       "3              False  \n",
       "4               True  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates = [\n",
    "    (\"qa_strict\", QA_STRICT_PROMPT),\n",
    "    (\"summary\", SUMMARY_PROMPT),\n",
    "    (\"reasoning\", REASONING_PROMPT),\n",
    "]\n",
    "\n",
    "temperature_grid = [0.0, 0.2]  # compare deterministic vs slightly creative\n",
    "\n",
    "records = []\n",
    "for q in test_questions:\n",
    "    r = get_retrieval_for_query(q)\n",
    "    for (name, tpl) in templates:\n",
    "        for temp in temperature_grid:\n",
    "            rec = run_generation(\n",
    "                question=q,\n",
    "                retrieval_row=r,\n",
    "                prompt=tpl,\n",
    "                template_name=name,\n",
    "                temperature=temp,\n",
    "                topic=\"RAG / retrieval fundamentals\"\n",
    "            )\n",
    "            records.append(rec)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f233eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/tkhamidulin/Desktop/First Project - RAG/artifacts/week3_prompt_experiments.csv\n",
      "Saved: /Users/tkhamidulin/Desktop/First Project - RAG/artifacts/week3_generations.jsonl\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(W3_EXPERIMENTS_CSV, index=False)\n",
    "\n",
    "with open(W3_GENERATIONS_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in records:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", W3_EXPERIMENTS_CSV)\n",
    "print(\"Saved:\", W3_GENERATIONS_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d96db46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>template</th>\n",
       "      <th>temperature</th>\n",
       "      <th>n</th>\n",
       "      <th>avg_latency_s</th>\n",
       "      <th>avg_answer_len</th>\n",
       "      <th>idk_rate</th>\n",
       "      <th>citation_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qa_strict</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.4064</td>\n",
       "      <td>131.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa_strict</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8578</td>\n",
       "      <td>131.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.2148</td>\n",
       "      <td>921.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>4.4916</td>\n",
       "      <td>937.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>9.5356</td>\n",
       "      <td>1836.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summary</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>9.4170</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    template  temperature  n  avg_latency_s  avg_answer_len  idk_rate  \\\n",
       "0  qa_strict          0.0  5         1.4064           131.6       0.6   \n",
       "1  qa_strict          0.2  5         0.8578           131.6       0.6   \n",
       "2  reasoning          0.0  5         5.2148           921.8       0.0   \n",
       "3  reasoning          0.2  5         4.4916           937.0       0.0   \n",
       "4    summary          0.0  5         9.5356          1836.2       0.0   \n",
       "5    summary          0.2  5         9.4170          1880.0       0.0   \n",
       "\n",
       "   citation_rate  \n",
       "0            0.4  \n",
       "1            0.4  \n",
       "2            1.0  \n",
       "3            1.0  \n",
       "4            0.0  \n",
       "5            0.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"answer_len\"] = df[\"answer\"].fillna(\"\").str.len()\n",
    "\n",
    "summary = (\n",
    "    df.groupby([\"template\", \"temperature\"])\n",
    "      .agg(\n",
    "          n=(\"run_id\", \"count\"),\n",
    "          avg_latency_s=(\"latency_s\", \"mean\"),\n",
    "          avg_answer_len=(\"answer_len\", \"mean\"),\n",
    "          idk_rate=(\"idk_flag\", \"mean\"),\n",
    "          citation_rate=(\"has_citation_flag\", \"mean\"),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values([\"template\", \"temperature\"])\n",
    ")\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a771e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>template</th>\n",
       "      <th>temperature</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qa_strict</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Retrieval-augmented generation, or RAG, is a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa_strict</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Retrieval-augmented generation, or RAG, is a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Here’s a summary of the provided context about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summary</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Here’s a summary of the provided context about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1) Answer: Retrieval-augmented generation (RAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1) Answer: Retrieval-augmented generation (RAG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    template  temperature                                             answer\n",
       "0  qa_strict          0.0  Retrieval-augmented generation, or RAG, is a t...\n",
       "1  qa_strict          0.2  Retrieval-augmented generation, or RAG, is a t...\n",
       "2    summary          0.0  Here’s a summary of the provided context about...\n",
       "3    summary          0.2  Here’s a summary of the provided context about...\n",
       "4  reasoning          0.0  1) Answer: Retrieval-augmented generation (RAG...\n",
       "5  reasoning          0.2  1) Answer: Retrieval-augmented generation (RAG..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = test_questions[0]\n",
    "df[df[\"question\"] == q][[\"template\", \"temperature\", \"answer\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d483b19",
   "metadata": {},
   "source": [
    "## Conclusions (Week 3)\n",
    "\n",
    "### Prompt behavior findings\n",
    "- **Strict Q&A**: most grounded and consistent with refusal rules.\n",
    "- **Summary style**: best readability, but may drop details needed for precise Q&A.\n",
    "- **Reasoning style**: better explanations, but more sensitive to weak context.\n",
    "\n",
    "### Retrieval-to-generation dependency\n",
    "Answer quality is directly limited by retrieval quality from Week 2: weak context leads to weaker grounded answers regardless of prompt style.\n",
    "\n",
    "### Handoff to Week 4\n",
    "Week 4 adds guardrails around this Week 3 pipeline: pre-checks before LLM calls and post-checks on outputs to reduce hallucinations, leakage, and unsafe responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10a748f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
