{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce48175",
   "metadata": {},
   "source": [
    "# Week 2 — Local Retrieval Pipeline (FAISS)\n",
    "\n",
    "This notebook implements a local retrieval pipeline:\n",
    "- chunk documents with multiple chunk sizes/overlaps\n",
    "- build FAISS vector index\n",
    "- compare two embedding models (MiniLM vs MPNet)\n",
    "- evaluate retrieval quality with hit@k and MRR\n",
    "- do manual inspection and summarize findings\n",
    "\n",
    "**Important note:** If you see MRR≈1 and hit@k≈1 for all queries, it is usually caused by an easy evaluation setup\n",
    "(homogeneous corpus + direct queries), not by a perfect retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4baec79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef520bf",
   "metadata": {},
   "source": [
    "## Load Documents (GenAI & RAG domain only)\n",
    "\n",
    "Replace `docs` with your real texts.\n",
    "Each doc has: id, topic, text.\n",
    "Topics can still be inside the domain (RAG, Chunking, Embeddings, VectorDB, HyDE, Evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7116da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 documents\n",
      "Topics: ['RAG', 'RAG', 'Chunking', 'Chunking', 'Embeddings', 'VectorDB', 'HyDE', 'Evaluation']\n"
     ]
    }
   ],
   "source": [
    "RAG_KNOWLEDGE_BASE = [\n",
    "    {\n",
    "        \"id\": \"rag_01\",\n",
    "        \"topic\": \"RAG\",\n",
    "        \"text\": \"\"\"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. Instead of relying solely on the knowledge stored in model parameters, RAG systems retrieve relevant documents from an external knowledge base and use them as context for generating responses. This approach reduces hallucinations by grounding the model's output in factual, retrieved information. The typical RAG pipeline consists of three stages: indexing documents into a searchable format, retrieving relevant passages given a query, and generating a response using the retrieved context. RAG is particularly useful for knowledge-intensive tasks where the model needs access to up-to-date or domain-specific information that wasn't part of its training data.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"rag_02\",\n",
    "        \"topic\": \"RAG\",\n",
    "        \"text\": \"\"\"The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model. This makes RAG cost-effective and flexible for enterprise applications. RAG systems also provide transparency since retrieved sources can be cited, allowing users to verify the information. Common challenges in RAG include retrieval quality, context window limitations, and handling conflicting information from multiple sources. Advanced RAG architectures may include query rewriting, multi-hop retrieval, and fusion techniques to improve answer quality.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"chunking_01\",\n",
    "        \"topic\": \"Chunking\",\n",
    "        \"text\": \"\"\"Chunking is the process of splitting documents into smaller pieces for indexing and retrieval. The chunk size significantly impacts retrieval quality. Small chunks (100-200 tokens) provide precise matches but may lose context. Large chunks (500-1000 tokens) preserve context but may include irrelevant information. Overlap between chunks helps maintain continuity across chunk boundaries. Common chunking strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundaries. The optimal chunk size depends on the embedding model's context window, the nature of queries, and the document structure.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"chunking_02\",\n",
    "        \"topic\": \"Chunking\",\n",
    "        \"text\": \"\"\"Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits. This preserves semantic coherence within chunks. Chunk overlap (typically 10-20% of chunk size) ensures that information spanning chunk boundaries is not lost. For structured documents like code or markdown, specialized splitters that respect syntax boundaries produce better results. Parent-child chunking stores both small chunks for precise retrieval and their parent documents for expanded context.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"embeddings_01\",\n",
    "        \"topic\": \"Embeddings\",\n",
    "        \"text\": \"\"\"Text embeddings are dense vector representations that capture semantic meaning. In RAG systems, both documents and queries are converted to embeddings, and similarity search finds the most relevant documents. Popular embedding models include OpenAI's text-embedding-ada-002, Sentence Transformers (like all-MiniLM-L6-v2), and BGE models. The embedding dimension affects storage requirements and search speed. Normalized embeddings allow using inner product as a similarity metric, which is computationally efficient. The choice of embedding model significantly impacts retrieval quality and should match the domain of your documents.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"vectordb_01\",\n",
    "        \"topic\": \"VectorDB\",\n",
    "        \"text\": \"\"\"Vector databases store embeddings and enable fast similarity search at scale. FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search. It supports various index types: IndexFlatIP for exact search, IndexIVF for approximate search with clustering, and IndexHNSW for graph-based approximate search. For production systems, managed vector databases like Pinecone, Weaviate, Milvus, and Qdrant provide additional features like filtering, hybrid search, and automatic scaling. The choice between exact and approximate nearest neighbor search depends on the dataset size and latency requirements.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"hyde_01\",\n",
    "        \"topic\": \"HyDE\",\n",
    "        \"text\": \"\"\"HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality by generating a hypothetical answer before retrieval. Instead of embedding the query directly, HyDE uses an LLM to generate a hypothetical document that would answer the query, then embeds this generated document for retrieval. This bridges the semantic gap between short queries and longer document passages. HyDE is particularly effective when queries are vague or use different terminology than the indexed documents. The technique adds latency due to the generation step but often significantly improves retrieval accuracy.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"evaluation_01\",\n",
    "        \"topic\": \"Evaluation\",\n",
    "        \"text\": \"\"\"Evaluating RAG systems requires measuring both retrieval quality and generation quality. Retrieval metrics include Hit@K (whether relevant documents appear in top K results), Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG). Generation quality can be measured using BLEU, ROUGE, or model-based metrics like faithfulness and relevance scores. End-to-end evaluation often uses human judgment or LLM-as-judge approaches. Important aspects to evaluate include factual accuracy, completeness, relevance to the query, and proper attribution of sources. A/B testing with real users provides the most reliable quality signal.\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "docs = RAG_KNOWLEDGE_BASE\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "print(\"Topics:\", [d[\"topic\"] for d in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516bba8",
   "metadata": {},
   "source": [
    "# Chunking (3 configs)\n",
    "\n",
    "We test multiple chunk configs and compare retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947f3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_configs = [\n",
    "    {\"chunk_size\": 200, \"overlap\": 20},\n",
    "    {\"chunk_size\": 500, \"overlap\": 50},\n",
    "    {\"chunk_size\": 800, \"overlap\": 80},\n",
    "]\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end].strip())\n",
    "        if end == len(text):\n",
    "            break\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    return [c for c in chunks if c]\n",
    "\n",
    "def make_chunks(docs, cfg):\n",
    "    chunks = []\n",
    "    for d in docs:\n",
    "        parts = chunk_text(d[\"text\"], cfg[\"chunk_size\"], cfg[\"overlap\"])\n",
    "        for i, p in enumerate(parts):\n",
    "            chunks.append({\n",
    "                \"doc_id\": d[\"id\"],\n",
    "                \"topic\": d[\"topic\"],\n",
    "                \"chunk_id\": f\"{d['id']}::c{i}\",\n",
    "                \"text\": p\n",
    "            })\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480ea15f",
   "metadata": {},
   "source": [
    "# Build FAISS Retriever (cosine similarity)\n",
    "\n",
    "Cosine is implemented as:\n",
    "- normalize embeddings\n",
    "- use inner product index (IndexFlatIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21248ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(texts, model_name):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb = model.encode(texts, normalize_embeddings=True)\n",
    "    emb = np.array(emb, dtype=\"float32\")\n",
    "\n",
    "    index = faiss.IndexFlatIP(emb.shape[1])\n",
    "    index.add(emb)\n",
    "    return model, index\n",
    "\n",
    "def retrieve(model, index, query, top_k):\n",
    "    q = model.encode([query], normalize_embeddings=True)\n",
    "    q = np.array(q, dtype=\"float32\")\n",
    "    scores, idxs = index.search(q, top_k)\n",
    "    return scores[0], idxs[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb697b",
   "metadata": {},
   "source": [
    "## Evaluation Queries\n",
    "\n",
    "We include:\n",
    "- direct queries (easy)\n",
    "- paraphrases (harder)\n",
    "- ambiguous/trap queries (should sometimes fail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab8eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries: 16\n",
      "Expected topics: {'Evaluation', 'Chunking', 'VectorDB', 'HyDE', 'RAG', 'Embeddings'}\n"
     ]
    }
   ],
   "source": [
    "eval_queries = [\n",
    "    # RAG topic\n",
    "    {\"query\": \"Explain retrieval augmented generation in simple terms\", \"expected_topic\": \"RAG\"},\n",
    "    {\"query\": \"How does retrieval reduce hallucinations?\", \"expected_topic\": \"RAG\"},\n",
    "    {\"query\": \"What is the advantage of RAG over fine-tuning?\", \"expected_topic\": \"RAG\"},\n",
    "    \n",
    "    # Chunking topic\n",
    "    {\"query\": \"What is chunking and why do we use overlap?\", \"expected_topic\": \"Chunking\"},\n",
    "    {\"query\": \"How does chunk size affect retrieval quality?\", \"expected_topic\": \"Chunking\"},\n",
    "    {\"query\": \"What is recursive character text splitting?\", \"expected_topic\": \"Chunking\"},\n",
    "    \n",
    "    # Embeddings topic\n",
    "    {\"query\": \"What are text embeddings and how do they work?\", \"expected_topic\": \"Embeddings\"},\n",
    "    {\"query\": \"Which embedding models are popular for RAG?\", \"expected_topic\": \"Embeddings\"},\n",
    "    \n",
    "    # VectorDB topic\n",
    "    {\"query\": \"What is FAISS and how does it work?\", \"expected_topic\": \"VectorDB\"},\n",
    "    {\"query\": \"What are the different FAISS index types?\", \"expected_topic\": \"VectorDB\"},\n",
    "    \n",
    "    # HyDE topic\n",
    "    {\"query\": \"What is HyDE in retrieval?\", \"expected_topic\": \"HyDE\"},\n",
    "    {\"query\": \"How does hypothetical document embedding improve search?\", \"expected_topic\": \"HyDE\"},\n",
    "    \n",
    "    # Evaluation topic\n",
    "    {\"query\": \"How do we evaluate retrieval quality?\", \"expected_topic\": \"Evaluation\"},\n",
    "    {\"query\": \"What is Mean Reciprocal Rank (MRR)?\", \"expected_topic\": \"Evaluation\"},\n",
    "    \n",
    "    # Harder / cross-topic queries\n",
    "    {\"query\": \"Why does adding retrieved context help LLM answers stay accurate?\", \"expected_topic\": \"RAG\"},\n",
    "    {\"query\": \"Does overlap always improve retrieval precision?\", \"expected_topic\": \"Chunking\"},\n",
    "]\n",
    "\n",
    "print(f\"Total queries: {len(eval_queries)}\")\n",
    "print(\"Expected topics:\", set(q[\"expected_topic\"] for q in eval_queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98773023",
   "metadata": {},
   "source": [
    "# Run Experiments: chunk configs × embedding models\n",
    "\n",
    "We compare:\n",
    "- MiniLM vs MPNet\n",
    "- chunk configs\n",
    "and compute hit@k and MRR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a1ba9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 2106.17it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 2052.54it/s, Materializing param=pooler.dense.weight]                        \n",
      "MPNetModel LOAD REPORT from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 2053.84it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 2383.03it/s, Materializing param=pooler.dense.weight]                        \n",
      "MPNetModel LOAD REPORT from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 2127.97it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1777.23it/s, Materializing param=pooler.dense.weight]                        \n",
      "MPNetModel LOAD REPORT from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>overlap</th>\n",
       "      <th>model</th>\n",
       "      <th>query</th>\n",
       "      <th>expected</th>\n",
       "      <th>hit@1</th>\n",
       "      <th>hit@3</th>\n",
       "      <th>hit@5</th>\n",
       "      <th>mrr</th>\n",
       "      <th>top_topics</th>\n",
       "      <th>top1_score</th>\n",
       "      <th>gap_1_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>Explain retrieval augmented generation in simp...</td>\n",
       "      <td>RAG</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[RAG, HyDE, HyDE, RAG, RAG]</td>\n",
       "      <td>0.740879</td>\n",
       "      <td>0.257921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>How does retrieval reduce hallucinations?</td>\n",
       "      <td>RAG</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[RAG, HyDE, Embeddings, RAG, RAG]</td>\n",
       "      <td>0.552341</td>\n",
       "      <td>0.242185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>What is the advantage of RAG over fine-tuning?</td>\n",
       "      <td>RAG</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[RAG, Evaluation, RAG, RAG, RAG]</td>\n",
       "      <td>0.782866</td>\n",
       "      <td>0.226946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>What is chunking and why do we use overlap?</td>\n",
       "      <td>Chunking</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Chunking, Chunking, Chunking, Chunking, Chunk...</td>\n",
       "      <td>0.759435</td>\n",
       "      <td>0.134919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>How does chunk size affect retrieval quality?</td>\n",
       "      <td>Chunking</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Chunking, Chunking, Chunking, Chunking, Embed...</td>\n",
       "      <td>0.728739</td>\n",
       "      <td>0.073879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_size  overlap   model  \\\n",
       "0         200       20  MiniLM   \n",
       "1         200       20  MiniLM   \n",
       "2         200       20  MiniLM   \n",
       "3         200       20  MiniLM   \n",
       "4         200       20  MiniLM   \n",
       "\n",
       "                                               query  expected  hit@1  hit@3  \\\n",
       "0  Explain retrieval augmented generation in simp...       RAG    1.0    1.0   \n",
       "1          How does retrieval reduce hallucinations?       RAG    1.0    1.0   \n",
       "2     What is the advantage of RAG over fine-tuning?       RAG    1.0    1.0   \n",
       "3        What is chunking and why do we use overlap?  Chunking    1.0    1.0   \n",
       "4      How does chunk size affect retrieval quality?  Chunking    1.0    1.0   \n",
       "\n",
       "   hit@5  mrr                                         top_topics  top1_score  \\\n",
       "0    1.0  1.0                        [RAG, HyDE, HyDE, RAG, RAG]    0.740879   \n",
       "1    1.0  1.0                  [RAG, HyDE, Embeddings, RAG, RAG]    0.552341   \n",
       "2    1.0  1.0                   [RAG, Evaluation, RAG, RAG, RAG]    0.782866   \n",
       "3    1.0  1.0  [Chunking, Chunking, Chunking, Chunking, Chunk...    0.759435   \n",
       "4    1.0  1.0  [Chunking, Chunking, Chunking, Chunking, Embed...    0.728739   \n",
       "\n",
       "    gap_1_2  \n",
       "0  0.257921  \n",
       "1  0.242185  \n",
       "2  0.226946  \n",
       "3  0.134919  \n",
       "4  0.073879  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "    \"MiniLM\": \"all-MiniLM-L6-v2\",\n",
    "    \"MPNet\": \"all-mpnet-base-v2\"\n",
    "}\n",
    "\n",
    "def hit_at_k(expected, retrieved_topics, k):\n",
    "    return 1.0 if expected in retrieved_topics[:k] else 0.0\n",
    "\n",
    "def mrr(expected, retrieved_topics):\n",
    "    for i, t in enumerate(retrieved_topics, start=1):\n",
    "        if t == expected:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "rows = []\n",
    "debug = []\n",
    "\n",
    "for cfg in chunk_configs:\n",
    "    chunks = make_chunks(docs, cfg)\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    topics = [c[\"topic\"] for c in chunks]\n",
    "\n",
    "    for model_label, model_name in models.items():\n",
    "        model, index = build_index(texts, model_name)\n",
    "\n",
    "        for q in eval_queries:\n",
    "            scores, idxs = retrieve(model, index, q[\"query\"], top_k=5)\n",
    "\n",
    "            retrieved_topics = [topics[i] for i in idxs]\n",
    "            gap_1_2 = float(scores[0] - scores[1]) if len(scores) > 1 else None\n",
    "\n",
    "            rows.append({\n",
    "                \"chunk_size\": cfg[\"chunk_size\"],\n",
    "                \"overlap\": cfg[\"overlap\"],\n",
    "                \"model\": model_label,\n",
    "                \"query\": q[\"query\"],\n",
    "                \"expected\": q[\"expected_topic\"],\n",
    "                \"hit@1\": hit_at_k(q[\"expected_topic\"], retrieved_topics, 1),\n",
    "                \"hit@3\": hit_at_k(q[\"expected_topic\"], retrieved_topics, 3),\n",
    "                \"hit@5\": hit_at_k(q[\"expected_topic\"], retrieved_topics, 5),\n",
    "                \"mrr\": mrr(q[\"expected_topic\"], retrieved_topics),\n",
    "                \"top_topics\": retrieved_topics,\n",
    "                \"top1_score\": float(scores[0]),\n",
    "                \"gap_1_2\": gap_1_2\n",
    "            })\n",
    "\n",
    "            # debug: store top3 chunks for manual check\n",
    "            for rank in range(3):\n",
    "                debug.append({\n",
    "                    \"chunk_size\": cfg[\"chunk_size\"],\n",
    "                    \"overlap\": cfg[\"overlap\"],\n",
    "                    \"model\": model_label,\n",
    "                    \"query\": q[\"query\"],\n",
    "                    \"rank\": rank+1,\n",
    "                    \"score\": float(scores[rank]),\n",
    "                    \"topic\": topics[idxs[rank]],\n",
    "                    \"chunk_id\": chunks[idxs[rank]][\"chunk_id\"],\n",
    "                    \"text_preview\": chunks[idxs[rank]][\"text\"][:250].replace(\"\\n\", \" \")\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df_debug = pd.DataFrame(debug)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12371bd",
   "metadata": {},
   "source": [
    "# Results Summary (compare configs and models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4a91c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>overlap</th>\n",
       "      <th>model</th>\n",
       "      <th>hit@1</th>\n",
       "      <th>hit@3</th>\n",
       "      <th>hit@5</th>\n",
       "      <th>mrr</th>\n",
       "      <th>top1_score</th>\n",
       "      <th>gap_1_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>800</td>\n",
       "      <td>80</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.533364</td>\n",
       "      <td>0.138228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>800</td>\n",
       "      <td>80</td>\n",
       "      <td>MPNet</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.474881</td>\n",
       "      <td>0.122695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.603624</td>\n",
       "      <td>0.114243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>MPNet</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.498471</td>\n",
       "      <td>0.068708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>MiniLM</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.544313</td>\n",
       "      <td>0.115198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>MPNet</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.864583</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>0.104315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_size  overlap   model   hit@1   hit@3   hit@5       mrr  top1_score  \\\n",
       "5         800       80  MiniLM  0.9375  1.0000  1.0000  0.968750    0.533364   \n",
       "4         800       80   MPNet  0.9375  1.0000  1.0000  0.958333    0.474881   \n",
       "1         200       20  MiniLM  0.8750  1.0000  1.0000  0.937500    0.603624   \n",
       "2         500       50   MPNet  0.8125  1.0000  1.0000  0.906250    0.498471   \n",
       "3         500       50  MiniLM  0.8125  1.0000  1.0000  0.895833    0.544313   \n",
       "0         200       20   MPNet  0.8125  0.9375  0.9375  0.864583    0.568848   \n",
       "\n",
       "    gap_1_2  \n",
       "5  0.138228  \n",
       "4  0.122695  \n",
       "1  0.114243  \n",
       "2  0.068708  \n",
       "3  0.115198  \n",
       "0  0.104315  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = (\n",
    "    df.groupby([\"chunk_size\", \"overlap\", \"model\"])\n",
    "      .agg({\"hit@1\":\"mean\",\"hit@3\":\"mean\",\"hit@5\":\"mean\",\"mrr\":\"mean\",\"top1_score\":\"mean\",\"gap_1_2\":\"mean\"})\n",
    "      .reset_index()\n",
    "      .sort_values([\"mrr\",\"hit@1\"], ascending=False)\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b911d3",
   "metadata": {},
   "source": [
    "# Manual Retrieval Evaluation \n",
    "\n",
    "- ✅ relevant \n",
    "- ⚠️ partially relevant \n",
    "- ❌ irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bc9f4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>topic</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text_preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1</td>\n",
       "      <td>0.449827</td>\n",
       "      <td>HyDE</td>\n",
       "      <td>hyde_01::c1</td>\n",
       "      <td>queries are vague or use different terminology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2</td>\n",
       "      <td>0.393171</td>\n",
       "      <td>Chunking</td>\n",
       "      <td>chunking_01::c0</td>\n",
       "      <td>Chunking is the process of splitting documents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>3</td>\n",
       "      <td>0.377681</td>\n",
       "      <td>RAG</td>\n",
       "      <td>rag_02::c0</td>\n",
       "      <td>The key advantage of RAG over fine-tuning is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rank     score     topic         chunk_id  \\\n",
       "141     1  0.449827      HyDE      hyde_01::c1   \n",
       "142     2  0.393171  Chunking  chunking_01::c0   \n",
       "143     3  0.377681       RAG       rag_02::c0   \n",
       "\n",
       "                                          text_preview  \n",
       "141  queries are vague or use different terminology...  \n",
       "142  Chunking is the process of splitting documents...  \n",
       "143  The key advantage of RAG over fine-tuning is t...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_debug(df_debug, chunk_size, overlap, model, query):\n",
    "    view = df_debug[\n",
    "        (df_debug[\"chunk_size\"]==chunk_size) &\n",
    "        (df_debug[\"overlap\"]==overlap) &\n",
    "        (df_debug[\"model\"]==model) &\n",
    "        (df_debug[\"query\"]==query)\n",
    "    ].sort_values(\"rank\")\n",
    "    return view[[\"rank\",\"score\",\"topic\",\"chunk_id\",\"text_preview\"]]\n",
    "\n",
    "show_debug(df_debug, 500, 50, \"MiniLM\", \"Does overlap always improve retrieval precision?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad2410",
   "metadata": {},
   "source": [
    "✅ Manual Retrieval Evaluation — Case: RAG (Baseline Definition)\n",
    "Query\n",
    "\n",
    "Explain retrieval augmented generation in simple terms\n",
    "\n",
    "Expected topic: RAG\n",
    "\n",
    "Top-1 result\n",
    "\n",
    "Rank: 1\n",
    "Score: 0.728\n",
    "Topic: RAG\n",
    "Chunk: rag_01::c0\n",
    "\n",
    "Verdict: ✅ Relevant\n",
    "\n",
    "Reason:\n",
    "The retrieved chunk provides a clear and concise definition of Retrieval-Augmented Generation, explaining its purpose and how it combines retrieval with generation. The explanation is suitable for a “simple terms” request and directly answers the question.\n",
    "\n",
    "Top-2 result\n",
    "\n",
    "Rank: 2\n",
    "Score: 0.584\n",
    "Topic: HyDE\n",
    "Chunk: hyde_01::c1\n",
    "\n",
    "Verdict: ⚠️ Partially relevant\n",
    "\n",
    "Reason:\n",
    "This chunk discusses challenges related to query–document semantic mismatch and advanced retrieval techniques. While conceptually related to retrieval, it does not explain RAG itself and is too specific for an introductory explanation.\n",
    "\n",
    "Top-3 result\n",
    "\n",
    "Rank: 3\n",
    "Score: 0.520\n",
    "Topic: RAG\n",
    "Chunk: rag_02::c1\n",
    "\n",
    "Verdict: ⚠️ Partially relevant\n",
    "\n",
    "Reason:\n",
    "The chunk refers to advanced RAG architectures and extensions, which are related to the topic but assume prior knowledge and do not focus on a simple, high-level explanation.\n",
    "\n",
    "Observation\n",
    "\n",
    "This query represents a direct definition-style baseline.\n",
    "The retriever correctly ranks a fully relevant RAG definition at top-1, while semantically related but more advanced concepts (HyDE and advanced RAG variants) appear in lower ranks. This indicates semantic retrieval rather than strict keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac5a4933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>topic</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text_preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>0.728413</td>\n",
       "      <td>RAG</td>\n",
       "      <td>rag_01::c0</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) is a tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2</td>\n",
       "      <td>0.584476</td>\n",
       "      <td>HyDE</td>\n",
       "      <td>hyde_01::c1</td>\n",
       "      <td>queries are vague or use different terminology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>3</td>\n",
       "      <td>0.519525</td>\n",
       "      <td>RAG</td>\n",
       "      <td>rag_02::c1</td>\n",
       "      <td>ed RAG architectures may include query rewriti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank     score topic     chunk_id  \\\n",
       "96     1  0.728413   RAG   rag_01::c0   \n",
       "97     2  0.584476  HyDE  hyde_01::c1   \n",
       "98     3  0.519525   RAG   rag_02::c1   \n",
       "\n",
       "                                         text_preview  \n",
       "96  Retrieval-Augmented Generation (RAG) is a tech...  \n",
       "97  queries are vague or use different terminology...  \n",
       "98  ed RAG architectures may include query rewriti...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_debug(df_debug, chunk_size, overlap, model, query):\n",
    "    view = df_debug[\n",
    "        (df_debug[\"chunk_size\"]==chunk_size) &\n",
    "        (df_debug[\"overlap\"]==overlap) &\n",
    "        (df_debug[\"model\"]==model) &\n",
    "        (df_debug[\"query\"]==query)\n",
    "    ].sort_values(\"rank\")\n",
    "    return view[[\"rank\",\"score\",\"topic\",\"chunk_id\",\"text_preview\"]]\n",
    "\n",
    "show_debug(df_debug, 500, 50, \"MiniLM\", \"Explain retrieval augmented generation in simple terms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd119c3",
   "metadata": {},
   "source": [
    "✅ Manual Retrieval Evaluation — Case: RAG Definition (Confirmed)\n",
    "Query\n",
    "\n",
    "Explain retrieval augmented generation in simple terms\n",
    "\n",
    "Top-1 result\n",
    "\n",
    "Rank: 1\n",
    "Score: 0.728\n",
    "Topic: RAG\n",
    "Chunk: rag_01::c0\n",
    "\n",
    "Verdict: ✅ Relevant\n",
    "\n",
    "Reason:\n",
    "The retrieved chunk explicitly defines Retrieval-Augmented Generation, describing its purpose, how it combines retrieval with generation, and why it is useful. The explanation aligns well with the request for a simple, introductory description.\n",
    "\n",
    "Top-2 result\n",
    "\n",
    "Rank: 2\n",
    "Score: 0.584\n",
    "Topic: HyDE\n",
    "Chunk: hyde_01::c1\n",
    "\n",
    "Verdict: ⚠️ Partially relevant\n",
    "\n",
    "Reason:\n",
    "This chunk discusses issues related to vague queries and terminology mismatch, which are relevant to retrieval in general, but it does not explain RAG itself. It provides contextual background but does not directly answer the question.\n",
    "\n",
    "Top-3 result\n",
    "\n",
    "Rank: 3\n",
    "Score: 0.520\n",
    "Topic: RAG\n",
    "Chunk: rag_02::c1\n",
    "\n",
    "Verdict: ⚠️ Partially relevant\n",
    "\n",
    "Reason:\n",
    "The chunk mentions advanced RAG architectures and techniques but assumes prior knowledge and focuses on extensions rather than a simple explanation.\n",
    "\n",
    "Observation\n",
    "\n",
    "This query represents a direct, definition-style baseline.\n",
    "The retriever correctly ranks a highly relevant RAG definition at top-1, while related but more advanced retrieval concepts (HyDE, advanced RAG architectures) appear in lower ranks, indicating semantic rather than keyword-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75ac5faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>topic</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text_preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>0.346955</td>\n",
       "      <td>RAG</td>\n",
       "      <td>rag_01::c0</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) is a tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "      <td>0.236940</td>\n",
       "      <td>HyDE</td>\n",
       "      <td>hyde_01::c1</td>\n",
       "      <td>queries are vague or use different terminology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>3</td>\n",
       "      <td>0.212321</td>\n",
       "      <td>RAG</td>\n",
       "      <td>rag_01::c1</td>\n",
       "      <td>stages: indexing documents into a searchable f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rank     score topic     chunk_id  \\\n",
       "99      1  0.346955   RAG   rag_01::c0   \n",
       "100     2  0.236940  HyDE  hyde_01::c1   \n",
       "101     3  0.212321   RAG   rag_01::c1   \n",
       "\n",
       "                                          text_preview  \n",
       "99   Retrieval-Augmented Generation (RAG) is a tech...  \n",
       "100  queries are vague or use different terminology...  \n",
       "101  stages: indexing documents into a searchable f...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_debug(df_debug, chunk_size, overlap, model, query):\n",
    "    view = df_debug[\n",
    "        (df_debug[\"chunk_size\"]==chunk_size) &\n",
    "        (df_debug[\"overlap\"]==overlap) &\n",
    "        (df_debug[\"model\"]==model) &\n",
    "        (df_debug[\"query\"]==query)\n",
    "    ].sort_values(\"rank\")\n",
    "    return view[[\"rank\",\"score\",\"topic\",\"chunk_id\",\"text_preview\"]]\n",
    "\n",
    "show_debug(df_debug, 500, 50, \"MiniLM\", \"How does retrieval reduce hallucinations?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9827e2",
   "metadata": {},
   "source": [
    "✅ Manual Retrieval Evaluation — Case: Hallucinations (RAG Grounding)\n",
    "Query\n",
    "\n",
    "How does retrieval reduce hallucinations?\n",
    "\n",
    "Expected topic: RAG\n",
    "\n",
    "Top-1 result\n",
    "\n",
    "Rank: 1\n",
    "Score: 0.347\n",
    "Topic: RAG\n",
    "Chunk: rag_01::c0\n",
    "\n",
    "Verdict: ⚠️ Partially relevant\n",
    "\n",
    "Reason:\n",
    "The chunk explains what RAG is and mentions grounding outputs in retrieved information, which is related to hallucination reduction. However, it does not directly explain how retrieval reduces hallucinations (e.g., by constraining generation to retrieved evidence and reducing reliance on parametric memory). The answer would still require an explicit mechanism-focused explanation.\n",
    "\n",
    "Top-2 result\n",
    "\n",
    "Rank: 2\n",
    "Score: 0.237\n",
    "Topic: HyDE\n",
    "Chunk: hyde_01::c1\n",
    "\n",
    "Verdict: ❌ Irrelevant\n",
    "\n",
    "Reason:\n",
    "This chunk discusses vague queries and terminology mismatch in retrieval, which is not directly about hallucinations or grounding. It does not answer the question.\n",
    "\n",
    "Top-3 result\n",
    "\n",
    "Rank: 3\n",
    "Score: 0.212\n",
    "Topic: RAG\n",
    "Chunk: rag_01::c1\n",
    "\n",
    "Verdict: ⚠️ Partially relevant\n",
    "\n",
    "Reason:\n",
    "This chunk lists stages of the RAG pipeline (indexing/retrieval/generation) but does not explicitly connect retrieval to hallucination reduction. It is contextually related but not a direct answer.\n",
    "\n",
    "Observation\n",
    "\n",
    "This query reveals a realistic retrieval limitation: the retriever selects generally relevant RAG definition/pipeline chunks, but not a chunk that explicitly explains the mechanism of hallucination reduction. The relatively low top-1 score (0.347) indicates weaker semantic alignment compared to direct definition questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "639a1a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>topic</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text_preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "      <td>0.629680</td>\n",
       "      <td>HyDE</td>\n",
       "      <td>hyde_01::c0</td>\n",
       "      <td>HyDE (Hypothetical Document Embeddings) is an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2</td>\n",
       "      <td>0.297483</td>\n",
       "      <td>RAG</td>\n",
       "      <td>rag_01::c1</td>\n",
       "      <td>stages: indexing documents into a searchable f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>3</td>\n",
       "      <td>0.265558</td>\n",
       "      <td>RAG</td>\n",
       "      <td>rag_01::c0</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) is a tech...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rank     score topic     chunk_id  \\\n",
       "126     1  0.629680  HyDE  hyde_01::c0   \n",
       "127     2  0.297483   RAG   rag_01::c1   \n",
       "128     3  0.265558   RAG   rag_01::c0   \n",
       "\n",
       "                                          text_preview  \n",
       "126  HyDE (Hypothetical Document Embeddings) is an ...  \n",
       "127  stages: indexing documents into a searchable f...  \n",
       "128  Retrieval-Augmented Generation (RAG) is a tech...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_debug(df_debug, chunk_size, overlap, model, query):\n",
    "    view = df_debug[\n",
    "        (df_debug[\"chunk_size\"]==chunk_size) &\n",
    "        (df_debug[\"overlap\"]==overlap) &\n",
    "        (df_debug[\"model\"]==model) &\n",
    "        (df_debug[\"query\"]==query)\n",
    "    ].sort_values(\"rank\")\n",
    "    return view[[\"rank\",\"score\",\"topic\",\"chunk_id\",\"text_preview\"]]\n",
    "\n",
    "show_debug(df_debug, 500, 50, \"MiniLM\", \"What is HyDE in retrieval?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ee607",
   "metadata": {},
   "source": [
    "✅ Manual Retrieval Evaluation — Case: HyDE Definition\n",
    "Query\n",
    "\n",
    "What is HyDE in retrieval?\n",
    "\n",
    "Expected topic: HyDE\n",
    "\n",
    "Top-1 result\n",
    "\n",
    "Rank: 1\n",
    "Score: 0.630\n",
    "Topic: HyDE\n",
    "Chunk: hyde_01::c0\n",
    "\n",
    "Verdict: ✅ Relevant\n",
    "\n",
    "Reason:\n",
    "The retrieved chunk explicitly explains HyDE (Hypothetical Document Embeddings), including the idea of generating a hypothetical document before retrieval and using it for embedding-based search. This directly answers the question and provides the correct definition.\n",
    "\n",
    "Top-2 result\n",
    "\n",
    "Rank: 2\n",
    "Score: 0.297\n",
    "Topic: RAG\n",
    "Chunk: rag_01::c1\n",
    "\n",
    "Verdict: ⚠️ Partially relevant\n",
    "\n",
    "Reason:\n",
    "This chunk describes the general RAG pipeline stages (indexing, retrieval, generation) but does not mention HyDE specifically. It provides related retrieval context but does not answer the question.\n",
    "\n",
    "Top-3 result\n",
    "\n",
    "Rank: 3\n",
    "Score: 0.266\n",
    "Topic: RAG\n",
    "Chunk: rag_01::c0\n",
    "\n",
    "Verdict: ❌ Irrelevant\n",
    "\n",
    "Reason:\n",
    "The chunk explains Retrieval-Augmented Generation at a high level without referencing HyDE or hypothetical document embeddings. It does not contribute to answering the question.\n",
    "\n",
    "Observation\n",
    "\n",
    "For a direct definition-style query, the retriever correctly places the HyDE definition at rank 1 with a clear score margin. Related RAG content appears in lower ranks due to semantic proximity between advanced retrieval techniques, but does not interfere with the correct top-1 result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a0dc8",
   "metadata": {},
   "source": [
    "# Week 2 Summary (Final)\n",
    "\n",
    "✅ Implemented FAISS local vector store  \n",
    "✅ Implemented chunking with multiple configs (chunk_size/overlap)  \n",
    "✅ Compared MiniLM vs MPNet retrievers  \n",
    "✅ Evaluated retrieval with hit@k and MRR  \n",
    "✅ Performed manual retrieval inspection and summarized findings  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a5ccb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1886.13it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO_ANSWER Detection Test\n",
      "==================================================\n",
      "✅ 'How to create a VM in GCP?...' -> NO_ANSWER (score=0.037)\n",
      "✅ 'How do I merge a git branch safely?...' -> NO_ANSWER (score=0.127)\n",
      "✅ 'What is the best recipe for pasta carbon...' -> NO_ANSWER (score=0.147)\n",
      "✅ 'How to fix a flat tire on a bicycle?...' -> NO_ANSWER (score=0.024)\n"
     ]
    }
   ],
   "source": [
    "# NO_ANSWER detection: out-of-domain queries should have low scores\n",
    "T_SCORE = 0.45\n",
    "\n",
    "no_answer_queries = [\n",
    "    \"How to create a VM in GCP?\",\n",
    "    \"How do I merge a git branch safely?\",\n",
    "    \"What is the best recipe for pasta carbonara?\",\n",
    "    \"How to fix a flat tire on a bicycle?\",\n",
    "]\n",
    "\n",
    "cfg = {\"chunk_size\": 500, \"overlap\": 50}\n",
    "chunks = make_chunks(docs, cfg)\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "\n",
    "model, index = build_index(texts, models[\"MiniLM\"])\n",
    "\n",
    "print(\"NO_ANSWER Detection Test\")\n",
    "print(\"=\" * 50)\n",
    "for q in no_answer_queries:\n",
    "    scores, idxs = retrieve(model, index, q, top_k=1)\n",
    "    if scores[0] < T_SCORE:\n",
    "        print(f\"✅ '{q[:40]}...' -> NO_ANSWER (score={scores[0]:.3f})\")\n",
    "    else:\n",
    "        print(f\"⚠️ '{q[:40]}...' -> FALSE POSITIVE (score={scores[0]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7cf03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
