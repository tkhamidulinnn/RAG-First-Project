{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# Week 2: Building the Local Retrieval Pipeline\n",
        "\n",
        "**Scope:** local retrieval pipeline only: load PDFs -> chunk -> embed -> FAISS index -> retrieve.\n",
        "\n",
        "**Input from Week 1:**\n",
        "- **Champion model:** **MPNet** (`all-mpnet-base-v2`) — chosen in Week 1 for better semantic separation.\n",
        "- **Champion metric:** **cosine similarity**.\n",
        "- Week 2 uses only the champion: one embedding model (MPNet) and one metric (cosine). An optional section later compares with MiniLM for reference.\n",
        "\n",
        "**Week 2 objectives covered:**\n",
        "1. Local vector store (FAISS).\n",
        "2. Chunking with different chunk sizes/overlap.\n",
        "3. Retriever + manual quality evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279c89a2",
      "metadata": {},
      "source": [
        "### Notebook structure\n",
        "\n",
        "1. **Setup** — imports, paths, and Week 1 metric helpers (dot, cosine, Euclidean); retrieval uses cosine via normalized embeddings + FAISS.\n",
        "2. **Experiment 1** — load PDFs from `data/` (RAG, GIT, GCP); each subfolder is a topic; we get page-level docs with metadata.\n",
        "3. **Chunking** — two strategies (A: 300/50, B: 800/100) with `RecursiveCharacterTextSplitter`; compare chunk counts per topic.\n",
        "4. **VectorStore & RAG** — `VectorStore.build_index()` / `retrieve()`; `RAG.retrieve()` returns `(score, chunk)` for evaluation.\n",
        "5. **Retrieval experiment** — 5 queries, top-3 for A and B; manual evaluation table (expected topic vs top-1/top-3).\n",
        "6. **Strategy choice** — aggregate top1_accuracy, top3_hit_rate, avg_top1_score; pick best strategy; save CSVs to `artifacts/`.\n",
        "7. **Analysis** — chunk size/overlap effects; alignment with Week 1 (cosine, MiniLM); handoff to Week 3.\n",
        "8. **Experiment 4** — compare chunk sizes (300/30, 500/50, 800/80) with fixed model and metric.\n",
        "9. **Experiment 5** — compare MiniLM vs MPNet on the chosen chunk strategy (separate cells to avoid memory issues).\n",
        "10. **Conclusions** — chosen strategy, champion model (MPNet), metric (cosine); Week 3 reuses retriever and adds LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cell-1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS configured: Using CPU mode\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "faiss.omp_set_num_threads(4)\n",
        "print(\"FAISS configured: Using CPU mode\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "## Pipeline Architecture\n",
        "\n",
        "```\n",
        "PDF folder -> extract text -> chunk -> embed (MPNet) -> index (FAISS) -> retrieve (top-k)\n",
        "```\n",
        "\n",
        "| Step | What we use | Why |\n",
        "|------|-------------|-----|\n",
        "| Load | `PyPDFLoader` | Local PDF parsing with metadata |\n",
        "| Chunk | `RecursiveCharacterTextSplitter` | Compare different chunk strategies |\n",
        "| Embed | MPNet, `normalize_embeddings=True` | Week 1 champion model |\n",
        "| Index | FAISS `IndexFlatIP` | Inner product on normalized vectors = cosine |\n",
        "| Retrieve | `index.search` | Fast top-k retrieval |\n",
        "\n",
        "Cosine is implemented via `normalize_embeddings=True` + `IndexFlatIP`, consistent with Week 1 metric decision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e4f7e89",
      "metadata": {},
      "source": [
        "## Paths and directories\n",
        "\n",
        "- **`PROJECT_ROOT`** — project root (parent of `notebooks/`).\n",
        "- **`DATA_DIR`** — `data/` with subfolders RAG, GIT, GCP containing PDFs.\n",
        "- **`ARTIFACTS_DIR`** — `artifacts/` for saving evaluation CSVs (chunk strategy, model comparison, final decisions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cell-3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/tkhamidulin/Desktop/First Project - RAG\n",
            "Data dir: /Users/tkhamidulin/Desktop/First Project - RAG/data (exists: True)\n",
            "Artifacts dir: /Users/tkhamidulin/Desktop/First Project - RAG/artifacts\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
        "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Data dir: {DATA_DIR} (exists: {DATA_DIR.exists()})\")\n",
        "print(f\"Artifacts dir: {ARTIFACTS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc30935",
      "metadata": {},
      "source": [
        "## Similarity metric functions (from Week 1, explicit numpy)\n",
        "\n",
        "Week 1 chose **cosine similarity** as the champion metric. We keep dot, cosine, and Euclidean here for reference; the FAISS pipeline uses cosine via `normalize_embeddings=True` and `IndexFlatIP` (inner product on unit vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1305adb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def dot_product(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.sum(a * b))\n",
        "\n",
        "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    dot = np.sum(a * b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    return float(dot / (norm_a * norm_b))\n",
        "\n",
        "def euclidean_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.linalg.norm(a - b))\n",
        "\n",
        "# Week 2 retrieval uses cosine (inner product on L2-normalized embeddings in FAISS)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "---\n",
        "## Experiment 1: Load PDF Documents\n",
        "\n",
        "**Goal:** Load real PDF documents from the `data/` folder.\n",
        "\n",
        "**Structure:**\n",
        "```\n",
        "data/\n",
        "├── RAG/     (3 PDFs about RAG, HyDE, LangChain)\n",
        "├── GIT/     (3 PDFs about Git)\n",
        "└── GCP/     (3 PDFs about Google Cloud)\n",
        "```\n",
        "\n",
        "Each subfolder name becomes the **topic** for evaluation.\n",
        "\n",
        "**Conclusion:** We obtain a flat list of page-level documents with `topic`, `source`, and `page` metadata, ready for chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cell-5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[GCP]\n",
            "  + gcloud-cheat-sheet.pdf (2 pages)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "incorrect startxref pointer(1)\n",
            "parsing for Object Streams\n",
            "Error -3 while decompressing data: incorrect header check\n",
            "found 0 objects within Object(775,0) whereas 200 expected\n",
            "Error -3 while decompressing data: incorrect header check\n",
            "found 0 objects within Object(776,0) whereas 20 expected\n",
            "Cannot find \"/Root\" key in trailer\n",
            "Searching object with \"/Catalog\" key\n",
            "Ignoring wrong pointing object 11 0 (offset 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  + google_security_wp.pdf (18 pages)\n",
            "  ERROR: A-Complete-Guide-to-the-Google-Cloud-Platform.pdf - Cannot find Root object in pdf\n",
            "\n",
            "[GIT]\n",
            "  + GitGuide.pdf (8 pages)\n",
            "  + git-cheat-sheet-education.pdf (2 pages)\n",
            "  + How_to_Git.pdf (45 pages)\n",
            "\n",
            "[RAG]\n",
            "  + Advanced RAG — Improving retrieval using Hypothetical Document Embeddings(HyDE) _ by Plaban Nayak _ AI Planet.pdf (19 pages)\n",
            "  + Retrieval - Docs by LangChain.pdf (3 pages)\n",
            "  + Retrieval-Augmented Generation (RAG) _ Pinecone.pdf (4 pages)\n",
            "\n",
            "Loaded: 101 pages from 3 topics: ['GCP', 'GIT', 'RAG']\n"
          ]
        }
      ],
      "source": [
        "def load_pdfs(data_dir: Path) -> list:\n",
        "    \"\"\"\n",
        "    Load all PDFs from topic subfolders.\n",
        "    Adds metadata: topic (folder name), source (filename), page.\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "    topics_found = []\n",
        "    \n",
        "    for topic_dir in sorted(data_dir.iterdir()):\n",
        "        if not topic_dir.is_dir():\n",
        "            continue\n",
        "        \n",
        "        topic = topic_dir.name\n",
        "        pdf_files = list(topic_dir.glob(\"*.pdf\"))\n",
        "        \n",
        "        if not pdf_files:\n",
        "            print(f\"  WARNING: No PDFs in {topic}/\")\n",
        "            continue\n",
        "        \n",
        "        topics_found.append(topic)\n",
        "        print(f\"\\n[{topic}]\")\n",
        "        \n",
        "        for pdf_path in pdf_files:\n",
        "            try:\n",
        "                loader = PyPDFLoader(str(pdf_path))\n",
        "                pdf_docs = loader.load()\n",
        "                for doc in pdf_docs:\n",
        "                    doc.metadata[\"topic\"] = topic\n",
        "                    doc.metadata[\"source\"] = pdf_path.name\n",
        "                    docs.append(doc)\n",
        "                print(f\"  + {pdf_path.name} ({len(pdf_docs)} pages)\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ERROR: {pdf_path.name} - {e}\")\n",
        "    \n",
        "    print(f\"\\nLoaded: {len(docs)} pages from {len(topics_found)} topics: {topics_found}\")\n",
        "    return docs\n",
        "\n",
        "\n",
        "# Load all PDFs\n",
        "raw_docs = load_pdfs(DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cell-6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample document:\n",
            "  Topic: GCP\n",
            "  Source: gcloud-cheat-sheet.pdf\n",
            "  Page: 0\n",
            "  Content preview: gcloud init\n",
            "I n i t i a l i z e ,  a u t h o r i z e ,  a n d  c o n fi g u r e  g c l o u d\n",
            "gcloud version\n",
            "D i s p l a y  ve r s i o n  a n d  i n s t a l l e d  c o m p o n e n t s\n",
            "gcloud components...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Sample document:\")\n",
        "print(f\"  Topic: {raw_docs[0].metadata['topic']}\")\n",
        "print(f\"  Source: {raw_docs[0].metadata['source']}\")\n",
        "print(f\"  Page: {raw_docs[0].metadata.get('page', 0)}\")\n",
        "print(f\"  Content preview: {raw_docs[0].page_content[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {},
      "source": [
        "---\n",
        "## Chunking: Two Strategies\n",
        "\n",
        "**Goal:** Split documents into chunks so we can compare how chunk size and overlap affect retrieval.\n",
        "\n",
        "**Why RecursiveCharacterTextSplitter?**\n",
        "- Splits on natural boundaries (paragraphs, sentences) when possible\n",
        "- Falls back to character-level splits if needed\n",
        "- Preserves semantic coherence better than fixed-size splits\n",
        "\n",
        "**Two configurations (as required):**\n",
        "- **Strategy A (small):** `chunk_size=300`, `chunk_overlap=50` — more chunks, finer granularity.\n",
        "- **Strategy B (larger):** `chunk_size=800`, `chunk_overlap=100` — fewer chunks, more context per chunk.\n",
        "\n",
        "**Conclusion:** Strategy A produces more chunks (finer granularity); Strategy B produces fewer, longer chunks. We index both and compare retrieval quality in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cell-8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original documents: 101 pages\n",
            "Strategy A ({'chunk_size': 300, 'chunk_overlap': 50}): 833 chunks\n",
            "Strategy B ({'chunk_size': 800, 'chunk_overlap': 100}): 330 chunks\n"
          ]
        }
      ],
      "source": [
        "def chunk_documents(docs, chunk_size: int, chunk_overlap: int, separators=None):\n",
        "    \"\"\"\n",
        "    Split a list of LangChain documents into chunks.\n",
        "    Returns list of documents (each has page_content and metadata).\n",
        "    \"\"\"\n",
        "    if separators is None:\n",
        "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=separators,\n",
        "    )\n",
        "    return splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "# Two chunk strategies (Strategy A = small, Strategy B = larger)\n",
        "CHUNK_STRATEGY_A = {\"chunk_size\": 300, \"chunk_overlap\": 50}\n",
        "CHUNK_STRATEGY_B = {\"chunk_size\": 800, \"chunk_overlap\": 100}\n",
        "\n",
        "chunks_a = chunk_documents(raw_docs, **CHUNK_STRATEGY_A)\n",
        "chunks_b = chunk_documents(raw_docs, **CHUNK_STRATEGY_B)\n",
        "\n",
        "print(f\"Original documents: {len(raw_docs)} pages\")\n",
        "print(f\"Strategy A ({CHUNK_STRATEGY_A}): {len(chunks_a)} chunks\")\n",
        "print(f\"Strategy B ({CHUNK_STRATEGY_B}): {len(chunks_b)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cell-9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks per topic — Strategy A (small):\n",
            "  GCP: 202 chunks\n",
            "  GIT: 145 chunks\n",
            "  RAG: 486 chunks\n",
            "\n",
            "Chunks per topic — Strategy B (larger):\n",
            "  GCP: 78 chunks\n",
            "  GIT: 69 chunks\n",
            "  RAG: 183 chunks\n"
          ]
        }
      ],
      "source": [
        "def chunks_per_topic(chunks):\n",
        "    out = {}\n",
        "    for c in chunks:\n",
        "        t = c.metadata[\"topic\"]\n",
        "        out[t] = out.get(t, 0) + 1\n",
        "    return out\n",
        "\n",
        "print(\"Chunks per topic — Strategy A (small):\")\n",
        "for topic, count in sorted(chunks_per_topic(chunks_a).items()):\n",
        "    print(f\"  {topic}: {count} chunks\")\n",
        "print(\"\\nChunks per topic — Strategy B (larger):\")\n",
        "for topic, count in sorted(chunks_per_topic(chunks_b).items()):\n",
        "    print(f\"  {topic}: {count} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cell-10",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample chunk — Strategy A (small):\n",
            "  Topic: RAG, Source: Advanced RAG — Improving retrieval using Hypothetical Document Embeddings(HyDE) _ by Plaban Nayak _ AI Planet.pdf\n",
            "  Length: 298 chars\n",
            "  Content: of their decisions on org anizational stakeholders:  \n",
            " \n",
            "• The “utilitarian” rule:  An ethical decision is one that produces the greatest\n",
            "number of people, which means that managers should compare alte...\n",
            "\n",
            "Sample chunk — Strategy B (larger):\n",
            "  Topic: RAG, Source: Advanced RAG — Improving retrieval using Hypothetical Document Embeddings(HyDE) _ by Plaban Nayak _ AI Planet.pdf\n",
            "  Length: 780 chars\n",
            "  Content: 0.094572514295578,\n",
            " -0.0002244404749944806,\n",
            " 0.005685583222657442,\n",
            " -0.038341324776411057,\n",
            " -0.030211780220270157,\n",
            " -0.04658368602395058,\n",
            " 0.048414770513772964,\n",
            " -0.26101475954055786,\n",
            " -0.010802186094...\n"
          ]
        }
      ],
      "source": [
        "# Inspect one sample from each strategy\n",
        "print(\"Sample chunk — Strategy A (small):\")\n",
        "c = chunks_a[len(chunks_a) // 2]\n",
        "print(f\"  Topic: {c.metadata['topic']}, Source: {c.metadata['source']}\")\n",
        "print(f\"  Length: {len(c.page_content)} chars\")\n",
        "print(f\"  Content: {c.page_content[:200]}...\")\n",
        "print(\"\\nSample chunk — Strategy B (larger):\")\n",
        "c = chunks_b[len(chunks_b) // 2]\n",
        "print(f\"  Topic: {c.metadata['topic']}, Source: {c.metadata['source']}\")\n",
        "print(f\"  Length: {len(c.page_content)} chars\")\n",
        "print(f\"  Content: {c.page_content[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3726dd3",
      "metadata": {},
      "source": [
        "---\n",
        "## VectorStore and Retriever Classes\n",
        "\n",
        "**VectorStore**\n",
        "- `build_index(chunks)` builds FAISS `IndexFlatIP` on normalized embeddings.\n",
        "- `retrieve(query, top_k=3)` returns top-k scores and indices.\n",
        "\n",
        "**Retriever wrapper**\n",
        "- `retrieve(query, top_k=3)` returns `(score, chunk)` for debugging and manual evaluation.\n",
        "\n",
        "**Conclusion:** Retrieval scores are cosine similarity (normalized embeddings + IndexFlatIP). The pipeline uses the Week 1 champion (MPNet). Experiment 5 optionally compares with MiniLM for reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "27e736c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VectorStore and RAG classes defined. Embedding model: all-mpnet-base-v2\n"
          ]
        }
      ],
      "source": [
        "# Week 2 uses Week 1 champion: MPNet (all-mpnet-base-v2).\n",
        "EMBEDDING_MODEL_ID = \"all-mpnet-base-v2\"\n",
        "\n",
        "\n",
        "class VectorStore:\n",
        "    \"\"\"\n",
        "    Local vector store: chunks + embeddings + FAISS IndexFlatIP.\n",
        "    Similarity = cosine (inner product on L2-normalized vectors).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_id: str = EMBEDDING_MODEL_ID):\n",
        "        self.model_id = model_id\n",
        "        self.model = SentenceTransformer(model_id)\n",
        "        self.chunks = []\n",
        "        self._index = None\n",
        "\n",
        "    def build_index(self, chunks):\n",
        "        \"\"\"Build FAISS index from chunk documents.\"\"\"\n",
        "        self.chunks = list(chunks)\n",
        "        if not self.chunks:\n",
        "            raise ValueError(\"Cannot index: chunks list is empty\")\n",
        "        texts = [c.page_content for c in self.chunks]\n",
        "        emb = self.model.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
        "        emb = np.ascontiguousarray(emb, dtype=np.float32)\n",
        "        dim = emb.shape[1]\n",
        "        self._index = faiss.IndexFlatIP(dim)\n",
        "        self._index.add(emb)\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 3):\n",
        "        \"\"\"Return (scores, indices) for top-k chunks. Scores are cosine similarity.\"\"\"\n",
        "        if not query or not query.strip():\n",
        "            raise ValueError(\"Query cannot be empty\")\n",
        "        if self._index is None or self._index.ntotal == 0:\n",
        "            raise ValueError(\"Index is empty; call build_index(chunks) first\")\n",
        "        q = self.model.encode([query], normalize_embeddings=True)\n",
        "        q = np.ascontiguousarray(q, dtype=np.float32)\n",
        "        k = min(top_k, self._index.ntotal)\n",
        "        scores, indices = self._index.search(q, k)\n",
        "        return scores[0], indices[0]\n",
        "\n",
        "\n",
        "class RAG:\n",
        "    \"\"\"Retrieval-only wrapper (no generation in Week 2).\"\"\"\n",
        "\n",
        "    def __init__(self, vector_store: VectorStore):\n",
        "        self.vector_store = vector_store\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 3):\n",
        "        scores, indices = self.vector_store.retrieve(query, top_k=top_k)\n",
        "        return [(float(s), self.vector_store.chunks[i]) for s, i in zip(scores, indices)]\n",
        "\n",
        "\n",
        "print(\"VectorStore and RAG classes defined. Embedding model:\", EMBEDDING_MODEL_ID)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {},
      "source": [
        "---\n",
        "## Experiment: Retrieval with Two Chunk Strategies\n",
        "\n",
        "**Goal:** Build two retrievers (A/B chunking), run at least 5 queries, print top-3 results, and evaluate quality manually.\n",
        "\n",
        "**Setup:**\n",
        "1. Build `store_a` and `store_b` with the same embedding model (MPNet, Week 1 champion) and same metric (cosine via normalized IP).\n",
        "2. Run the query set.\n",
        "3. Compare relevance and context quality across chunk strategies.\n",
        "\n",
        "**Conclusion:** The printed top-3 and the evaluation table show whether the expected topic is retrieved at top-1 and in top-3 for each strategy; the best strategy is selected in the next cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cell-12",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c01f8baa9ab4a868f4c9c51f6d72453",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MPNetModel LOAD REPORT from: sentence-transformers/all-mpnet-base-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e6db373d4e549b5a38ec52b461629c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Build VectorStores for both chunk strategies using the same model/metric (MPNet + cosine)\n",
        "store_a = VectorStore()\n",
        "store_a.build_index(chunks_a)\n",
        "store_b = VectorStore()\n",
        "store_b.build_index(chunks_b)\n",
        "\n",
        "rag_a = RAG(store_a)\n",
        "rag_b = RAG(store_b)\n",
        "\n",
        "# At least 5 test queries (mix of topics)\n",
        "TEST_QUERIES = [\n",
        "    \"What is RAG?\",\n",
        "    \"What is GIT?\",\n",
        "    \"What is GCP?\",\n",
        "    \"How to create a git branch?\",\n",
        "    \"What is HyDE in RAG?\",\n",
        "]\n",
        "\n",
        "def print_top3(label, results):\n",
        "    for rank, (score, chunk) in enumerate(results, 1):\n",
        "        topic = chunk.metadata.get(\"topic\", \"?\")\n",
        "        src = chunk.metadata.get(\"source\", \"?\")[:40]\n",
        "        text = chunk.page_content[:120].replace(\"\\n\", \" \")\n",
        "        print(f\"  {rank}. [{topic}] score={score:.4f} | {src}\")\n",
        "        print(f\"      {text}...\")\n",
        "\n",
        "for query in TEST_QUERIES:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Query: {query!r}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(\"Strategy A (small chunks):\")\n",
        "    print_top3(\"A\", rag_a.retrieve(query, top_k=3))\n",
        "    print(\"\\nStrategy B (larger chunks):\")\n",
        "    print_top3(\"B\", rag_b.retrieve(query, top_k=3))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "165125da",
      "metadata": {},
      "source": [
        "### Manual Evaluation Summary\n",
        "\n",
        "The table below records, per query: **expected_topic**, and for strategies A and B — **A_top1_topic**, **A_top1_score**, **B_top1_topic**, **B_top1_score**, and whether the expected topic appears in top-3 (**A_top3_hit**, **B_top3_hit**). The next cell aggregates **top1_accuracy**, **top3_hit_rate**, and **avg_top1_score** per strategy.\n",
        "\n",
        "**Conclusion:** We choose the strategy with the best top1_accuracy; ties are broken by avg_top1_score. Both strategies use the same model (MPNet) and metric (cosine), so any difference in scores or relevance is due to chunking only.\n",
        "\n",
        "In practice: smaller chunks (A) often give more precise, local matches; larger chunks (B) give more context per hit but can dilute the top-1 match.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cell-13",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manual evaluation table (chunk strategy A vs B):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>expected_topic</th>\n",
              "      <th>A_top1_topic</th>\n",
              "      <th>A_top1_score</th>\n",
              "      <th>A_top3_hit</th>\n",
              "      <th>B_top1_topic</th>\n",
              "      <th>B_top1_score</th>\n",
              "      <th>B_top3_hit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is RAG?</td>\n",
              "      <td>RAG</td>\n",
              "      <td>RAG</td>\n",
              "      <td>0.6892</td>\n",
              "      <td>True</td>\n",
              "      <td>RAG</td>\n",
              "      <td>0.5923</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is GIT?</td>\n",
              "      <td>GIT</td>\n",
              "      <td>GIT</td>\n",
              "      <td>0.7795</td>\n",
              "      <td>True</td>\n",
              "      <td>GIT</td>\n",
              "      <td>0.7399</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is GCP?</td>\n",
              "      <td>GCP</td>\n",
              "      <td>GCP</td>\n",
              "      <td>0.7446</td>\n",
              "      <td>True</td>\n",
              "      <td>GCP</td>\n",
              "      <td>0.6310</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How to create a git branch?</td>\n",
              "      <td>GIT</td>\n",
              "      <td>GIT</td>\n",
              "      <td>0.6926</td>\n",
              "      <td>True</td>\n",
              "      <td>GIT</td>\n",
              "      <td>0.6746</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is HyDE in RAG?</td>\n",
              "      <td>RAG</td>\n",
              "      <td>RAG</td>\n",
              "      <td>0.4953</td>\n",
              "      <td>True</td>\n",
              "      <td>RAG</td>\n",
              "      <td>0.4013</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         query expected_topic A_top1_topic  A_top1_score  \\\n",
              "0                 What is RAG?            RAG          RAG        0.6892   \n",
              "1                 What is GIT?            GIT          GIT        0.7795   \n",
              "2                 What is GCP?            GCP          GCP        0.7446   \n",
              "3  How to create a git branch?            GIT          GIT        0.6926   \n",
              "4         What is HyDE in RAG?            RAG          RAG        0.4953   \n",
              "\n",
              "   A_top3_hit B_top1_topic  B_top1_score  B_top3_hit  \n",
              "0        True          RAG        0.5923        True  \n",
              "1        True          GIT        0.7399        True  \n",
              "2        True          GCP        0.6310        True  \n",
              "3        True          GIT        0.6746        True  \n",
              "4        True          RAG        0.4013        True  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Manual evaluation table: expected topic vs retrieved topic (top-1 and top-3)\n",
        "QUERY_EXPECTED_TOPIC = {\n",
        "    \"What is RAG?\": \"RAG\",\n",
        "    \"What is GIT?\": \"GIT\",\n",
        "    \"What is GCP?\": \"GCP\",\n",
        "    \"How to create a git branch?\": \"GIT\",\n",
        "    \"What is HyDE in RAG?\": \"RAG\",\n",
        "}\n",
        "\n",
        "rows = []\n",
        "for query in TEST_QUERIES:\n",
        "    expected = QUERY_EXPECTED_TOPIC.get(query, \"?\")\n",
        "\n",
        "    ra = rag_a.retrieve(query, top_k=3)\n",
        "    rb = rag_b.retrieve(query, top_k=3)\n",
        "\n",
        "    a_topics = [item[1].metadata.get(\"topic\", \"?\") for item in ra]\n",
        "    b_topics = [item[1].metadata.get(\"topic\", \"?\") for item in rb]\n",
        "\n",
        "    rows.append({\n",
        "        \"query\": query,\n",
        "        \"expected_topic\": expected,\n",
        "        \"A_top1_topic\": a_topics[0] if a_topics else \"-\",\n",
        "        \"A_top1_score\": round(ra[0][0], 4) if ra else None,\n",
        "        \"A_top3_hit\": expected in a_topics,\n",
        "        \"B_top1_topic\": b_topics[0] if b_topics else \"-\",\n",
        "        \"B_top1_score\": round(rb[0][0], 4) if rb else None,\n",
        "        \"B_top3_hit\": expected in b_topics,\n",
        "    })\n",
        "\n",
        "eval_df = pd.DataFrame(rows)\n",
        "print(\"Manual evaluation table (chunk strategy A vs B):\")\n",
        "eval_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cell-14",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk strategy summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>strategy</th>\n",
              "      <th>top1_accuracy</th>\n",
              "      <th>top3_hit_rate</th>\n",
              "      <th>avg_top1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A_small_300_50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B_large_800_100</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6078</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          strategy  top1_accuracy  top3_hit_rate  avg_top1_score\n",
              "0   A_small_300_50            1.0            1.0          0.6802\n",
              "1  B_large_800_100            1.0            1.0          0.6078"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Strategy-level summary (same model + metric; only chunking changes)\n",
        "strategy_summary = pd.DataFrame([\n",
        "    {\n",
        "        \"strategy\": \"A_small_300_50\",\n",
        "        \"top1_accuracy\": (eval_df[\"A_top1_topic\"] == eval_df[\"expected_topic\"]).mean(),\n",
        "        \"top3_hit_rate\": eval_df[\"A_top3_hit\"].mean(),\n",
        "        \"avg_top1_score\": eval_df[\"A_top1_score\"].mean(),\n",
        "    },\n",
        "    {\n",
        "        \"strategy\": \"B_large_800_100\",\n",
        "        \"top1_accuracy\": (eval_df[\"B_top1_topic\"] == eval_df[\"expected_topic\"]).mean(),\n",
        "        \"top3_hit_rate\": eval_df[\"B_top3_hit\"].mean(),\n",
        "        \"avg_top1_score\": eval_df[\"B_top1_score\"].mean(),\n",
        "    },\n",
        "])\n",
        "\n",
        "strategy_summary[[\"top1_accuracy\", \"top3_hit_rate\", \"avg_top1_score\"]] = (\n",
        "    strategy_summary[[\"top1_accuracy\", \"top3_hit_rate\", \"avg_top1_score\"]].round(4)\n",
        ")\n",
        "\n",
        "# Primary decision rule: top1_accuracy, tie-breaker: avg_top1_score\n",
        "best_idx = strategy_summary[[\"top1_accuracy\", \"avg_top1_score\"]].astype(float).idxmax().iloc[0]\n",
        "CHOSEN_STRATEGY = strategy_summary.loc[best_idx, \"strategy\"]\n",
        "CHOSEN_CHUNKS = chunks_a if CHOSEN_STRATEGY.startswith(\"A_\") else chunks_b\n",
        "\n",
        "print(\"Chunk strategy summary:\")\n",
        "strategy_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "32ad0cbf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected chunk strategy for the rest of Week 2: A_small_300_50\n",
            "Reason: best top-1 relevance (tie-breaker: higher average top-1 cosine score).\n"
          ]
        }
      ],
      "source": [
        "print(f\"Selected chunk strategy for the rest of Week 2: {CHOSEN_STRATEGY}\")\n",
        "print(\"Reason: best top-1 relevance (tie-breaker: higher average top-1 cosine score).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "07640fd0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved:\n",
            "- artifacts/week2_chunk_strategy_eval.csv\n",
            "- artifacts/week2_chunk_strategy_summary.csv\n"
          ]
        }
      ],
      "source": [
        "# Persist retrieval evaluation artifacts for report/mentor review\n",
        "(eval_df.sort_values(\"query\")\n",
        " .to_csv(ARTIFACTS_DIR / \"week2_chunk_strategy_eval.csv\", index=False))\n",
        "strategy_summary.to_csv(ARTIFACTS_DIR / \"week2_chunk_strategy_summary.csv\", index=False)\n",
        "print(\"Saved:\")\n",
        "print(\"- artifacts/week2_chunk_strategy_eval.csv\")\n",
        "print(\"- artifacts/week2_chunk_strategy_summary.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e01ec2b9",
      "metadata": {},
      "source": [
        "---\n",
        "## Analysis\n",
        "\n",
        "### 1) Chunk size and overlap\n",
        "- Smaller chunks usually improve pinpoint matching.\n",
        "- Larger chunks usually improve context completeness.\n",
        "- Overlap reduces boundary loss.\n",
        "\n",
        "### 2) Metric consistency with Week 1\n",
        "- Champion metric is cosine.\n",
        "- With normalized embeddings, `IndexFlatIP` returns cosine-equivalent ranking.\n",
        "\n",
        "### 3) Model usage logic from Week 1\n",
        "- **Pipeline model:** MPNet (Week 1 champion).\n",
        "- **Optional:** Experiment 5 compares with MiniLM for reference.\n",
        "\n",
        "**Conclusion:** Chunk size and overlap directly affect retrieval; we use the same metric (cosine) and operational model (MiniLM) for consistency with Week 1.\n",
        "\n",
        "### Handoff to Week 3\n",
        "Week 3 reuses this retriever (same chunking and indexing) and adds prompt + LLM generation over the retrieved context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2f8c09c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model comparison will use strategy: A_small_300_50\n",
            "Chunks used: 1139\n"
          ]
        }
      ],
      "source": [
        "# Setup for model comparison: keep chunking fixed to the selected strategy\n",
        "chunks = CHOSEN_CHUNKS\n",
        "texts = [c.page_content for c in chunks]\n",
        "topics = [c.metadata[\"topic\"] for c in chunks]\n",
        "\n",
        "print(f\"Model comparison will use strategy: {CHOSEN_STRATEGY}\")\n",
        "print(f\"Chunks used: {len(chunks)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {},
      "source": [
        "---\n",
        "## Experiment 4: Compare Chunk Sizes\n",
        "\n",
        "**Goal:** Understand how chunk size affects retrieval quality.\n",
        "\n",
        "**Configurations:**\n",
        "- Small: 300 chars, 30 overlap → more precise, less context\n",
        "- Medium: 500 chars, 50 overlap → balanced\n",
        "- Large: 800 chars, 80 overlap → more context, may include noise\n",
        "\n",
        "**Expected behavior:**\n",
        "- Small chunks → more chunks, precise matches\n",
        "- Large chunks → fewer chunks, broader context\n",
        "\n",
        "**Conclusion:** The table below shows top1_accuracy, top3_hit_rate, and avg_top1_score for each configuration; the best combination depends on the query set and corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cell-16",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00c58fc6c2764d689c69ea5dc6fbf67c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb0402fb069e47879173fef71463703c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/35 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ff4efcdd9d74efc95c94ea8ed63f8b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a41ec207f38b4d9690bcdbcfa1aae475",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "110d24c6f32f45d09c2df75b307b317f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "100d93e8bd594aefb02d75fd5a79c2b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk-size experiment summary (MiniLM + cosine):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk_size</th>\n",
              "      <th>overlap</th>\n",
              "      <th>num_chunks</th>\n",
              "      <th>top1_accuracy</th>\n",
              "      <th>top3_hit_rate</th>\n",
              "      <th>avg_top1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>300</td>\n",
              "      <td>30</td>\n",
              "      <td>1097</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>500</td>\n",
              "      <td>50</td>\n",
              "      <td>663</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>800</td>\n",
              "      <td>80</td>\n",
              "      <td>435</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   chunk_size  overlap  num_chunks  top1_accuracy  top3_hit_rate  \\\n",
              "0         300       30        1097            1.0            1.0   \n",
              "1         500       50         663            1.0            1.0   \n",
              "2         800       80         435            1.0            1.0   \n",
              "\n",
              "   avg_top1_score  \n",
              "0          0.6502  \n",
              "1          0.6211  \n",
              "2          0.6104  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunk_configs = [\n",
        "    {\"chunk_size\": 300, \"overlap\": 30},\n",
        "    {\"chunk_size\": 500, \"overlap\": 50},\n",
        "    {\"chunk_size\": 800, \"overlap\": 80},\n",
        "]\n",
        "\n",
        "# Evaluate chunk-size impact with fixed model/metric (MPNet + cosine)\n",
        "chunk_eval_rows = []\n",
        "for cfg in chunk_configs:\n",
        "    chunks_temp = chunk_documents(raw_docs, cfg[\"chunk_size\"], cfg[\"overlap\"])\n",
        "    store_temp = VectorStore(model_id=EMBEDDING_MODEL_ID)\n",
        "    store_temp.build_index(chunks_temp)\n",
        "    rag_temp = RAG(store_temp)\n",
        "\n",
        "    top1_hits = 0\n",
        "    top3_hits = 0\n",
        "    top1_scores = []\n",
        "\n",
        "    for query in TEST_QUERIES:\n",
        "        expected = QUERY_EXPECTED_TOPIC[query]\n",
        "        results = rag_temp.retrieve(query, top_k=3)\n",
        "        top_topics = [r[1].metadata.get(\"topic\", \"?\") for r in results]\n",
        "\n",
        "        if top_topics and top_topics[0] == expected:\n",
        "            top1_hits += 1\n",
        "        if expected in top_topics:\n",
        "            top3_hits += 1\n",
        "        if results:\n",
        "            top1_scores.append(results[0][0])\n",
        "\n",
        "    chunk_eval_rows.append({\n",
        "        \"chunk_size\": cfg[\"chunk_size\"],\n",
        "        \"overlap\": cfg[\"overlap\"],\n",
        "        \"num_chunks\": len(chunks_temp),\n",
        "        \"top1_accuracy\": round(top1_hits / len(TEST_QUERIES), 4),\n",
        "        \"top3_hit_rate\": round(top3_hits / len(TEST_QUERIES), 4),\n",
        "        \"avg_top1_score\": round(float(np.mean(top1_scores)), 4),\n",
        "    })\n",
        "\n",
        "chunk_eval_df = pd.DataFrame(chunk_eval_rows).sort_values([\"top1_accuracy\", \"avg_top1_score\"], ascending=False)\n",
        "print(\"Chunk-size experiment summary (MPNet + cosine):\")\n",
        "chunk_eval_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {},
      "source": [
        "---\n",
        "## Experiment 5 (Optional): Compare Embedding Models\n",
        "\n",
        "**Goal:** Compare MiniLM and MPNet on the same retrieval setup. The pipeline already uses the Week 1 champion (MPNet); this section is for reference.\n",
        "\n",
        "Method in this notebook:\n",
        "- Fix chunking to the selected strategy from the chunking experiment.\n",
        "- Keep metric fixed to cosine (normalized IP).\n",
        "- Evaluate both models on the same 5-query set with top-1 accuracy, top-3 hit rate, and average top-1 score.\n",
        "\n",
        "| Model | Dimensions | Speed | Expected quality |\n",
        "|-------|------------|-------|------------------|\n",
        "| all-MiniLM-L6-v2 | 384 | Faster | Good |\n",
        "| all-mpnet-base-v2 | 768 | Slower | Slightly better |\n",
        "\n",
        "**Conclusion:** This is optional (pipeline already uses the champion MPNet). Run the helper cell first, then MiniLM and MPNet cells; the table compares both on the same chunks and metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "cell-18",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model comparison setup ready.\n",
            "Fixed settings:\n",
            "- Chunk strategy: A_small_300_50\n",
            "- Metric: cosine (normalized embeddings + IndexFlatIP)\n"
          ]
        }
      ],
      "source": [
        "models = {\n",
        "    \"MiniLM\": \"all-MiniLM-L6-v2\",\n",
        "    \"MPNet\": \"all-mpnet-base-v2\",\n",
        "}\n",
        "\n",
        "\n",
        "def build_index_with_model(texts, model: SentenceTransformer):\n",
        "    if not texts:\n",
        "        raise ValueError(\"Cannot build index: texts list is empty\")\n",
        "\n",
        "    batch_size = 32\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        embeddings.append(model.encode(batch, normalize_embeddings=True, show_progress_bar=False))\n",
        "\n",
        "    emb = np.ascontiguousarray(np.vstack(embeddings), dtype=np.float32)\n",
        "    index = faiss.IndexFlatIP(emb.shape[1])\n",
        "    index.add(emb)\n",
        "    return index\n",
        "\n",
        "\n",
        "def retrieve(model: SentenceTransformer, index, query: str, top_k: int = 3):\n",
        "    if not query or not query.strip():\n",
        "        raise ValueError(\"Query cannot be empty\")\n",
        "    if index.ntotal == 0:\n",
        "        raise ValueError(\"Index is empty\")\n",
        "\n",
        "    q = np.ascontiguousarray(model.encode([query], normalize_embeddings=True), dtype=np.float32)\n",
        "    k = min(top_k, index.ntotal)\n",
        "    scores, indices = index.search(q, k)\n",
        "    return scores[0], indices[0]\n",
        "\n",
        "\n",
        "def evaluate_model_on_queries(model_name: str, model_id: str, texts, chunks, queries, expected_topic):\n",
        "    model = SentenceTransformer(model_id)\n",
        "    idx = build_index_with_model(texts, model)\n",
        "\n",
        "    rows = []\n",
        "    for query in queries:\n",
        "        scores, indices = retrieve(model, idx, query, top_k=3)\n",
        "        retrieved_topics = [chunks[i].metadata.get(\"topic\", \"?\") for i in indices]\n",
        "        expected = expected_topic[query]\n",
        "        rows.append({\n",
        "            \"model\": model_name,\n",
        "            \"query\": query,\n",
        "            \"expected_topic\": expected,\n",
        "            \"top1_topic\": retrieved_topics[0],\n",
        "            \"top1_score\": float(scores[0]),\n",
        "            \"top1_correct\": retrieved_topics[0] == expected,\n",
        "            \"top3_hit\": expected in retrieved_topics,\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    summary = {\n",
        "        \"model\": model_name,\n",
        "        \"top1_accuracy\": round(float(df[\"top1_correct\"].mean()), 4),\n",
        "        \"top3_hit_rate\": round(float(df[\"top3_hit\"].mean()), 4),\n",
        "        \"avg_top1_score\": round(float(df[\"top1_score\"].mean()), 4),\n",
        "    }\n",
        "\n",
        "    del idx\n",
        "    del model\n",
        "    return df, summary\n",
        "\n",
        "\n",
        "print(\"Model comparison setup ready.\")\n",
        "print(\"Fixed settings:\")\n",
        "print(f\"- Chunk strategy: {CHOSEN_STRATEGY}\")\n",
        "print(\"- Metric: cosine (normalized embeddings + IndexFlatIP)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4815ab7",
      "metadata": {},
      "source": [
        "---\n",
        "## Model Comparison: MiniLM\n",
        "\n",
        "**Goal:** Test MiniLM model (384d embeddings) on the query.\n",
        "\n",
        "**Why separate cells?** Each model is loaded and evaluated in its own cell to avoid memory overload. Run the previous cell (with `evaluate_model_on_queries` and `build_index_with_model`) before this one.\n",
        "\n",
        "**Conclusion:** The output is the MiniLM summary (top1_accuracy, top3_hit_rate, avg_top1_score); the MPNet cell produces the same metrics for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6e47793c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading MiniLM...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3546d2b52f0a47c59e59521070df69f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ MiniLM loaded successfully\n",
            "\n",
            "Building index...\n",
            "\n",
            "✗ ERROR with MiniLM: name 'build_index_with_model' is not defined\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/z8/cqzr9q312nzfqkq1hrzrwqnr0000gp/T/ipykernel_28878/4041347915.py\", line 11, in <module>\n",
            "    idx = build_index_with_model(texts, model)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "NameError: name 'build_index_with_model' is not defined\n"
          ]
        }
      ],
      "source": [
        "# Evaluate MiniLM on the selected chunk strategy\n",
        "mini_df, mini_summary = evaluate_model_on_queries(\n",
        "    model_name=\"MiniLM\",\n",
        "    model_id=models[\"MiniLM\"],\n",
        "    texts=texts,\n",
        "    chunks=chunks,\n",
        "    queries=TEST_QUERIES,\n",
        "    expected_topic=QUERY_EXPECTED_TOPIC,\n",
        ")\n",
        "\n",
        "print(\"MiniLM summary:\")\n",
        "print(pd.DataFrame([mini_summary]))\n",
        "mini_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd7cdcf3",
      "metadata": {},
      "source": [
        "---\n",
        "## Model Comparison: MPNet\n",
        "\n",
        "Evaluate MPNet (768d) on the same chunks and queries, then compare with MiniLM.\n",
        "\n",
        "**Conclusion:** The pipeline uses MPNet (Week 1 champion); this comparison confirms MPNet vs MiniLM on the same setup. Results are saved to `artifacts/week2_model_eval_*.csv` and `week2_model_comparison_summary.csv`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99f5f2f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading MPNet...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77f72c2096884f6fb345783d6a8c6f4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPNetModel LOAD REPORT from: sentence-transformers/all-mpnet-base-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Evaluate MPNet on the selected chunk strategy and compare to MiniLM\n",
        "mpnet_df, mpnet_summary = evaluate_model_on_queries(\n",
        "    model_name=\"MPNet\",\n",
        "    model_id=models[\"MPNet\"],\n",
        "    texts=texts,\n",
        "    chunks=chunks,\n",
        "    queries=TEST_QUERIES,\n",
        "    expected_topic=QUERY_EXPECTED_TOPIC,\n",
        ")\n",
        "\n",
        "model_compare_df = pd.DataFrame([mini_summary, mpnet_summary]).sort_values(\n",
        "    [\"top1_accuracy\", \"avg_top1_score\"], ascending=False\n",
        ")\n",
        "\n",
        "# Pipeline uses Week 1 champion\n",
        "WEEK2_OPERATIONAL_MODEL = \"MPNet\"\n",
        "MODEL_BENCHMARK_WINNER = model_compare_df.iloc[0][\"model\"]\n",
        "\n",
        "print(\"Model comparison summary:\")\n",
        "print(model_compare_df)\n",
        "print(f\"\n",
        "Benchmark winner on this sample: {MODEL_BENCHMARK_WINNER}\")\n",
        "print(f\"Operational model for pipeline continuity: {WEEK2_OPERATIONAL_MODEL}\")\n",
        "\n",
        "# Save artifacts\n",
        "mini_df.to_csv(ARTIFACTS_DIR / \"week2_model_eval_minilm.csv\", index=False)\n",
        "mpnet_df.to_csv(ARTIFACTS_DIR / \"week2_model_eval_mpnet.csv\", index=False)\n",
        "model_compare_df.to_csv(ARTIFACTS_DIR / \"week2_model_comparison_summary.csv\", index=False)\n",
        "print(\"Saved model-comparison artifacts to artifacts/.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "047196f7",
      "metadata": {},
      "source": [
        "---\n",
        "## Final Week 2 Conclusions\n",
        "\n",
        "1. **Chunking decision:** selected by measured retrieval relevance (top-1/top-3) on the fixed query set.\n",
        "2. **Model comparison:** MiniLM vs MPNet evaluated on the same chunking and metric.\n",
        "3. **Pipeline model:** **MPNet + cosine** (Week 1 champion).\n",
        "\n",
        "### Handoff to Week 3\n",
        "Week 3 should reuse:\n",
        "- selected chunking strategy,\n",
        "- MPNet embedding (champion from Week 1),\n",
        "- cosine retrieval via FAISS IndexFlatIP,\n",
        "and add prompt + LLM generation on top.\n",
        "\n",
        "**Conclusion:** The final decisions (chunk strategy, operational model, metric) are written to `artifacts/week2_final_decisions.csv` for reproducibility and for use in Week 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a59314a",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_week2_summary = pd.DataFrame([\n",
        "    {\n",
        "        \"decision\": \"chunk_strategy\",\n",
        "        \"value\": CHOSEN_STRATEGY,\n",
        "        \"basis\": \"top1_accuracy -> tie-break avg_top1_score\",\n",
        "    },\n",
        "    {\n",
        "        \"decision\": \"benchmark_model_winner\",\n",
        "        \"value\": MODEL_BENCHMARK_WINNER,\n",
        "        \"basis\": \"same chunks + same metric + same query set\",\n",
        "    },\n",
        "    {\n",
        "        \"decision\": \"operational_model\",\n",
        "        \"value\": WEEK2_OPERATIONAL_MODEL,\n",
        "        \"basis\": \"Week 1 champion (MPNet)\",\n",
        "    },\n",
        "    {\n",
        "        \"decision\": \"metric\",\n",
        "        \"value\": \"cosine\",\n",
        "        \"basis\": \"Week1 champion; implemented via normalized IP\",\n",
        "    },\n",
        "])\n",
        "\n",
        "final_week2_summary.to_csv(ARTIFACTS_DIR / \"week2_final_decisions.csv\", index=False)\n",
        "print(\"Final Week 2 decisions:\")\n",
        "final_week2_summary\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag_env (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
