chunk_size,overlap,model,query,rank,score,topic,chunk_id,text_preview
200,20,MiniLM,Explain retrieval augmented generation in simple terms,1,0.7669997215270996,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
200,20,MiniLM,Explain retrieval augmented generation in simple terms,2,0.52021324634552,RAG,rag_02::c3,"ing, multi-hop retrieval, and fusion techniques to improve answer quality."
200,20,MiniLM,Explain retrieval augmented generation in simple terms,3,0.5115157961845398,HyDE,hyde_01::c1,"ead of embedding the query directly, HyDE uses an LLM to generate a hypothetical document that would answer the query,           then embeds this generated document for retrieval.            This brid"
200,20,MiniLM,How does retrieval reduce hallucinations?,1,0.5809216499328613,RAG,rag_01::c1,"rameters,          RAG systems retrieve relevant documents from an external knowledge base and use them as context for generating responses.          This approach reduces hallucinations by grounding"
200,20,MiniLM,How does retrieval reduce hallucinations?,2,0.24880874156951904,HyDE,hyde_01::c3,nology than the indexed documents.            The technique adds latency due to the generation step but often significantly improves retrieval accuracy.
200,20,MiniLM,How does retrieval reduce hallucinations?,3,0.24217122793197632,RAG,rag_02::c1,"applications.          RAG systems also provide transparency since retrieved sources can be cited,          allowing users to verify the information. Common challenges in RAG include retrieval qualit"
200,20,MiniLM,What is the advantage of RAG over fine-tuning?,1,0.7679013609886169,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.
200,20,MiniLM,What is the advantage of RAG over fine-tuning?,2,0.547143280506134,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
200,20,MiniLM,What is the advantage of RAG over fine-tuning?,3,0.45710670948028564,RAG,rag_02::c1,"applications.          RAG systems also provide transparency since retrieved sources can be cited,          allowing users to verify the information. Common challenges in RAG include retrieval qualit"
200,20,MiniLM,What is chunking and why do we use overlap?,1,0.7298055291175842,Chunking,chunking_02::c1,s.          This preserves semantic coherence within chunks.          Chunk overlap (typically 10-20% of chunk size) ensures that information spanning chunk boundaries is not lost.          For struct
200,20,MiniLM,What is chunking and why do we use overlap?,2,0.611879825592041,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
200,20,MiniLM,What is chunking and why do we use overlap?,3,0.5678441524505615,Chunking,chunking_01::c2,"etween chunks helps maintain continuity across chunk boundaries.          Common chunking strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundar"
200,20,MiniLM,How does chunk size affect retrieval quality?,1,0.7522113919258118,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
200,20,MiniLM,How does chunk size affect retrieval quality?,2,0.6120526194572449,Chunking,chunking_02::c1,s.          This preserves semantic coherence within chunks.          Chunk overlap (typically 10-20% of chunk size) ensures that information spanning chunk boundaries is not lost.          For struct
200,20,MiniLM,How does chunk size affect retrieval quality?,3,0.5484426617622375,Chunking,chunking_01::c3,"sed on topic boundaries.          The optimal chunk size depends on the embedding model's context window, the nature of queries, and the document structure."
200,20,MiniLM,What is recursive character text splitting?,1,0.8872708678245544,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
200,20,MiniLM,What is recursive character text splitting?,2,0.5076967477798462,Chunking,chunking_02::c2,"For structured documents like code or markdown, specialized splitters that respect syntax boundaries produce better results.          Parent-child chunking stores both small chunks for preci"
200,20,MiniLM,What is recursive character text splitting?,3,0.41066429018974304,Chunking,chunking_01::c2,"etween chunks helps maintain continuity across chunk boundaries.          Common chunking strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundar"
200,20,MiniLM,What are text embeddings and how do they work?,1,0.6213207244873047,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
200,20,MiniLM,What are text embeddings and how do they work?,2,0.5858149528503418,Embeddings,embeddings_01::c1,"rity search finds the most relevant documents.          Popular embedding models include OpenAI's text-embedding-ada-002,          Sentence Transformers (like all-MiniLM-L6-v2), and BGE models."
200,20,MiniLM,What are text embeddings and how do they work?,3,0.5331513285636902,Embeddings,embeddings_01::c3,which is computationally efficient.          The choice of embedding model significantly impacts retrieval quality and should match the domain of your documents.
200,20,MiniLM,Which embedding models are popular for RAG?,1,0.591090202331543,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
200,20,MiniLM,Which embedding models are popular for RAG?,2,0.4870419502258301,Embeddings,embeddings_01::c1,"rity search finds the most relevant documents.          Popular embedding models include OpenAI's text-embedding-ada-002,          Sentence Transformers (like all-MiniLM-L6-v2), and BGE models."
200,20,MiniLM,Which embedding models are popular for RAG?,3,0.4856296181678772,RAG,rag_01::c3,"retrieving relevant passages given a query, and generating a response using the retrieved context.          RAG is particularly useful for knowledge-intensive tasks where the model needs acces"
200,20,MiniLM,What is FAISS and how does it work?,1,0.25452667474746704,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.
200,20,MiniLM,What is FAISS and how does it work?,2,0.17352181673049927,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
200,20,MiniLM,What is FAISS and how does it work?,3,0.12638917565345764,Evaluation,evaluation_01::c2,"e faithfulness and relevance scores. End-to-end evaluation often uses human judgment or LLM-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevanc"
200,20,MiniLM,What are the different FAISS index types?,1,0.46611347794532776,VectorDB,vectordb_01::c1,"ty search.          It supports various index types: IndexFlatIP for exact search, IndexIVF for approximate search with clustering,          and IndexHNSW for graph-based approximate search. For produ"
200,20,MiniLM,What are the different FAISS index types?,2,0.3131403625011444,HyDE,hyde_01::c3,nology than the indexed documents.            The technique adds latency due to the generation step but often significantly improves retrieval accuracy.
200,20,MiniLM,What are the different FAISS index types?,3,0.2870911955833435,VectorDB,vectordb_01::c2,"te search. For production systems, managed vector databases like Pinecone,          Weaviate, Milvus, and Qdrant provide additional features like filtering, hybrid search, and automatic scaling."
200,20,MiniLM,What is HyDE in retrieval?,1,0.62940514087677,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
200,20,MiniLM,What is HyDE in retrieval?,2,0.46986645460128784,HyDE,hyde_01::c2,This bridges the semantic gap between short queries and longer document passages.            HyDE is particularly effective when queries are vague or use different terminology than the inde
200,20,MiniLM,What is HyDE in retrieval?,3,0.4647020399570465,HyDE,hyde_01::c1,"ead of embedding the query directly, HyDE uses an LLM to generate a hypothetical document that would answer the query,           then embeds this generated document for retrieval.            This brid"
200,20,MiniLM,How does hypothetical document embedding improve search?,1,0.7112147808074951,Embeddings,embeddings_01::c3,which is computationally efficient.          The choice of embedding model significantly impacts retrieval quality and should match the domain of your documents.
200,20,MiniLM,How does hypothetical document embedding improve search?,2,0.6165214776992798,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
200,20,MiniLM,How does hypothetical document embedding improve search?,3,0.5743847489356995,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
200,20,MiniLM,How do we evaluate retrieval quality?,1,0.6465587615966797,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
200,20,MiniLM,How do we evaluate retrieval quality?,2,0.5885111093521118,Evaluation,evaluation_01::c2,"e faithfulness and relevance scores. End-to-end evaluation often uses human judgment or LLM-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevanc"
200,20,MiniLM,How do we evaluate retrieval quality?,3,0.529495358467102,Evaluation,evaluation_01::c3,"mpleteness, relevance to the query, and proper attribution of sources.          A/B testing with real users provides the most reliable quality signal."
200,20,MiniLM,What is Mean Reciprocal Rank (MRR)?,1,0.37033766508102417,Evaluation,evaluation_01::c1,"s),          Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG). Generation quality can be measured using BLEU,          ROUGE, or model-based metrics like faithfulness and r"
200,20,MiniLM,What is Mean Reciprocal Rank (MRR)?,2,0.28360262513160706,Evaluation,evaluation_01::c2,"e faithfulness and relevance scores. End-to-end evaluation often uses human judgment or LLM-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevanc"
200,20,MiniLM,What is Mean Reciprocal Rank (MRR)?,3,0.2608739733695984,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
200,20,MiniLM,Why does adding retrieved context help LLM answers stay accurate?,1,0.5268526077270508,RAG,rag_01::c3,"retrieving relevant passages given a query, and generating a response using the retrieved context.          RAG is particularly useful for knowledge-intensive tasks where the model needs acces"
200,20,MiniLM,Why does adding retrieved context help LLM answers stay accurate?,2,0.4677843451499939,RAG,rag_02::c3,"ing, multi-hop retrieval, and fusion techniques to improve answer quality."
200,20,MiniLM,Why does adding retrieved context help LLM answers stay accurate?,3,0.40486887097358704,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
200,20,MiniLM,Does overlap always improve retrieval precision?,1,0.4081434905529022,Chunking,chunking_02::c1,s.          This preserves semantic coherence within chunks.          Chunk overlap (typically 10-20% of chunk size) ensures that information spanning chunk boundaries is not lost.          For struct
200,20,MiniLM,Does overlap always improve retrieval precision?,2,0.40418121218681335,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
200,20,MiniLM,Does overlap always improve retrieval precision?,3,0.38105422258377075,HyDE,hyde_01::c3,nology than the indexed documents.            The technique adds latency due to the generation step but often significantly improves retrieval accuracy.
200,20,MiniLM,Should I retrain my model or just add a search layer?,1,0.3296844959259033,RAG,rag_01::c4,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
200,20,MiniLM,Should I retrain my model or just add a search layer?,2,0.3174951672554016,Embeddings,embeddings_01::c3,which is computationally efficient.          The choice of embedding model significantly impacts retrieval quality and should match the domain of your documents.
200,20,MiniLM,Should I retrain my model or just add a search layer?,3,0.3159235119819641,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.
200,20,MiniLM,When is updating model weights better than external retrieval?,1,0.45590466260910034,Embeddings,embeddings_01::c3,which is computationally efficient.          The choice of embedding model significantly impacts retrieval quality and should match the domain of your documents.
200,20,MiniLM,When is updating model weights better than external retrieval?,2,0.42867133021354675,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.
200,20,MiniLM,When is updating model weights better than external retrieval?,3,0.3645617365837097,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
200,20,MiniLM,Can retrieval completely replace parameter updates for domain adaptation?,1,0.4650958776473999,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
200,20,MiniLM,Can retrieval completely replace parameter updates for domain adaptation?,2,0.4384106695652008,RAG,rag_02::c2,"ude retrieval quality,          context window limitations, and handling conflicting information from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retri"
200,20,MiniLM,Can retrieval completely replace parameter updates for domain adaptation?,3,0.3860540986061096,RAG,rag_01::c3,"retrieving relevant passages given a query, and generating a response using the retrieved context.          RAG is particularly useful for knowledge-intensive tasks where the model needs acces"
200,20,MiniLM,What happens if my chunks are too small to contain a complete answer?,1,0.5463129878044128,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
200,20,MiniLM,What happens if my chunks are too small to contain a complete answer?,2,0.46658337116241455,Chunking,chunking_01::c3,"sed on topic boundaries.          The optimal chunk size depends on the embedding model's context window, the nature of queries, and the document structure."
200,20,MiniLM,What happens if my chunks are too small to contain a complete answer?,3,0.4481508731842041,Chunking,chunking_02::c1,s.          This preserves semantic coherence within chunks.          Chunk overlap (typically 10-20% of chunk size) ensures that information spanning chunk boundaries is not lost.          For struct
200,20,MiniLM,Does splitting documents always improve search precision?,1,0.6010657548904419,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
200,20,MiniLM,Does splitting documents always improve search precision?,2,0.5016978979110718,Chunking,chunking_02::c2,"For structured documents like code or markdown, specialized splitters that respect syntax boundaries produce better results.          Parent-child chunking stores both small chunks for preci"
200,20,MiniLM,Does splitting documents always improve search precision?,3,0.45293617248535156,Chunking,chunking_02::c3,all chunks for precise retrieval and their parent documents for expanded context.
200,20,MiniLM,How do I handle tables and code blocks when segmenting documents?,1,0.4611964225769043,Chunking,chunking_02::c2,"For structured documents like code or markdown, specialized splitters that respect syntax boundaries produce better results.          Parent-child chunking stores both small chunks for preci"
200,20,MiniLM,How do I handle tables and code blocks when segmenting documents?,2,0.4204431176185608,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
200,20,MiniLM,How do I handle tables and code blocks when segmenting documents?,3,0.37738460302352905,Chunking,chunking_02::c3,all chunks for precise retrieval and their parent documents for expanded context.
200,20,MiniLM,Is generating a fake answer before search the same as fine-tuning?,1,0.37144795060157776,Evaluation,evaluation_01::c3,"mpleteness, relevance to the query, and proper attribution of sources.          A/B testing with real users provides the most reliable quality signal."
200,20,MiniLM,Is generating a fake answer before search the same as fine-tuning?,2,0.3376014530658722,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
200,20,MiniLM,Is generating a fake answer before search the same as fine-tuning?,3,0.33394673466682434,RAG,rag_02::c3,"ing, multi-hop retrieval, and fusion techniques to improve answer quality."
200,20,MiniLM,Why would I want the LLM to hallucinate before retrieval?,1,0.38450223207473755,RAG,rag_01::c1,"rameters,          RAG systems retrieve relevant documents from an external knowledge base and use them as context for generating responses.          This approach reduces hallucinations by grounding"
200,20,MiniLM,Why would I want the LLM to hallucinate before retrieval?,2,0.2162838578224182,Evaluation,evaluation_01::c2,"e faithfulness and relevance scores. End-to-end evaluation often uses human judgment or LLM-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevanc"
200,20,MiniLM,Why would I want the LLM to hallucinate before retrieval?,3,0.21572375297546387,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
200,20,MiniLM,Does HyDE work when the query uses jargon the LLM doesn't know?,1,0.6426851153373718,HyDE,hyde_01::c2,This bridges the semantic gap between short queries and longer document passages.            HyDE is particularly effective when queries are vague or use different terminology than the inde
200,20,MiniLM,Does HyDE work when the query uses jargon the LLM doesn't know?,2,0.594375729560852,HyDE,hyde_01::c1,"ead of embedding the query directly, HyDE uses an LLM to generate a hypothetical document that would answer the query,           then embeds this generated document for retrieval.            This brid"
200,20,MiniLM,Does HyDE work when the query uses jargon the LLM doesn't know?,3,0.559241771697998,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
200,20,MiniLM,Why do two semantically similar sentences sometimes have low cosine scores?,1,0.3412259817123413,Chunking,chunking_02::c1,s.          This preserves semantic coherence within chunks.          Chunk overlap (typically 10-20% of chunk size) ensures that information spanning chunk boundaries is not lost.          For struct
200,20,MiniLM,Why do two semantically similar sentences sometimes have low cosine scores?,2,0.3101921081542969,RAG,rag_02::c3,"ing, multi-hop retrieval, and fusion techniques to improve answer quality."
200,20,MiniLM,Why do two semantically similar sentences sometimes have low cosine scores?,3,0.308017760515213,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
200,20,MiniLM,Can inner product and cosine similarity give different ranking results?,1,0.37547576427459717,VectorDB,vectordb_01::c3,matic scaling.          The choice between exact and approximate nearest neighbor search depends on the dataset size and latency requirements.
200,20,MiniLM,Can inner product and cosine similarity give different ranking results?,2,0.36258596181869507,Embeddings,embeddings_01::c2,"d BGE models.          The embedding dimension affects storage requirements and search speed.          Normalized embeddings allow using inner product as a similarity metric,          which is computa"
200,20,MiniLM,Can inner product and cosine similarity give different ranking results?,3,0.3347824811935425,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.
200,20,MPNet,Explain retrieval augmented generation in simple terms,1,0.6249933838844299,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
200,20,MPNet,Explain retrieval augmented generation in simple terms,2,0.5585868954658508,HyDE,hyde_01::c3,nology than the indexed documents.            The technique adds latency due to the generation step but often significantly improves retrieval accuracy.
200,20,MPNet,Explain retrieval augmented generation in simple terms,3,0.5177221298217773,RAG,rag_01::c3,"retrieving relevant passages given a query, and generating a response using the retrieved context.          RAG is particularly useful for knowledge-intensive tasks where the model needs acces"
200,20,MPNet,How does retrieval reduce hallucinations?,1,0.5728269815444946,RAG,rag_01::c1,"rameters,          RAG systems retrieve relevant documents from an external knowledge base and use them as context for generating responses.          This approach reduces hallucinations by grounding"
200,20,MPNet,How does retrieval reduce hallucinations?,2,0.22737331688404083,HyDE,hyde_01::c3,nology than the indexed documents.            The technique adds latency due to the generation step but often significantly improves retrieval accuracy.
200,20,MPNet,How does retrieval reduce hallucinations?,3,0.21896257996559143,RAG,rag_02::c1,"applications.          RAG systems also provide transparency since retrieved sources can be cited,          allowing users to verify the information. Common challenges in RAG include retrieval qualit"
200,20,MPNet,What is the advantage of RAG over fine-tuning?,1,0.8157325983047485,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.
200,20,MPNet,What is the advantage of RAG over fine-tuning?,2,0.46277064085006714,RAG,rag_01::c2,"ations by grounding the model's output in factual, retrieved information.          The typical RAG pipeline consists of three stages: indexing documents into a searchable format,          retrieving r"
200,20,MPNet,What is the advantage of RAG over fine-tuning?,3,0.4420663118362427,RAG,rag_02::c1,"applications.          RAG systems also provide transparency since retrieved sources can be cited,          allowing users to verify the information. Common challenges in RAG include retrieval qualit"
200,20,MPNet,What is chunking and why do we use overlap?,1,0.6596480011940002,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
200,20,MPNet,What is chunking and why do we use overlap?,2,0.6473394632339478,Chunking,chunking_02::c1,s.          This preserves semantic coherence within chunks.          Chunk overlap (typically 10-20% of chunk size) ensures that information spanning chunk boundaries is not lost.          For struct
200,20,MPNet,What is chunking and why do we use overlap?,3,0.5923565030097961,Chunking,chunking_01::c2,"etween chunks helps maintain continuity across chunk boundaries.          Common chunking strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundar"
200,20,MPNet,How does chunk size affect retrieval quality?,1,0.6834737062454224,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
200,20,MPNet,How does chunk size affect retrieval quality?,2,0.6000291109085083,Chunking,chunking_02::c3,all chunks for precise retrieval and their parent documents for expanded context.
200,20,MPNet,How does chunk size affect retrieval quality?,3,0.5608620047569275,Chunking,chunking_02::c1,s.          This preserves semantic coherence within chunks.          Chunk overlap (typically 10-20% of chunk size) ensures that information spanning chunk boundaries is not lost.          For struct
200,20,MPNet,What is recursive character text splitting?,1,0.8415653109550476,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
200,20,MPNet,What is recursive character text splitting?,2,0.5435062646865845,Chunking,chunking_02::c2,"For structured documents like code or markdown, specialized splitters that respect syntax boundaries produce better results.          Parent-child chunking stores both small chunks for preci"
200,20,MPNet,What is recursive character text splitting?,3,0.46470633149147034,Chunking,chunking_01::c2,"etween chunks helps maintain continuity across chunk boundaries.          Common chunking strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundar"
200,20,MPNet,What are text embeddings and how do they work?,1,0.5611940026283264,Embeddings,embeddings_01::c1,"rity search finds the most relevant documents.          Popular embedding models include OpenAI's text-embedding-ada-002,          Sentence Transformers (like all-MiniLM-L6-v2), and BGE models."
200,20,MPNet,What are text embeddings and how do they work?,2,0.5230481028556824,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
200,20,MPNet,What are text embeddings and how do they work?,3,0.42943310737609863,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
200,20,MPNet,Which embedding models are popular for RAG?,1,0.5732327103614807,Embeddings,embeddings_01::c3,which is computationally efficient.          The choice of embedding model significantly impacts retrieval quality and should match the domain of your documents.
200,20,MPNet,Which embedding models are popular for RAG?,2,0.5661792159080505,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
200,20,MPNet,Which embedding models are popular for RAG?,3,0.540625810623169,Embeddings,embeddings_01::c1,"rity search finds the most relevant documents.          Popular embedding models include OpenAI's text-embedding-ada-002,          Sentence Transformers (like all-MiniLM-L6-v2), and BGE models."
200,20,MPNet,What is FAISS and how does it work?,1,0.3244253396987915,RAG,rag_02::c1,"applications.          RAG systems also provide transparency since retrieved sources can be cited,          allowing users to verify the information. Common challenges in RAG include retrieval qualit"
200,20,MPNet,What is FAISS and how does it work?,2,0.3073021471500397,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.
200,20,MPNet,What is FAISS and how does it work?,3,0.28855979442596436,RAG,rag_01::c2,"ations by grounding the model's output in factual, retrieved information.          The typical RAG pipeline consists of three stages: indexing documents into a searchable format,          retrieving r"
200,20,MPNet,What are the different FAISS index types?,1,0.3454911708831787,VectorDB,vectordb_01::c1,"ty search.          It supports various index types: IndexFlatIP for exact search, IndexIVF for approximate search with clustering,          and IndexHNSW for graph-based approximate search. For produ"
200,20,MPNet,What are the different FAISS index types?,2,0.3240854740142822,Evaluation,evaluation_01::c3,"mpleteness, relevance to the query, and proper attribution of sources.          A/B testing with real users provides the most reliable quality signal."
200,20,MPNet,What are the different FAISS index types?,3,0.320437490940094,Evaluation,evaluation_01::c2,"e faithfulness and relevance scores. End-to-end evaluation often uses human judgment or LLM-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevanc"
200,20,MPNet,What is HyDE in retrieval?,1,0.6144624352455139,HyDE,hyde_01::c2,This bridges the semantic gap between short queries and longer document passages.            HyDE is particularly effective when queries are vague or use different terminology than the inde
200,20,MPNet,What is HyDE in retrieval?,2,0.5280439853668213,HyDE,hyde_01::c1,"ead of embedding the query directly, HyDE uses an LLM to generate a hypothetical document that would answer the query,           then embeds this generated document for retrieval.            This brid"
200,20,MPNet,What is HyDE in retrieval?,3,0.3976677656173706,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
200,20,MPNet,How does hypothetical document embedding improve search?,1,0.6612601280212402,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
200,20,MPNet,How does hypothetical document embedding improve search?,2,0.5755919218063354,Embeddings,embeddings_01::c3,which is computationally efficient.          The choice of embedding model significantly impacts retrieval quality and should match the domain of your documents.
200,20,MPNet,How does hypothetical document embedding improve search?,3,0.5588884949684143,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
200,20,MPNet,How do we evaluate retrieval quality?,1,0.6353874802589417,Evaluation,evaluation_01::c3,"mpleteness, relevance to the query, and proper attribution of sources.          A/B testing with real users provides the most reliable quality signal."
200,20,MPNet,How do we evaluate retrieval quality?,2,0.5882893800735474,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
200,20,MPNet,How do we evaluate retrieval quality?,3,0.5439832806587219,Evaluation,evaluation_01::c2,"e faithfulness and relevance scores. End-to-end evaluation often uses human judgment or LLM-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevanc"
200,20,MPNet,What is Mean Reciprocal Rank (MRR)?,1,0.3736664652824402,Evaluation,evaluation_01::c1,"s),          Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG). Generation quality can be measured using BLEU,          ROUGE, or model-based metrics like faithfulness and r"
200,20,MPNet,What is Mean Reciprocal Rank (MRR)?,2,0.305786669254303,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
200,20,MPNet,What is Mean Reciprocal Rank (MRR)?,3,0.27753543853759766,HyDE,hyde_01::c3,nology than the indexed documents.            The technique adds latency due to the generation step but often significantly improves retrieval accuracy.
200,20,MPNet,Why does adding retrieved context help LLM answers stay accurate?,1,0.4427986741065979,RAG,rag_01::c3,"retrieving relevant passages given a query, and generating a response using the retrieved context.          RAG is particularly useful for knowledge-intensive tasks where the model needs acces"
200,20,MPNet,Why does adding retrieved context help LLM answers stay accurate?,2,0.4291888177394867,Evaluation,evaluation_01::c2,"e faithfulness and relevance scores. End-to-end evaluation often uses human judgment or LLM-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevanc"
200,20,MPNet,Why does adding retrieved context help LLM answers stay accurate?,3,0.42214879393577576,RAG,rag_02::c3,"ing, multi-hop retrieval, and fusion techniques to improve answer quality."
200,20,MPNet,Does overlap always improve retrieval precision?,1,0.4355282187461853,HyDE,hyde_01::c3,nology than the indexed documents.            The technique adds latency due to the generation step but often significantly improves retrieval accuracy.
200,20,MPNet,Does overlap always improve retrieval precision?,2,0.4301225543022156,RAG,rag_02::c2,"ude retrieval quality,          context window limitations, and handling conflicting information from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retri"
200,20,MPNet,Does overlap always improve retrieval precision?,3,0.4104580879211426,Chunking,chunking_01::c1,s (100-200 tokens) provide precise matches but may lose context.          Large chunks (500-1000 tokens) preserve context but may include irrelevant information.          Overlap between chunks helps
200,20,MPNet,Should I retrain my model or just add a search layer?,1,0.3242405652999878,RAG,rag_01::c4,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
200,20,MPNet,Should I retrain my model or just add a search layer?,2,0.2810242772102356,Embeddings,embeddings_01::c3,which is computationally efficient.          The choice of embedding model significantly impacts retrieval quality and should match the domain of your documents.
200,20,MPNet,Should I retrain my model or just add a search layer?,3,0.26689255237579346,Chunking,chunking_01::c1,s (100-200 tokens) provide precise matches but may lose context.          Large chunks (500-1000 tokens) preserve context but may include irrelevant information.          Overlap between chunks helps
200,20,MPNet,When is updating model weights better than external retrieval?,1,0.4176751971244812,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.
200,20,MPNet,When is updating model weights better than external retrieval?,2,0.37876462936401367,Embeddings,embeddings_01::c3,which is computationally efficient.          The choice of embedding model significantly impacts retrieval quality and should match the domain of your documents.
200,20,MPNet,When is updating model weights better than external retrieval?,3,0.3714296817779541,RAG,rag_01::c4,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
200,20,MPNet,Can retrieval completely replace parameter updates for domain adaptation?,1,0.48080313205718994,RAG,rag_02::c2,"ude retrieval quality,          context window limitations, and handling conflicting information from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retri"
200,20,MPNet,Can retrieval completely replace parameter updates for domain adaptation?,2,0.43725281953811646,RAG,rag_01::c3,"retrieving relevant passages given a query, and generating a response using the retrieved context.          RAG is particularly useful for knowledge-intensive tasks where the model needs acces"
200,20,MPNet,Can retrieval completely replace parameter updates for domain adaptation?,3,0.41131311655044556,RAG,rag_01::c4,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
200,20,MPNet,What happens if my chunks are too small to contain a complete answer?,1,0.41576144099235535,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
200,20,MPNet,What happens if my chunks are too small to contain a complete answer?,2,0.379844069480896,Chunking,chunking_01::c2,"etween chunks helps maintain continuity across chunk boundaries.          Common chunking strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundar"
200,20,MPNet,What happens if my chunks are too small to contain a complete answer?,3,0.36781561374664307,Chunking,chunking_02::c3,all chunks for precise retrieval and their parent documents for expanded context.
200,20,MPNet,Does splitting documents always improve search precision?,1,0.5884323716163635,Chunking,chunking_02::c2,"For structured documents like code or markdown, specialized splitters that respect syntax boundaries produce better results.          Parent-child chunking stores both small chunks for preci"
200,20,MPNet,Does splitting documents always improve search precision?,2,0.5180772542953491,Chunking,chunking_02::c3,all chunks for precise retrieval and their parent documents for expanded context.
200,20,MPNet,Does splitting documents always improve search precision?,3,0.48765289783477783,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
200,20,MPNet,How do I handle tables and code blocks when segmenting documents?,1,0.5766381025314331,Chunking,chunking_02::c2,"For structured documents like code or markdown, specialized splitters that respect syntax boundaries produce better results.          Parent-child chunking stores both small chunks for preci"
200,20,MPNet,How do I handle tables and code blocks when segmenting documents?,2,0.49172133207321167,Chunking,chunking_02::c3,all chunks for precise retrieval and their parent documents for expanded context.
200,20,MPNet,How do I handle tables and code blocks when segmenting documents?,3,0.4431813955307007,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
200,20,MPNet,Is generating a fake answer before search the same as fine-tuning?,1,0.3866581320762634,HyDE,hyde_01::c3,nology than the indexed documents.            The technique adds latency due to the generation step but often significantly improves retrieval accuracy.
200,20,MPNet,Is generating a fake answer before search the same as fine-tuning?,2,0.3322688341140747,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.
200,20,MPNet,Is generating a fake answer before search the same as fine-tuning?,3,0.2968863248825073,Evaluation,evaluation_01::c3,"mpleteness, relevance to the query, and proper attribution of sources.          A/B testing with real users provides the most reliable quality signal."
200,20,MPNet,Why would I want the LLM to hallucinate before retrieval?,1,0.4596015512943268,RAG,rag_01::c1,"rameters,          RAG systems retrieve relevant documents from an external knowledge base and use them as context for generating responses.          This approach reduces hallucinations by grounding"
200,20,MPNet,Why would I want the LLM to hallucinate before retrieval?,2,0.24313601851463318,Evaluation,evaluation_01::c2,"e faithfulness and relevance scores. End-to-end evaluation often uses human judgment or LLM-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevanc"
200,20,MPNet,Why would I want the LLM to hallucinate before retrieval?,3,0.17512814700603485,HyDE,hyde_01::c1,"ead of embedding the query directly, HyDE uses an LLM to generate a hypothetical document that would answer the query,           then embeds this generated document for retrieval.            This brid"
200,20,MPNet,Does HyDE work when the query uses jargon the LLM doesn't know?,1,0.6733217239379883,HyDE,hyde_01::c2,This bridges the semantic gap between short queries and longer document passages.            HyDE is particularly effective when queries are vague or use different terminology than the inde
200,20,MPNet,Does HyDE work when the query uses jargon the LLM doesn't know?,2,0.6569033265113831,HyDE,hyde_01::c1,"ead of embedding the query directly, HyDE uses an LLM to generate a hypothetical document that would answer the query,           then embeds this generated document for retrieval.            This brid"
200,20,MPNet,Does HyDE work when the query uses jargon the LLM doesn't know?,3,0.45331019163131714,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
200,20,MPNet,Why do two semantically similar sentences sometimes have low cosine scores?,1,0.3029744625091553,RAG,rag_02::c3,"ing, multi-hop retrieval, and fusion techniques to improve answer quality."
200,20,MPNet,Why do two semantically similar sentences sometimes have low cosine scores?,2,0.2832590937614441,Evaluation,evaluation_01::c2,"e faithfulness and relevance scores. End-to-end evaluation often uses human judgment or LLM-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevanc"
200,20,MPNet,Why do two semantically similar sentences sometimes have low cosine scores?,3,0.2565110921859741,Chunking,chunking_01::c1,s (100-200 tokens) provide precise matches but may lose context.          Large chunks (500-1000 tokens) preserve context but may include irrelevant information.          Overlap between chunks helps
200,20,MPNet,Can inner product and cosine similarity give different ranking results?,1,0.3732629120349884,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.
200,20,MPNet,Can inner product and cosine similarity give different ranking results?,2,0.3625125288963318,Embeddings,embeddings_01::c2,"d BGE models.          The embedding dimension affects storage requirements and search speed.          Normalized embeddings allow using inner product as a similarity metric,          which is computa"
200,20,MPNet,Can inner product and cosine similarity give different ranking results?,3,0.36054283380508423,VectorDB,vectordb_01::c3,matic scaling.          The choice between exact and approximate nearest neighbor search depends on the dataset size and latency requirements.
500,50,MiniLM,Explain retrieval augmented generation in simple terms,1,0.7066153287887573,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MiniLM,Explain retrieval augmented generation in simple terms,2,0.4878934621810913,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
500,50,MiniLM,Explain retrieval augmented generation in simple terms,3,0.4878656268119812,RAG,rag_02::c1,"mation from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retrieval, and fusion techniques to improve answer quality."
500,50,MiniLM,How does retrieval reduce hallucinations?,1,0.3593703508377075,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MiniLM,How does retrieval reduce hallucinations?,2,0.2155367136001587,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MiniLM,How does retrieval reduce hallucinations?,3,0.2054629623889923,RAG,rag_01::c1,"ical RAG pipeline consists of three stages: indexing documents into a searchable format,          retrieving relevant passages given a query, and generating a response using the retrieved context.    "
500,50,MiniLM,What is the advantage of RAG over fine-tuning?,1,0.5888254642486572,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
500,50,MiniLM,What is the advantage of RAG over fine-tuning?,2,0.520201563835144,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
500,50,MiniLM,What is the advantage of RAG over fine-tuning?,3,0.43268102407455444,RAG,rag_01::c1,"ical RAG pipeline consists of three stages: indexing documents into a searchable format,          retrieving relevant passages given a query, and generating a response using the retrieved context.    "
500,50,MiniLM,What is chunking and why do we use overlap?,1,0.6851403713226318,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MiniLM,What is chunking and why do we use overlap?,2,0.6199502944946289,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
500,50,MiniLM,What is chunking and why do we use overlap?,3,0.48799431324005127,Chunking,chunking_01::c1,"strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundaries.          The optimal chunk size depends on the embedding model's context window, the "
500,50,MiniLM,How does chunk size affect retrieval quality?,1,0.7119191288948059,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MiniLM,How does chunk size affect retrieval quality?,2,0.4995659589767456,Chunking,chunking_01::c1,"strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundaries.          The optimal chunk size depends on the embedding model's context window, the "
500,50,MiniLM,How does chunk size affect retrieval quality?,3,0.48160380125045776,Chunking,chunking_02::c1,ct syntax boundaries produce better results.          Parent-child chunking stores both small chunks for precise retrieval and their parent documents for expanded context.
500,50,MiniLM,What is recursive character text splitting?,1,0.8179610371589661,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
500,50,MiniLM,What is recursive character text splitting?,2,0.4646802246570587,Chunking,chunking_02::c1,ct syntax boundaries produce better results.          Parent-child chunking stores both small chunks for precise retrieval and their parent documents for expanded context.
500,50,MiniLM,What is recursive character text splitting?,3,0.41553163528442383,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MiniLM,What are text embeddings and how do they work?,1,0.5888437032699585,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
500,50,MiniLM,What are text embeddings and how do they work?,2,0.4864896535873413,Embeddings,embeddings_01::c1,"ed.          Normalized embeddings allow using inner product as a similarity metric,          which is computationally efficient.          The choice of embedding model significantly impacts retrieval"
500,50,MiniLM,What are text embeddings and how do they work?,3,0.43400412797927856,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
500,50,MiniLM,Which embedding models are popular for RAG?,1,0.5581165552139282,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
500,50,MiniLM,Which embedding models are popular for RAG?,2,0.538268506526947,RAG,rag_02::c1,"mation from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retrieval, and fusion techniques to improve answer quality."
500,50,MiniLM,Which embedding models are popular for RAG?,3,0.4865465760231018,RAG,rag_01::c1,"ical RAG pipeline consists of three stages: indexing documents into a searchable format,          retrieving relevant passages given a query, and generating a response using the retrieved context.    "
500,50,MiniLM,What is FAISS and how does it work?,1,0.2543308138847351,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.          
500,50,MiniLM,What is FAISS and how does it work?,2,0.16338488459587097,RAG,rag_01::c1,"ical RAG pipeline consists of three stages: indexing documents into a searchable format,          retrieving relevant passages given a query, and generating a response using the retrieved context.    "
500,50,MiniLM,What is FAISS and how does it work?,3,0.15393634140491486,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
500,50,MiniLM,What are the different FAISS index types?,1,0.3831672668457031,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.          
500,50,MiniLM,What are the different FAISS index types?,2,0.2995721697807312,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MiniLM,What are the different FAISS index types?,3,0.28312909603118896,VectorDB,vectordb_01::c1,"te, Milvus, and Qdrant provide additional features like filtering, hybrid search, and automatic scaling.          The choice between exact and approximate nearest neighbor search depends on the datase"
500,50,MiniLM,What is HyDE in retrieval?,1,0.6214565634727478,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
500,50,MiniLM,What is HyDE in retrieval?,2,0.49948325753211975,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MiniLM,What is HyDE in retrieval?,3,0.28026747703552246,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MiniLM,How does hypothetical document embedding improve search?,1,0.6379768252372742,Embeddings,embeddings_01::c1,"ed.          Normalized embeddings allow using inner product as a similarity metric,          which is computationally efficient.          The choice of embedding model significantly impacts retrieval"
500,50,MiniLM,How does hypothetical document embedding improve search?,2,0.5899155139923096,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
500,50,MiniLM,How does hypothetical document embedding improve search?,3,0.5823565721511841,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
500,50,MiniLM,How do we evaluate retrieval quality?,1,0.6580579280853271,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
500,50,MiniLM,How do we evaluate retrieval quality?,2,0.5863640308380127,Evaluation,evaluation_01::c1,"M-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevance to the query, and proper attribution of sources.          A/B testing with real users pr"
500,50,MiniLM,How do we evaluate retrieval quality?,3,0.47957539558410645,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MiniLM,What is Mean Reciprocal Rank (MRR)?,1,0.35605156421661377,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
500,50,MiniLM,What is Mean Reciprocal Rank (MRR)?,2,0.23497840762138367,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
500,50,MiniLM,What is Mean Reciprocal Rank (MRR)?,3,0.2296631932258606,VectorDB,vectordb_01::c1,"te, Milvus, and Qdrant provide additional features like filtering, hybrid search, and automatic scaling.          The choice between exact and approximate nearest neighbor search depends on the datase"
500,50,MiniLM,Why does adding retrieved context help LLM answers stay accurate?,1,0.403576135635376,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MiniLM,Why does adding retrieved context help LLM answers stay accurate?,2,0.3980678617954254,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
500,50,MiniLM,Why does adding retrieved context help LLM answers stay accurate?,3,0.37664586305618286,RAG,rag_02::c1,"mation from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retrieval, and fusion techniques to improve answer quality."
500,50,MiniLM,Does overlap always improve retrieval precision?,1,0.4285014867782593,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MiniLM,Does overlap always improve retrieval precision?,2,0.38049042224884033,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MiniLM,Does overlap always improve retrieval precision?,3,0.3719854950904846,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
500,50,MiniLM,Should I retrain my model or just add a search layer?,1,0.30377882719039917,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MiniLM,Should I retrain my model or just add a search layer?,2,0.25610220432281494,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
500,50,MiniLM,Should I retrain my model or just add a search layer?,3,0.2347683310508728,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MiniLM,When is updating model weights better than external retrieval?,1,0.4002012014389038,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
500,50,MiniLM,When is updating model weights better than external retrieval?,2,0.385092556476593,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MiniLM,When is updating model weights better than external retrieval?,3,0.34393951296806335,Embeddings,embeddings_01::c1,"ed.          Normalized embeddings allow using inner product as a similarity metric,          which is computationally efficient.          The choice of embedding model significantly impacts retrieval"
500,50,MiniLM,Can retrieval completely replace parameter updates for domain adaptation?,1,0.42459988594055176,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MiniLM,Can retrieval completely replace parameter updates for domain adaptation?,2,0.42267757654190063,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
500,50,MiniLM,Can retrieval completely replace parameter updates for domain adaptation?,3,0.3940533995628357,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MiniLM,What happens if my chunks are too small to contain a complete answer?,1,0.5501379370689392,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MiniLM,What happens if my chunks are too small to contain a complete answer?,2,0.4385325610637665,Chunking,chunking_01::c1,"strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundaries.          The optimal chunk size depends on the embedding model's context window, the "
500,50,MiniLM,What happens if my chunks are too small to contain a complete answer?,3,0.3936953544616699,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
500,50,MiniLM,Does splitting documents always improve search precision?,1,0.5615909695625305,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MiniLM,Does splitting documents always improve search precision?,2,0.5047054886817932,Chunking,chunking_01::c1,"strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundaries.          The optimal chunk size depends on the embedding model's context window, the "
500,50,MiniLM,Does splitting documents always improve search precision?,3,0.4982965886592865,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
500,50,MiniLM,How do I handle tables and code blocks when segmenting documents?,1,0.3935978412628174,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MiniLM,How do I handle tables and code blocks when segmenting documents?,2,0.39357316493988037,Chunking,chunking_01::c1,"strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundaries.          The optimal chunk size depends on the embedding model's context window, the "
500,50,MiniLM,How do I handle tables and code blocks when segmenting documents?,3,0.3783135414123535,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
500,50,MiniLM,Is generating a fake answer before search the same as fine-tuning?,1,0.351459264755249,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
500,50,MiniLM,Is generating a fake answer before search the same as fine-tuning?,2,0.3469655215740204,Evaluation,evaluation_01::c1,"M-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevance to the query, and proper attribution of sources.          A/B testing with real users pr"
500,50,MiniLM,Is generating a fake answer before search the same as fine-tuning?,3,0.33156877756118774,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MiniLM,Why would I want the LLM to hallucinate before retrieval?,1,0.26270240545272827,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MiniLM,Why would I want the LLM to hallucinate before retrieval?,2,0.25026363134384155,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
500,50,MiniLM,Why would I want the LLM to hallucinate before retrieval?,3,0.20496562123298645,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
500,50,MiniLM,Does HyDE work when the query uses jargon the LLM doesn't know?,1,0.65097975730896,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
500,50,MiniLM,Does HyDE work when the query uses jargon the LLM doesn't know?,2,0.5997811555862427,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MiniLM,Does HyDE work when the query uses jargon the LLM doesn't know?,3,0.2966725528240204,RAG,rag_02::c1,"mation from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retrieval, and fusion techniques to improve answer quality."
500,50,MiniLM,Why do two semantically similar sentences sometimes have low cosine scores?,1,0.3142207860946655,Chunking,chunking_02::c1,ct syntax boundaries produce better results.          Parent-child chunking stores both small chunks for precise retrieval and their parent documents for expanded context.
500,50,MiniLM,Why do two semantically similar sentences sometimes have low cosine scores?,2,0.2771476209163666,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
500,50,MiniLM,Why do two semantically similar sentences sometimes have low cosine scores?,3,0.2599852979183197,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
500,50,MiniLM,Can inner product and cosine similarity give different ranking results?,1,0.4653714895248413,Embeddings,embeddings_01::c1,"ed.          Normalized embeddings allow using inner product as a similarity metric,          which is computationally efficient.          The choice of embedding model significantly impacts retrieval"
500,50,MiniLM,Can inner product and cosine similarity give different ranking results?,2,0.37627679109573364,VectorDB,vectordb_01::c1,"te, Milvus, and Qdrant provide additional features like filtering, hybrid search, and automatic scaling.          The choice between exact and approximate nearest neighbor search depends on the datase"
500,50,MiniLM,Can inner product and cosine similarity give different ranking results?,3,0.3632270395755768,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.          
500,50,MPNet,Explain retrieval augmented generation in simple terms,1,0.668979823589325,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MPNet,Explain retrieval augmented generation in simple terms,2,0.4941004812717438,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MPNet,Explain retrieval augmented generation in simple terms,3,0.47128310799598694,RAG,rag_02::c1,"mation from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retrieval, and fusion techniques to improve answer quality."
500,50,MPNet,How does retrieval reduce hallucinations?,1,0.24066752195358276,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MPNet,How does retrieval reduce hallucinations?,2,0.18502222001552582,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MPNet,How does retrieval reduce hallucinations?,3,0.17268632352352142,RAG,rag_01::c1,"ical RAG pipeline consists of three stages: indexing documents into a searchable format,          retrieving relevant passages given a query, and generating a response using the retrieved context.    "
500,50,MPNet,What is the advantage of RAG over fine-tuning?,1,0.4614372253417969,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
500,50,MPNet,What is the advantage of RAG over fine-tuning?,2,0.32777005434036255,RAG,rag_01::c1,"ical RAG pipeline consists of three stages: indexing documents into a searchable format,          retrieving relevant passages given a query, and generating a response using the retrieved context.    "
500,50,MPNet,What is the advantage of RAG over fine-tuning?,3,0.2887498438358307,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
500,50,MPNet,What is chunking and why do we use overlap?,1,0.6567494869232178,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MPNet,What is chunking and why do we use overlap?,2,0.5571624040603638,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
500,50,MPNet,What is chunking and why do we use overlap?,3,0.44628527760505676,Chunking,chunking_02::c1,ct syntax boundaries produce better results.          Parent-child chunking stores both small chunks for precise retrieval and their parent documents for expanded context.
500,50,MPNet,How does chunk size affect retrieval quality?,1,0.6373969316482544,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MPNet,How does chunk size affect retrieval quality?,2,0.5121778845787048,Chunking,chunking_02::c1,ct syntax boundaries produce better results.          Parent-child chunking stores both small chunks for precise retrieval and their parent documents for expanded context.
500,50,MPNet,How does chunk size affect retrieval quality?,3,0.4063560366630554,Chunking,chunking_01::c1,"strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundaries.          The optimal chunk size depends on the embedding model's context window, the "
500,50,MPNet,What is recursive character text splitting?,1,0.8429660797119141,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
500,50,MPNet,What is recursive character text splitting?,2,0.4461834728717804,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MPNet,What is recursive character text splitting?,3,0.40258902311325073,Chunking,chunking_01::c1,"strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundaries.          The optimal chunk size depends on the embedding model's context window, the "
500,50,MPNet,What are text embeddings and how do they work?,1,0.5271748304367065,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
500,50,MPNet,What are text embeddings and how do they work?,2,0.4664493203163147,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
500,50,MPNet,What are text embeddings and how do they work?,3,0.44035768508911133,Chunking,chunking_01::c1,"strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundaries.          The optimal chunk size depends on the embedding model's context window, the "
500,50,MPNet,Which embedding models are popular for RAG?,1,0.588266134262085,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
500,50,MPNet,Which embedding models are popular for RAG?,2,0.46225690841674805,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
500,50,MPNet,Which embedding models are popular for RAG?,3,0.45950329303741455,RAG,rag_02::c1,"mation from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retrieval, and fusion techniques to improve answer quality."
500,50,MPNet,What is FAISS and how does it work?,1,0.23134729266166687,RAG,rag_01::c1,"ical RAG pipeline consists of three stages: indexing documents into a searchable format,          retrieving relevant passages given a query, and generating a response using the retrieved context.    "
500,50,MPNet,What is FAISS and how does it work?,2,0.22851109504699707,Evaluation,evaluation_01::c1,"M-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevance to the query, and proper attribution of sources.          A/B testing with real users pr"
500,50,MPNet,What is FAISS and how does it work?,3,0.22589370608329773,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MPNet,What are the different FAISS index types?,1,0.3599451780319214,VectorDB,vectordb_01::c1,"te, Milvus, and Qdrant provide additional features like filtering, hybrid search, and automatic scaling.          The choice between exact and approximate nearest neighbor search depends on the datase"
500,50,MPNet,What are the different FAISS index types?,2,0.3160555064678192,Evaluation,evaluation_01::c1,"M-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevance to the query, and proper attribution of sources.          A/B testing with real users pr"
500,50,MPNet,What are the different FAISS index types?,3,0.29479748010635376,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MPNet,What is HyDE in retrieval?,1,0.6662552356719971,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MPNet,What is HyDE in retrieval?,2,0.3668164014816284,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
500,50,MPNet,What is HyDE in retrieval?,3,0.2739693522453308,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MPNet,How does hypothetical document embedding improve search?,1,0.6579190492630005,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
500,50,MPNet,How does hypothetical document embedding improve search?,2,0.5900659561157227,Embeddings,embeddings_01::c1,"ed.          Normalized embeddings allow using inner product as a similarity metric,          which is computationally efficient.          The choice of embedding model significantly impacts retrieval"
500,50,MPNet,How does hypothetical document embedding improve search?,3,0.5381478071212769,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
500,50,MPNet,How do we evaluate retrieval quality?,1,0.6240861415863037,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
500,50,MPNet,How do we evaluate retrieval quality?,2,0.5925874710083008,Evaluation,evaluation_01::c1,"M-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevance to the query, and proper attribution of sources.          A/B testing with real users pr"
500,50,MPNet,How do we evaluate retrieval quality?,3,0.3919627070426941,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MPNet,What is Mean Reciprocal Rank (MRR)?,1,0.3444153070449829,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
500,50,MPNet,What is Mean Reciprocal Rank (MRR)?,2,0.2834928035736084,Evaluation,evaluation_01::c1,"M-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevance to the query, and proper attribution of sources.          A/B testing with real users pr"
500,50,MPNet,What is Mean Reciprocal Rank (MRR)?,3,0.27119240164756775,VectorDB,vectordb_01::c1,"te, Milvus, and Qdrant provide additional features like filtering, hybrid search, and automatic scaling.          The choice between exact and approximate nearest neighbor search depends on the datase"
500,50,MPNet,Why does adding retrieved context help LLM answers stay accurate?,1,0.3761069178581238,RAG,rag_02::c1,"mation from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retrieval, and fusion techniques to improve answer quality."
500,50,MPNet,Why does adding retrieved context help LLM answers stay accurate?,2,0.35497087240219116,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
500,50,MPNet,Why does adding retrieved context help LLM answers stay accurate?,3,0.3434778153896332,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
500,50,MPNet,Does overlap always improve retrieval precision?,1,0.43561795353889465,Chunking,chunking_02::c1,ct syntax boundaries produce better results.          Parent-child chunking stores both small chunks for precise retrieval and their parent documents for expanded context.
500,50,MPNet,Does overlap always improve retrieval precision?,2,0.3957791328430176,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MPNet,Does overlap always improve retrieval precision?,3,0.3263669013977051,RAG,rag_02::c1,"mation from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retrieval, and fusion techniques to improve answer quality."
500,50,MPNet,Should I retrain my model or just add a search layer?,1,0.25480520725250244,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MPNet,Should I retrain my model or just add a search layer?,2,0.24880090355873108,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
500,50,MPNet,Should I retrain my model or just add a search layer?,3,0.2457398772239685,VectorDB,vectordb_01::c1,"te, Milvus, and Qdrant provide additional features like filtering, hybrid search, and automatic scaling.          The choice between exact and approximate nearest neighbor search depends on the datase"
500,50,MPNet,When is updating model weights better than external retrieval?,1,0.3885955810546875,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
500,50,MPNet,When is updating model weights better than external retrieval?,2,0.3394549489021301,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
500,50,MPNet,When is updating model weights better than external retrieval?,3,0.3116811215877533,RAG,rag_02::c1,"mation from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retrieval, and fusion techniques to improve answer quality."
500,50,MPNet,Can retrieval completely replace parameter updates for domain adaptation?,1,0.46535494923591614,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
500,50,MPNet,Can retrieval completely replace parameter updates for domain adaptation?,2,0.42040953040122986,RAG,rag_02::c1,"mation from multiple sources.          Advanced RAG architectures may include query rewriting, multi-hop retrieval, and fusion techniques to improve answer quality."
500,50,MPNet,Can retrieval completely replace parameter updates for domain adaptation?,3,0.37135395407676697,RAG,rag_01::c1,"ical RAG pipeline consists of three stages: indexing documents into a searchable format,          retrieving relevant passages given a query, and generating a response using the retrieved context.    "
500,50,MPNet,What happens if my chunks are too small to contain a complete answer?,1,0.36572349071502686,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MPNet,What happens if my chunks are too small to contain a complete answer?,2,0.30955415964126587,Chunking,chunking_01::c1,"strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundaries.          The optimal chunk size depends on the embedding model's context window, the "
500,50,MPNet,What happens if my chunks are too small to contain a complete answer?,3,0.29828453063964844,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
500,50,MPNet,Does splitting documents always improve search precision?,1,0.5635780692100525,Chunking,chunking_02::c1,ct syntax boundaries produce better results.          Parent-child chunking stores both small chunks for precise retrieval and their parent documents for expanded context.
500,50,MPNet,Does splitting documents always improve search precision?,2,0.5388696193695068,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MPNet,Does splitting documents always improve search precision?,3,0.45986443758010864,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MPNet,How do I handle tables and code blocks when segmenting documents?,1,0.5138405561447144,Chunking,chunking_02::c1,ct syntax boundaries produce better results.          Parent-child chunking stores both small chunks for precise retrieval and their parent documents for expanded context.
500,50,MPNet,How do I handle tables and code blocks when segmenting documents?,2,0.49356451630592346,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
500,50,MPNet,How do I handle tables and code blocks when segmenting documents?,3,0.4495161473751068,Chunking,chunking_01::c1,"strategies include fixed-size chunking, sentence-based splitting, and semantic chunking based on topic boundaries.          The optimal chunk size depends on the embedding model's context window, the "
500,50,MPNet,Is generating a fake answer before search the same as fine-tuning?,1,0.36107149720191956,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MPNet,Is generating a fake answer before search the same as fine-tuning?,2,0.3182668387889862,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
500,50,MPNet,Is generating a fake answer before search the same as fine-tuning?,3,0.31063297390937805,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MPNet,Why would I want the LLM to hallucinate before retrieval?,1,0.19662047922611237,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
500,50,MPNet,Why would I want the LLM to hallucinate before retrieval?,2,0.1455732136964798,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MPNet,Why would I want the LLM to hallucinate before retrieval?,3,0.13997650146484375,RAG,rag_01::c1,"ical RAG pipeline consists of three stages: indexing documents into a searchable format,          retrieving relevant passages given a query, and generating a response using the retrieved context.    "
500,50,MPNet,Does HyDE work when the query uses jargon the LLM doesn't know?,1,0.653192400932312,HyDE,hyde_01::c1,s.            HyDE is particularly effective when queries are vague or use different terminology than the indexed documents.            The technique adds latency due to the generation step but often 
500,50,MPNet,Does HyDE work when the query uses jargon the LLM doesn't know?,2,0.48430579900741577,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
500,50,MPNet,Does HyDE work when the query uses jargon the LLM doesn't know?,3,0.3169676661491394,Evaluation,evaluation_01::c1,"M-as-judge approaches.          Important aspects to evaluate include factual accuracy, completeness, relevance to the query, and proper attribution of sources.          A/B testing with real users pr"
500,50,MPNet,Why do two semantically similar sentences sometimes have low cosine scores?,1,0.30277523398399353,Chunking,chunking_02::c1,ct syntax boundaries produce better results.          Parent-child chunking stores both small chunks for precise retrieval and their parent documents for expanded context.
500,50,MPNet,Why do two semantically similar sentences sometimes have low cosine scores?,2,0.2562083899974823,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
500,50,MPNet,Why do two semantically similar sentences sometimes have low cosine scores?,3,0.2515566945075989,Embeddings,embeddings_01::c1,"ed.          Normalized embeddings allow using inner product as a similarity metric,          which is computationally efficient.          The choice of embedding model significantly impacts retrieval"
500,50,MPNet,Can inner product and cosine similarity give different ranking results?,1,0.4525066614151001,Embeddings,embeddings_01::c1,"ed.          Normalized embeddings allow using inner product as a similarity metric,          which is computationally efficient.          The choice of embedding model significantly impacts retrieval"
500,50,MPNet,Can inner product and cosine similarity give different ranking results?,2,0.371224969625473,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.          
500,50,MPNet,Can inner product and cosine similarity give different ranking results?,3,0.3094630837440491,VectorDB,vectordb_01::c1,"te, Milvus, and Qdrant provide additional features like filtering, hybrid search, and automatic scaling.          The choice between exact and approximate nearest neighbor search depends on the datase"
800,80,MiniLM,Explain retrieval augmented generation in simple terms,1,0.724215030670166,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,Explain retrieval augmented generation in simple terms,2,0.5398838520050049,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MiniLM,Explain retrieval augmented generation in simple terms,3,0.45857107639312744,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MiniLM,How does retrieval reduce hallucinations?,1,0.3257463574409485,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,How does retrieval reduce hallucinations?,2,0.18409067392349243,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MiniLM,How does retrieval reduce hallucinations?,3,0.17815235257148743,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MiniLM,What is the advantage of RAG over fine-tuning?,1,0.5459879636764526,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MiniLM,What is the advantage of RAG over fine-tuning?,2,0.4765125513076782,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MiniLM,What is the advantage of RAG over fine-tuning?,3,0.3459259569644928,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,What is chunking and why do we use overlap?,1,0.6274346709251404,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MiniLM,What is chunking and why do we use overlap?,2,0.618882417678833,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MiniLM,What is chunking and why do we use overlap?,3,0.2171953320503235,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MiniLM,How does chunk size affect retrieval quality?,1,0.6683230996131897,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MiniLM,How does chunk size affect retrieval quality?,2,0.47492316365242004,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MiniLM,How does chunk size affect retrieval quality?,3,0.39786720275878906,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MiniLM,What is recursive character text splitting?,1,0.7851308584213257,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MiniLM,What is recursive character text splitting?,2,0.393526554107666,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MiniLM,What is recursive character text splitting?,3,0.1741672158241272,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MiniLM,What are text embeddings and how do they work?,1,0.6119892597198486,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MiniLM,What are text embeddings and how do they work?,2,0.424983412027359,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MiniLM,What are text embeddings and how do they work?,3,0.35597479343414307,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MiniLM,Which embedding models are popular for RAG?,1,0.5699025988578796,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MiniLM,Which embedding models are popular for RAG?,2,0.45929521322250366,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MiniLM,Which embedding models are popular for RAG?,3,0.43415263295173645,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,What is FAISS and how does it work?,1,0.2449982464313507,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.          
800,80,MiniLM,What is FAISS and how does it work?,2,0.14424410462379456,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MiniLM,What is FAISS and how does it work?,3,0.09341925382614136,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,What are the different FAISS index types?,1,0.36704325675964355,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.          
800,80,MiniLM,What are the different FAISS index types?,2,0.1980915367603302,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MiniLM,What are the different FAISS index types?,3,0.16829267144203186,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MiniLM,What is HyDE in retrieval?,1,0.5800195932388306,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MiniLM,What is HyDE in retrieval?,2,0.2802048921585083,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,What is HyDE in retrieval?,3,0.20495520532131195,RAG,rag_01::c1,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
800,80,MiniLM,How does hypothetical document embedding improve search?,1,0.6403634548187256,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MiniLM,How does hypothetical document embedding improve search?,2,0.6116032004356384,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MiniLM,How does hypothetical document embedding improve search?,3,0.47622960805892944,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MiniLM,How do we evaluate retrieval quality?,1,0.6729621291160583,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MiniLM,How do we evaluate retrieval quality?,2,0.4668862223625183,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,How do we evaluate retrieval quality?,3,0.45910805463790894,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MiniLM,What is Mean Reciprocal Rank (MRR)?,1,0.34895676374435425,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MiniLM,What is Mean Reciprocal Rank (MRR)?,2,0.22979654371738434,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MiniLM,What is Mean Reciprocal Rank (MRR)?,3,0.16854266822338104,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MiniLM,Why does adding retrieved context help LLM answers stay accurate?,1,0.45758378505706787,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,Why does adding retrieved context help LLM answers stay accurate?,2,0.4359605312347412,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MiniLM,Why does adding retrieved context help LLM answers stay accurate?,3,0.37267395853996277,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MiniLM,Does overlap always improve retrieval precision?,1,0.4060494005680084,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MiniLM,Does overlap always improve retrieval precision?,2,0.38693898916244507,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MiniLM,Does overlap always improve retrieval precision?,3,0.3703113794326782,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,Should I retrain my model or just add a search layer?,1,0.33977729082107544,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,Should I retrain my model or just add a search layer?,2,0.3296844959259033,RAG,rag_01::c1,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
800,80,MiniLM,Should I retrain my model or just add a search layer?,3,0.29411566257476807,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MiniLM,When is updating model weights better than external retrieval?,1,0.43275612592697144,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,When is updating model weights better than external retrieval?,2,0.38640648126602173,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MiniLM,When is updating model weights better than external retrieval?,3,0.36243391036987305,RAG,rag_01::c1,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
800,80,MiniLM,Can retrieval completely replace parameter updates for domain adaptation?,1,0.47400233149528503,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,Can retrieval completely replace parameter updates for domain adaptation?,2,0.4413301348686218,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MiniLM,Can retrieval completely replace parameter updates for domain adaptation?,3,0.38445353507995605,RAG,rag_01::c1,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
800,80,MiniLM,What happens if my chunks are too small to contain a complete answer?,1,0.5109556913375854,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MiniLM,What happens if my chunks are too small to contain a complete answer?,2,0.4094948470592499,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MiniLM,What happens if my chunks are too small to contain a complete answer?,3,0.21785733103752136,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MiniLM,Does splitting documents always improve search precision?,1,0.583583414554596,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MiniLM,Does splitting documents always improve search precision?,2,0.5067424178123474,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MiniLM,Does splitting documents always improve search precision?,3,0.31280753016471863,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,How do I handle tables and code blocks when segmenting documents?,1,0.38268226385116577,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MiniLM,How do I handle tables and code blocks when segmenting documents?,2,0.3824288547039032,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MiniLM,How do I handle tables and code blocks when segmenting documents?,3,0.2174210548400879,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,Is generating a fake answer before search the same as fine-tuning?,1,0.3971341550350189,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MiniLM,Is generating a fake answer before search the same as fine-tuning?,2,0.3307223916053772,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MiniLM,Is generating a fake answer before search the same as fine-tuning?,3,0.32230478525161743,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MiniLM,Why would I want the LLM to hallucinate before retrieval?,1,0.24916236102581024,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,Why would I want the LLM to hallucinate before retrieval?,2,0.23230956494808197,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MiniLM,Why would I want the LLM to hallucinate before retrieval?,3,0.19695106148719788,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MiniLM,Does HyDE work when the query uses jargon the LLM doesn't know?,1,0.6352283358573914,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MiniLM,Does HyDE work when the query uses jargon the LLM doesn't know?,2,0.32409757375717163,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MiniLM,Does HyDE work when the query uses jargon the LLM doesn't know?,3,0.30533260107040405,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MiniLM,Why do two semantically similar sentences sometimes have low cosine scores?,1,0.30699604749679565,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MiniLM,Why do two semantically similar sentences sometimes have low cosine scores?,2,0.27154141664505005,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MiniLM,Why do two semantically similar sentences sometimes have low cosine scores?,3,0.26062893867492676,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MiniLM,Can inner product and cosine similarity give different ranking results?,1,0.38246941566467285,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.          
800,80,MiniLM,Can inner product and cosine similarity give different ranking results?,2,0.3287900388240814,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MiniLM,Can inner product and cosine similarity give different ranking results?,3,0.255531370639801,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,Explain retrieval augmented generation in simple terms,1,0.6321811079978943,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MPNet,Explain retrieval augmented generation in simple terms,2,0.4723784327507019,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MPNet,Explain retrieval augmented generation in simple terms,3,0.44936424493789673,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,How does retrieval reduce hallucinations?,1,0.1787140667438507,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MPNet,How does retrieval reduce hallucinations?,2,0.14835986495018005,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MPNet,How does retrieval reduce hallucinations?,3,0.12169240415096283,RAG,rag_01::c1,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
800,80,MPNet,What is the advantage of RAG over fine-tuning?,1,0.39482539892196655,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MPNet,What is the advantage of RAG over fine-tuning?,2,0.29052668809890747,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,What is the advantage of RAG over fine-tuning?,3,0.14018115401268005,RAG,rag_01::c1,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
800,80,MPNet,What is chunking and why do we use overlap?,1,0.5491010546684265,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MPNet,What is chunking and why do we use overlap?,2,0.5349228382110596,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MPNet,What is chunking and why do we use overlap?,3,0.20898984372615814,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MPNet,How does chunk size affect retrieval quality?,1,0.5591299533843994,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MPNet,How does chunk size affect retrieval quality?,2,0.38063156604766846,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MPNet,How does chunk size affect retrieval quality?,3,0.3681075870990753,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,What is recursive character text splitting?,1,0.7990330457687378,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MPNet,What is recursive character text splitting?,2,0.40532824397087097,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MPNet,What is recursive character text splitting?,3,0.20647157728672028,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MPNet,What are text embeddings and how do they work?,1,0.5781331062316895,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MPNet,What are text embeddings and how do they work?,2,0.4726218581199646,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MPNet,What are text embeddings and how do they work?,3,0.379623681306839,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MPNet,Which embedding models are popular for RAG?,1,0.6085334420204163,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MPNet,Which embedding models are popular for RAG?,2,0.4271005094051361,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MPNet,Which embedding models are popular for RAG?,3,0.4246147871017456,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,What is FAISS and how does it work?,1,0.2173236608505249,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.          
800,80,MPNet,What is FAISS and how does it work?,2,0.2090272307395935,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,What is FAISS and how does it work?,3,0.19105933606624603,RAG,rag_01::c1,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
800,80,MPNet,What are the different FAISS index types?,1,0.2601870894432068,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.          
800,80,MPNet,What are the different FAISS index types?,2,0.22674895823001862,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,What are the different FAISS index types?,3,0.18721893429756165,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MPNet,What is HyDE in retrieval?,1,0.4175461530685425,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MPNet,What is HyDE in retrieval?,2,0.2723357081413269,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,What is HyDE in retrieval?,3,0.2342066466808319,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MPNet,How does hypothetical document embedding improve search?,1,0.6620439887046814,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MPNet,How does hypothetical document embedding improve search?,2,0.6225837469100952,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MPNet,How does hypothetical document embedding improve search?,3,0.5328607559204102,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MPNet,How do we evaluate retrieval quality?,1,0.6595370173454285,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,How do we evaluate retrieval quality?,2,0.34600818157196045,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MPNet,How do we evaluate retrieval quality?,3,0.3206771910190582,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MPNet,What is Mean Reciprocal Rank (MRR)?,1,0.3417859971523285,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,What is Mean Reciprocal Rank (MRR)?,2,0.17503181099891663,RAG,rag_01::c1,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
800,80,MPNet,What is Mean Reciprocal Rank (MRR)?,3,0.1616780012845993,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MPNet,Why does adding retrieved context help LLM answers stay accurate?,1,0.4275661110877991,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MPNet,Why does adding retrieved context help LLM answers stay accurate?,2,0.3556707203388214,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MPNet,Why does adding retrieved context help LLM answers stay accurate?,3,0.3479481041431427,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MPNet,Does overlap always improve retrieval precision?,1,0.3194406032562256,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,Does overlap always improve retrieval precision?,2,0.3165592551231384,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MPNet,Does overlap always improve retrieval precision?,3,0.3063874840736389,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MPNet,Should I retrain my model or just add a search layer?,1,0.3242405652999878,RAG,rag_01::c1,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
800,80,MPNet,Should I retrain my model or just add a search layer?,2,0.2806062400341034,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MPNet,Should I retrain my model or just add a search layer?,3,0.23986734449863434,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.          
800,80,MPNet,When is updating model weights better than external retrieval?,1,0.3716016709804535,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MPNet,When is updating model weights better than external retrieval?,2,0.3714296817779541,RAG,rag_01::c1,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
800,80,MPNet,When is updating model weights better than external retrieval?,3,0.31967946887016296,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,Can retrieval completely replace parameter updates for domain adaptation?,1,0.45537182688713074,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MPNet,Can retrieval completely replace parameter updates for domain adaptation?,2,0.41131311655044556,RAG,rag_01::c1,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
800,80,MPNet,Can retrieval completely replace parameter updates for domain adaptation?,3,0.4056025743484497,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MPNet,What happens if my chunks are too small to contain a complete answer?,1,0.3276626467704773,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MPNet,What happens if my chunks are too small to contain a complete answer?,2,0.29166173934936523,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MPNet,What happens if my chunks are too small to contain a complete answer?,3,0.2127065807580948,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MPNet,Does splitting documents always improve search precision?,1,0.567649245262146,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MPNet,Does splitting documents always improve search precision?,2,0.45165443420410156,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MPNet,Does splitting documents always improve search precision?,3,0.3317279815673828,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MPNet,How do I handle tables and code blocks when segmenting documents?,1,0.4852294921875,Chunking,chunking_02::c0,Recursive character text splitting is a popular chunking method that tries to split on natural boundaries like paragraphs and sentences before falling back to character-level splits.          This pre
800,80,MPNet,How do I handle tables and code blocks when segmenting documents?,2,0.48127317428588867,Chunking,chunking_01::c0,Chunking is the process of splitting documents into smaller pieces for indexing and retrieval.          The chunk size significantly impacts retrieval quality.          Small chunks (100-200 tokens) p
800,80,MPNet,How do I handle tables and code blocks when segmenting documents?,3,0.2559766173362732,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MPNet,Is generating a fake answer before search the same as fine-tuning?,1,0.3447498083114624,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MPNet,Is generating a fake answer before search the same as fine-tuning?,2,0.30706408619880676,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MPNet,Is generating a fake answer before search the same as fine-tuning?,3,0.2993101477622986,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MPNet,Why would I want the LLM to hallucinate before retrieval?,1,0.16076676547527313,RAG,rag_01::c1,he model needs access to up-to-date or domain-specific information that wasn't part of its training data.
800,80,MPNet,Why would I want the LLM to hallucinate before retrieval?,2,0.15324139595031738,RAG,rag_01::c0,"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.          Instead of relying solely on the knowledge stored in model parameters,          R"
800,80,MPNet,Why would I want the LLM to hallucinate before retrieval?,3,0.1330987513065338,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MPNet,Does HyDE work when the query uses jargon the LLM doesn't know?,1,0.5220791101455688,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MPNet,Does HyDE work when the query uses jargon the LLM doesn't know?,2,0.2827390730381012,RAG,rag_02::c0,The key advantage of RAG over fine-tuning is that the knowledge base can be updated without retraining the model.          This makes RAG cost-effective and flexible for enterprise applications.      
800,80,MPNet,Does HyDE work when the query uses jargon the LLM doesn't know?,3,0.2825300693511963,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,Why do two semantically similar sentences sometimes have low cosine scores?,1,0.2735658884048462,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MPNet,Why do two semantically similar sentences sometimes have low cosine scores?,2,0.2252364605665207,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
800,80,MPNet,Why do two semantically similar sentences sometimes have low cosine scores?,3,0.20106391608715057,HyDE,hyde_01::c0,HyDE (Hypothetical Document Embeddings) is an advanced retrieval technique that improves search quality          by generating a hypothetical answer before retrieval.          Instead of embedding the
800,80,MPNet,Can inner product and cosine similarity give different ranking results?,1,0.3807494342327118,VectorDB,vectordb_01::c0,Vector databases store embeddings and enable fast similarity search at scale.          FAISS (Facebook AI Similarity Search) is a popular open-source library for efficient similarity search.          
800,80,MPNet,Can inner product and cosine similarity give different ranking results?,2,0.27919477224349976,Embeddings,embeddings_01::c0,"Text embeddings are dense vector representations that capture semantic meaning.          In RAG systems, both documents and queries are converted to embeddings,          and similarity search finds th"
800,80,MPNet,Can inner product and cosine similarity give different ranking results?,3,0.27506422996520996,Evaluation,evaluation_01::c0,"Evaluating RAG systems requires measuring both retrieval quality and generation quality.          Retrieval metrics include Hit@K (whether relevant documents appear in top K results),          Mean Re"
