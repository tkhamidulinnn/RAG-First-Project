{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61464f1c",
   "metadata": {},
   "source": [
    "# Week 3: LLM Integration and Prompts\n",
    "\n",
    "**Pipeline continuity:**\n",
    "- **Week 1 output:** model/metric decisions (`MPNet` operational, `cosine` champion).\n",
    "- **Week 2 output:** working local retriever (PDF -> chunk -> embed -> FAISS -> top-k).\n",
    "- **Week 3 focus:** keep the same retrieval layer and add generation (prompting + local LLM).\n",
    "\n",
    "**Goal:** end-to-end RAG flow for one query cycle:\n",
    "`query -> retrieve context (Week 2) -> build prompt -> LLM answer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf2c133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tkhamidulin/Desktop/First Project - RAG/rag_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "faiss.omp_set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d17d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/tkhamidulin/Desktop/First Project - RAG/artifacts/week2_retrieval_debug.jsonl'),\n",
       " PosixPath('/Users/tkhamidulin/Desktop/First Project - RAG/artifacts'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same paths as Week 2 (notebook is in notebooks/)\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "W3_EXPERIMENTS_CSV = ARTIFACTS_DIR / \"week3_prompt_experiments.csv\"\n",
    "W3_GENERATIONS_JSONL = ARTIFACTS_DIR / \"week3_generations.jsonl\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data dir: {DATA_DIR} (exists: {DATA_DIR.exists()})\")\n",
    "print(f\"Artifacts: {ARTIFACTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5d37a9",
   "metadata": {},
   "source": [
    "---\n",
    "## From Week 1 and Week 2: Retrieval Pipeline\n",
    "\n",
    "**From Week 1:** Используем ту же модель эмбеддингов (MPNet) и метрику (cosine via normalized embeddings + IndexFlatIP).\n",
    "\n",
    "**From Week 2:** Тот же путь данных, загрузка PDF, чанкинг и классы VectorStore и RAG. Мы повторяем их здесь, чтобы Week 3 работал сам по себе и явно опирался на Week 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f2cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- From Week 2: load PDFs and chunking ---\n",
    "def load_pdfs(data_dir: Path):\n",
    "    docs = []\n",
    "    for topic_dir in sorted(data_dir.iterdir()):\n",
    "        if not topic_dir.is_dir():\n",
    "            continue\n",
    "        topic = topic_dir.name\n",
    "        for pdf_path in topic_dir.glob(\"*.pdf\"):\n",
    "            try:\n",
    "                loader = PyPDFLoader(str(pdf_path))\n",
    "                for doc in loader.load():\n",
    "                    doc.metadata[\"topic\"] = topic\n",
    "                    doc.metadata[\"source\"] = pdf_path.name\n",
    "                    docs.append(doc)\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: {pdf_path.name} - {e}\")\n",
    "    return docs\n",
    "\n",
    "def chunk_documents(docs, chunk_size: int, chunk_overlap: int, separators=None):\n",
    "    if separators is None:\n",
    "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=separators)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "# --- From Week 2: VectorStore and RAG (Week 1 model + cosine) ---\n",
    "EMBEDDING_MODEL_ID = \"all-mpnet-base-v2\"\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, model_id: str = EMBEDDING_MODEL_ID):\n",
    "        self.model = SentenceTransformer(model_id)\n",
    "        self.chunks = []\n",
    "        self._index = None\n",
    "\n",
    "    def build_index(self, chunks):\n",
    "        self.chunks = list(chunks)\n",
    "        if not self.chunks:\n",
    "            raise ValueError(\"Cannot index: chunks list is empty\")\n",
    "        texts = [c.page_content for c in self.chunks]\n",
    "        emb = self.model.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
    "        emb = np.ascontiguousarray(emb, dtype=np.float32)\n",
    "        self._index = faiss.IndexFlatIP(emb.shape[1])\n",
    "        self._index.add(emb)\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 3):\n",
    "        if self._index is None or self._index.ntotal == 0:\n",
    "            raise ValueError(\"Index is empty; call build_index(chunks) first\")\n",
    "        q = self.model.encode([query], normalize_embeddings=True)\n",
    "        q = np.ascontiguousarray(q, dtype=np.float32)\n",
    "        k = min(top_k, self._index.ntotal)\n",
    "        scores, indices = self._index.search(q, k)\n",
    "        return scores[0], indices[0]\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, vector_store: VectorStore):\n",
    "        self.vector_store = vector_store\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 3):\n",
    "        scores, indices = self.vector_store.retrieve(query, top_k=top_k)\n",
    "        return [(float(s), self.vector_store.chunks[i]) for s, i in zip(scores, indices)]\n",
    "\n",
    "print(\"Loaded: load_pdfs, chunk_documents, VectorStore, RAG (same as Week 2). Embedding model:\", EMBEDDING_MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe71939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Week 2 pipeline: load PDFs -> chunk -> build index -> RAG\n",
    "raw_docs = load_pdfs(DATA_DIR)\n",
    "CHUNK_STRATEGY_A = {\"chunk_size\": 300, \"chunk_overlap\": 50}\n",
    "chunks = chunk_documents(raw_docs, **CHUNK_STRATEGY_A)\n",
    "\n",
    "store = VectorStore()\n",
    "store.build_index(chunks)\n",
    "rag = RAG(store)\n",
    "\n",
    "print(f\"Documents: {len(raw_docs)} pages -> {len(chunks)} chunks (Strategy A)\")\n",
    "print(\"RAG retriever ready. Use rag.retrieve(query, top_k=3) to get (score, chunk) list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dff120",
   "metadata": {},
   "source": [
    "## Local LLM (Ollama) via LangChain\n",
    "\n",
    "This notebook uses a local model through Ollama to keep experiments reproducible and offline-friendly.\n",
    "\n",
    "Example setup (outside notebook):\n",
    "- `ollama pull llama3.1` (or another model)\n",
    "- Ensure Ollama is running (default: `http://localhost:11434`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628a7d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"gemma3:4b\"   # exactly as in `ollama list`\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "TEMPERATURE_DEFAULT = 0.2\n",
    "\n",
    "def build_llm(temperature: float) -> OllamaLLM:\n",
    "    \"\"\"\n",
    "    Build a LangChain-compatible Ollama LLM.\n",
    "    Using a factory function avoids duplication across experiments.\n",
    "    \"\"\"\n",
    "    return OllamaLLM(\n",
    "        model=MODEL_NAME,\n",
    "        temperature=temperature,\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        validate_model_on_init=True,  # fail fast if model is missing\n",
    "    )\n",
    "\n",
    "# Smoke test\n",
    "build_llm(TEMPERATURE_DEFAULT).invoke(\"Reply: OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb8ed6",
   "metadata": {},
   "source": [
    "## Retrieval: Week 2 Pipeline Inside Week 3\n",
    "\n",
    "Контекст для LLM берём **здесь же**: для каждого запроса вызываем `rag.retrieve(query, top_k=3)` (тот же pipeline, что в Week 2). Файл Week 2 не нужен.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c0a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded retrieval rows: 246\n",
      "Keys: ['query', 'chunks', 'scores', 'meta']\n",
      "Chunks in first row: 0\n"
     ]
    }
   ],
   "source": [
    "def get_retrieval_for_query(query: str, top_k: int = 3) -> dict:\n",
    "    \"\"\"Use Week 2 pipeline: rag.retrieve() returns (score, chunk). Build row for prompt.\"\"\"\n",
    "    retrieved = rag.retrieve(query, top_k=top_k)\n",
    "    chunks = [c.page_content for _, c in retrieved]\n",
    "    scores = [s for s, _ in retrieved]\n",
    "    return {\"query\": query, \"chunks\": chunks, \"scores\": scores, \"meta\": {}}\n",
    "\n",
    "# Example: one query to verify retrieval works\n",
    "example = get_retrieval_for_query(\"What is RAG?\")\n",
    "print(f\"Example retrieval: query={example['query'][:40]}...\")\n",
    "print(f\"  Top-k chunks: {len(example['chunks'])}, scores: {[round(s, 4) for s in example['scores']]}\")\n",
    "print(f\"  First chunk preview: {example['chunks'][0][:80]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7317ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(chunks: list[str], scores: list[float] | None = None, max_chars: int = 7000) -> str:\n",
    "    parts = []\n",
    "    for i, ch in enumerate(chunks):\n",
    "        score_str = f\" (score={scores[i]:.4f})\" if scores is not None else \"\"\n",
    "        parts.append(f\"[Chunk {i+1}{score_str}]\\n{ch}\")\n",
    "    ctx = \"\\n\\n\".join(parts)\n",
    "    return ctx[:max_chars]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7afb77",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "I define three task-specific prompt templates:\n",
    "\n",
    "1) **Strict Q&A**  \n",
    "   - Answer using ONLY context  \n",
    "   - If missing: exact refusal message  \n",
    "   - Require chunk citations\n",
    "\n",
    "2) **Structured Summary**  \n",
    "   - Extract key points, definitions, practical notes, and gaps\n",
    "\n",
    "3) **Grounded Reasoning**  \n",
    "   - Provide short reasoning steps tied to evidence  \n",
    "   - Avoid outside knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51cd4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_STRICT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are a RAG assistant.\\n\"\n",
    "        \"You MUST answer using ONLY the provided CONTEXT.\\n\"\n",
    "        \"If the answer is not in the context, reply exactly:\\n\"\n",
    "        \"\\\"I don't know based on the provided context.\\\"\\n\\n\"\n",
    "        \"CONTEXT:\\n{context}\\n\\n\"\n",
    "        \"QUESTION:\\n{question}\\n\\n\"\n",
    "        \"RULES:\\n\"\n",
    "        \"- No outside knowledge.\\n\"\n",
    "        \"- Be concise.\\n\"\n",
    "        \"- Cite chunks like [Chunk 2].\\n\\n\"\n",
    "        \"ANSWER:\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "SUMMARY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"topic\"],\n",
    "    template=(\n",
    "        \"Summarize the provided context about: {topic}\\n\\n\"\n",
    "        \"CONTEXT:\\n{context}\\n\\n\"\n",
    "        \"OUTPUT FORMAT:\\n\"\n",
    "        \"- Key points (3–7 bullets)\\n\"\n",
    "        \"- Definitions (if any)\\n\"\n",
    "        \"- Practical notes (if any)\\n\"\n",
    "        \"- Missing information / open questions (if any)\\n\\n\"\n",
    "        \"SUMMARY:\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "REASONING_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are an analyst. Use ONLY the provided context.\\n\"\n",
    "        \"Do not add outside knowledge.\\n\\n\"\n",
    "        \"CONTEXT:\\n{context}\\n\\n\"\n",
    "        \"QUESTION:\\n{question}\\n\\n\"\n",
    "        \"OUTPUT FORMAT:\\n\"\n",
    "        \"1) Answer (1–3 sentences)\\n\"\n",
    "        \"2) Evidence: cite chunks like [Chunk 1]\\n\"\n",
    "        \"3) Reasoning: 3–6 short bullet steps tied to evidence\\n\\n\"\n",
    "        \"RESPONSE:\\n\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a964e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval is done by get_retrieval_for_query(question) — no find_retrieval_row needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc2cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_id(*parts) -> str:\n",
    "    s = \"||\".join(map(str, parts))\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def run_generation(\n",
    "    question: str,\n",
    "    retrieval_row: dict,\n",
    "    prompt: PromptTemplate,\n",
    "    template_name: str,\n",
    "    temperature: float,\n",
    "    topic: str = \"RAG notes\"\n",
    ") -> dict:\n",
    "    llm = build_llm(temperature)\n",
    "\n",
    "    context = format_context(retrieval_row[\"chunks\"], retrieval_row[\"scores\"])\n",
    "    kwargs = {\"context\": context}\n",
    "\n",
    "    if \"question\" in prompt.input_variables:\n",
    "        kwargs[\"question\"] = question\n",
    "    if \"topic\" in prompt.input_variables:\n",
    "        kwargs[\"topic\"] = topic\n",
    "\n",
    "    prompt_text = prompt.format(**kwargs)\n",
    "\n",
    "    t0 = time.time()\n",
    "    answer = llm.invoke(prompt_text)\n",
    "    latency = round(time.time() - t0, 3)\n",
    "\n",
    "    answer = (answer or \"\").strip()\n",
    "\n",
    "    return {\n",
    "        \"run_id\": stable_id(question, template_name, MODEL_NAME, temperature),\n",
    "        \"question\": question,\n",
    "        \"template\": template_name,\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"temperature\": temperature,\n",
    "        \"latency_s\": latency,\n",
    "        \"prompt_chars\": len(prompt_text),\n",
    "        \"context_chars\": len(context),\n",
    "        \"n_chunks\": len(retrieval_row[\"chunks\"]),\n",
    "        \"retrieval_query_used\": retrieval_row[\"query\"],\n",
    "        \"answer\": answer,\n",
    "        \"idk_flag\": (\"i don't know based on the provided context\" in answer.lower()),\n",
    "        \"has_citation_flag\": (\"[chunk\" in answer.lower()),  # lightweight heuristic\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e57dc",
   "metadata": {},
   "source": [
    "## Test Set\n",
    "\n",
    "A small, diverse set of questions is used to compare prompt behavior:\n",
    "- definitions (low risk)\n",
    "- “why” questions (higher hallucination risk)\n",
    "- comparison questions\n",
    "- pipeline-level questions\n",
    "\n",
    "This is a controlled prompt experiment, not a full benchmark yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9dc1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"What is Retrieval-Augmented Generation (RAG)?\",\n",
    "    \"Why do we use chunk overlap in retrieval systems?\",\n",
    "    \"Explain cosine similarity vs dot product similarity for embeddings.\",\n",
    "    \"What is the role of FAISS in a RAG pipeline?\",\n",
    "    \"When would you increase chunk size and why?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95e98a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>question</th>\n",
       "      <th>template</th>\n",
       "      <th>model</th>\n",
       "      <th>temperature</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>prompt_chars</th>\n",
       "      <th>context_chars</th>\n",
       "      <th>n_chunks</th>\n",
       "      <th>retrieval_query_used</th>\n",
       "      <th>answer</th>\n",
       "      <th>idk_flag</th>\n",
       "      <th>has_citation_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>773148cfcd0d205f</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>qa_strict</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.294</td>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explain retrieval augmented generation in simp...</td>\n",
       "      <td>I don't know based on the provided context.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ed2bea5911a9d13d</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>qa_strict</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.496</td>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explain retrieval augmented generation in simp...</td>\n",
       "      <td>I don't know based on the provided context.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>223dbe53cb99f7e7</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>summary</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.585</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explain retrieval augmented generation in simp...</td>\n",
       "      <td>Please provide the context about RAG/retrieval...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ec596e6f77b05533</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>summary</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.440</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explain retrieval augmented generation in simp...</td>\n",
       "      <td>Please provide the context about RAG/retrieval...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83b5662850e2bfeb</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.530</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explain retrieval augmented generation in simp...</td>\n",
       "      <td>1) Retrieval-Augmented Generation (RAG) is a t...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             run_id                                       question   template  \\\n",
       "0  773148cfcd0d205f  What is Retrieval-Augmented Generation (RAG)?  qa_strict   \n",
       "1  ed2bea5911a9d13d  What is Retrieval-Augmented Generation (RAG)?  qa_strict   \n",
       "2  223dbe53cb99f7e7  What is Retrieval-Augmented Generation (RAG)?    summary   \n",
       "3  ec596e6f77b05533  What is Retrieval-Augmented Generation (RAG)?    summary   \n",
       "4  83b5662850e2bfeb  What is Retrieval-Augmented Generation (RAG)?  reasoning   \n",
       "\n",
       "       model  temperature  latency_s  prompt_chars  context_chars  n_chunks  \\\n",
       "0  gemma3:4b          0.0      1.294           325              0         0   \n",
       "1  gemma3:4b          0.2      0.496           325              0         0   \n",
       "2  gemma3:4b          0.0      1.585           229              0         0   \n",
       "3  gemma3:4b          0.2      1.440           229              0         0   \n",
       "4  gemma3:4b          0.0      7.530           296              0         0   \n",
       "\n",
       "                                retrieval_query_used  \\\n",
       "0  Explain retrieval augmented generation in simp...   \n",
       "1  Explain retrieval augmented generation in simp...   \n",
       "2  Explain retrieval augmented generation in simp...   \n",
       "3  Explain retrieval augmented generation in simp...   \n",
       "4  Explain retrieval augmented generation in simp...   \n",
       "\n",
       "                                              answer  idk_flag  \\\n",
       "0        I don't know based on the provided context.      True   \n",
       "1        I don't know based on the provided context.      True   \n",
       "2  Please provide the context about RAG/retrieval...     False   \n",
       "3  Please provide the context about RAG/retrieval...     False   \n",
       "4  1) Retrieval-Augmented Generation (RAG) is a t...     False   \n",
       "\n",
       "   has_citation_flag  \n",
       "0              False  \n",
       "1              False  \n",
       "2              False  \n",
       "3              False  \n",
       "4               True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates = [\n",
    "    (\"qa_strict\", QA_STRICT_PROMPT),\n",
    "    (\"summary\", SUMMARY_PROMPT),\n",
    "    (\"reasoning\", REASONING_PROMPT),\n",
    "]\n",
    "\n",
    "temperature_grid = [0.0, 0.2]  # compare deterministic vs slightly creative\n",
    "\n",
    "records = []\n",
    "for q in test_questions:\n",
    "    r = get_retrieval_for_query(q)\n",
    "    for (name, tpl) in templates:\n",
    "        for temp in temperature_grid:\n",
    "            rec = run_generation(\n",
    "                question=q,\n",
    "                retrieval_row=r,\n",
    "                prompt=tpl,\n",
    "                template_name=name,\n",
    "                temperature=temp,\n",
    "                topic=\"RAG / retrieval fundamentals\"\n",
    "            )\n",
    "            records.append(rec)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f233eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/tkhamidulin/Desktop/First Project - RAG/artifacts/week3_prompt_experiments.csv\n",
      "Saved: /Users/tkhamidulin/Desktop/First Project - RAG/artifacts/week3_generations.jsonl\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(W3_EXPERIMENTS_CSV, index=False)\n",
    "\n",
    "with open(W3_GENERATIONS_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in records:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", W3_EXPERIMENTS_CSV)\n",
    "print(\"Saved:\", W3_GENERATIONS_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d96db46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>template</th>\n",
       "      <th>temperature</th>\n",
       "      <th>n</th>\n",
       "      <th>avg_latency_s</th>\n",
       "      <th>avg_answer_len</th>\n",
       "      <th>idk_rate</th>\n",
       "      <th>citation_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qa_strict</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6082</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa_strict</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4452</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.9418</td>\n",
       "      <td>1210.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>5.5042</td>\n",
       "      <td>1152.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.5362</td>\n",
       "      <td>267.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summary</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.4084</td>\n",
       "      <td>267.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    template  temperature  n  avg_latency_s  avg_answer_len  idk_rate  \\\n",
       "0  qa_strict          0.0  5         0.6082            43.0       1.0   \n",
       "1  qa_strict          0.2  5         0.4452            43.0       1.0   \n",
       "2  reasoning          0.0  5         5.9418          1210.2       0.0   \n",
       "3  reasoning          0.2  5         5.5042          1152.6       0.0   \n",
       "4    summary          0.0  5         1.5362           267.0       0.0   \n",
       "5    summary          0.2  5         1.4084           267.0       0.0   \n",
       "\n",
       "   citation_rate  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.8  \n",
       "3            0.8  \n",
       "4            0.0  \n",
       "5            0.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"answer_len\"] = df[\"answer\"].fillna(\"\").str.len()\n",
    "\n",
    "summary = (\n",
    "    df.groupby([\"template\", \"temperature\"])\n",
    "      .agg(\n",
    "          n=(\"run_id\", \"count\"),\n",
    "          avg_latency_s=(\"latency_s\", \"mean\"),\n",
    "          avg_answer_len=(\"answer_len\", \"mean\"),\n",
    "          idk_rate=(\"idk_flag\", \"mean\"),\n",
    "          citation_rate=(\"has_citation_flag\", \"mean\"),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values([\"template\", \"temperature\"])\n",
    ")\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a771e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>template</th>\n",
       "      <th>temperature</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qa_strict</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I don't know based on the provided context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa_strict</td>\n",
       "      <td>0.2</td>\n",
       "      <td>I don't know based on the provided context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Please provide the context about RAG/retrieval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summary</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Please provide the context about RAG/retrieval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1) Retrieval-Augmented Generation (RAG) is a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1) Retrieval-Augmented Generation (RAG) is a t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    template  temperature                                             answer\n",
       "0  qa_strict          0.0        I don't know based on the provided context.\n",
       "1  qa_strict          0.2        I don't know based on the provided context.\n",
       "2    summary          0.0  Please provide the context about RAG/retrieval...\n",
       "3    summary          0.2  Please provide the context about RAG/retrieval...\n",
       "4  reasoning          0.0  1) Retrieval-Augmented Generation (RAG) is a t...\n",
       "5  reasoning          0.2  1) Retrieval-Augmented Generation (RAG) is a t..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = test_questions[0]\n",
    "df[df[\"question\"] == q][[\"template\", \"temperature\", \"answer\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d483b19",
   "metadata": {},
   "source": [
    "## Conclusions (Week 3)\n",
    "\n",
    "### Prompt behavior findings\n",
    "- **Strict Q&A**: most grounded and consistent with refusal rules.\n",
    "- **Summary style**: best readability, but may drop details needed for precise Q&A.\n",
    "- **Reasoning style**: better explanations, but more sensitive to weak context.\n",
    "\n",
    "### Retrieval-to-generation dependency\n",
    "Answer quality is directly limited by retrieval quality from Week 2: weak context leads to weaker grounded answers regardless of prompt style.\n",
    "\n",
    "### Handoff to Week 4\n",
    "Week 4 adds guardrails around this Week 3 pipeline: pre-checks before LLM calls and post-checks on outputs to reduce hallucinations, leakage, and unsafe responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4718d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/tkhamidulin/Desktop/First Project - RAG/artifacts/week3_summary.md\n"
     ]
    }
   ],
   "source": [
    "summary_text = f\"\"\"# Week 3 — Prompt Engineering Summary\n",
    "\n",
    "## Setup\n",
    "- LLM: Ollama via LangChain\n",
    "- Model: {MODEL_NAME}\n",
    "- Templates: qa_strict, summary, reasoning\n",
    "- Temperatures tested: {temperature_grid}\n",
    "\n",
    "## Quick metrics (proxy)\n",
    "{summary.to_string(index=False)}\n",
    "\n",
    "## Notes (fill after manual review)\n",
    "- qa_strict:\n",
    "- summary:\n",
    "- reasoning:\n",
    "\n",
    "## Observed failure modes\n",
    "- \n",
    "\n",
    "## Week 4 plan\n",
    "- add guardrails for weak retrieval\n",
    "- enforce minimum-evidence rules before answering\n",
    "- improve refusal behavior and citation enforcement\n",
    "\"\"\"\n",
    "\n",
    "W3_SUMMARY_MD.write_text(summary_text, encoding=\"utf-8\")\n",
    "print(\"Saved:\", W3_SUMMARY_MD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a363a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}