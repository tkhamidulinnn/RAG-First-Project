{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61464f1c",
   "metadata": {},
   "source": [
    "# Week 3 — LLM Integration & Prompt Templates (LangChain + Ollama)\n",
    "\n",
    "**Objective:** Add the generation layer on top of Week 2 retrieval by integrating a local LLM (Ollama) with LangChain.  \n",
    "**Focus:** Prompt engineering experiments (Q&A vs Summary vs Grounded Reasoning) and how prompt structure changes outputs.\n",
    "\n",
    "**Outputs (artifacts):**\n",
    "- `artifacts/week3_prompt_experiments.csv`\n",
    "- `artifacts/week3_generations.jsonl`\n",
    "- `artifacts/week3_summary.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf2c133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tkhamidulin/Desktop/First Project - RAG/rag_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d17d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/tkhamidulin/Desktop/First Project - RAG/artifacts/week2_retrieval_debug.jsonl'),\n",
       " PosixPath('/Users/tkhamidulin/Desktop/First Project - RAG/artifacts'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Week 2 retrieval log to reuse (keeps Week 3 focused on prompting)\n",
    "W2_RETRIEVAL_JSONL = ARTIFACTS_DIR / \"week2_retrieval_debug.jsonl\"\n",
    "\n",
    "# Week 3 outputs\n",
    "W3_EXPERIMENTS_CSV = ARTIFACTS_DIR / \"week3_prompt_experiments.csv\"\n",
    "W3_GENERATIONS_JSONL = ARTIFACTS_DIR / \"week3_generations.jsonl\"\n",
    "W3_SUMMARY_MD = ARTIFACTS_DIR / \"week3_summary.md\"\n",
    "\n",
    "W2_RETRIEVAL_JSONL, ARTIFACTS_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dff120",
   "metadata": {},
   "source": [
    "## Local LLM (Ollama) via LangChain\n",
    "\n",
    "This notebook uses a local model through Ollama to keep experiments reproducible and offline-friendly.\n",
    "\n",
    "Example setup (outside notebook):\n",
    "- `ollama pull llama3.1` (or another model)\n",
    "- Ensure Ollama is running (default: `http://localhost:11434`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628a7d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"gemma3:4b\"   # exactly as in `ollama list`\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "TEMPERATURE_DEFAULT = 0.2\n",
    "\n",
    "def build_llm(temperature: float) -> OllamaLLM:\n",
    "    \"\"\"\n",
    "    Build a LangChain-compatible Ollama LLM.\n",
    "    Using a factory function avoids duplication across experiments.\n",
    "    \"\"\"\n",
    "    return OllamaLLM(\n",
    "        model=MODEL_NAME,\n",
    "        temperature=temperature,\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        validate_model_on_init=True,  # fail fast if model is missing\n",
    "    )\n",
    "\n",
    "# Smoke test\n",
    "build_llm(TEMPERATURE_DEFAULT).invoke(\"Reply: OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb8ed6",
   "metadata": {},
   "source": [
    "## Loading Retrieval Context from Week 2\n",
    "\n",
    "Week 2 already produced retrieval logs (queries → top chunks + scores).  \n",
    "Week 3 reuses those logs to isolate the effect of prompt structure on generation.\n",
    "\n",
    "If your schema differs, normalization handles multiple field names (e.g., `chunks`, `chunk_texts`, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c0a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded retrieval rows: 246\n",
      "Keys: ['query', 'chunks', 'scores', 'meta']\n",
      "Chunks in first row: 0\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(path: Path) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Load a JSONL file into a list of dicts.\n",
    "    Fails fast with a clear error if a line cannot be parsed.\n",
    "    \"\"\"\n",
    "    rows: list[dict] = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"Invalid JSON on line {i} in {path}: {e}\") from e\n",
    "    return rows\n",
    "\n",
    "\n",
    "def normalize_retrieval_row(r: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Normalize Week 2 retrieval log rows into a stable schema:\n",
    "    {\n",
    "      \"query\": str,\n",
    "      \"chunks\": list[str],\n",
    "      \"scores\": list[float] | None,\n",
    "      \"meta\": dict\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    query = r.get(\"query\") or r.get(\"question\") or r.get(\"q\") or \"\"\n",
    "\n",
    "    chunks = (\n",
    "        r.get(\"chunks\")\n",
    "        or r.get(\"chunk_texts\")\n",
    "        or r.get(\"texts\")\n",
    "        or r.get(\"retrieved_chunks\")\n",
    "        or []\n",
    "    )\n",
    "\n",
    "    scores = r.get(\"scores\") or r.get(\"similarities\") or r.get(\"distances\")\n",
    "\n",
    "    # defensive cleanup\n",
    "    if chunks is None:\n",
    "        chunks = []\n",
    "    if not isinstance(chunks, list):\n",
    "        chunks = [str(chunks)]\n",
    "\n",
    "    if scores is not None and (not isinstance(scores, list) or len(scores) != len(chunks)):\n",
    "        scores = None\n",
    "\n",
    "    meta = {k: v for k, v in r.items() if k not in [\n",
    "        \"query\", \"question\", \"q\",\n",
    "        \"chunks\", \"chunk_texts\", \"texts\", \"retrieved_chunks\",\n",
    "        \"scores\", \"similarities\", \"distances\"\n",
    "    ]}\n",
    "\n",
    "    return {\"query\": str(query), \"chunks\": [str(c) for c in chunks], \"scores\": scores, \"meta\": meta}\n",
    "\n",
    "\n",
    "raw_rows = load_jsonl(W2_RETRIEVAL_JSONL)\n",
    "retrieval_rows = [normalize_retrieval_row(r) for r in raw_rows]\n",
    "\n",
    "if not retrieval_rows:\n",
    "    raise ValueError(f\"No rows loaded from {W2_RETRIEVAL_JSONL}. File may be empty.\")\n",
    "\n",
    "print(\"Loaded retrieval rows:\", len(retrieval_rows))\n",
    "print(\"Keys:\", list(retrieval_rows[0].keys()))\n",
    "print(\"Chunks in first row:\", len(retrieval_rows[0][\"chunks\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7317ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(chunks: list[str], scores: list[float] | None = None, max_chars: int = 7000) -> str:\n",
    "    parts = []\n",
    "    for i, ch in enumerate(chunks):\n",
    "        score_str = f\" (score={scores[i]:.4f})\" if scores is not None else \"\"\n",
    "        parts.append(f\"[Chunk {i+1}{score_str}]\\n{ch}\")\n",
    "    ctx = \"\\n\\n\".join(parts)\n",
    "    return ctx[:max_chars]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7afb77",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "I define three task-specific prompt templates:\n",
    "\n",
    "1) **Strict Q&A**  \n",
    "   - Answer using ONLY context  \n",
    "   - If missing: exact refusal message  \n",
    "   - Require chunk citations\n",
    "\n",
    "2) **Structured Summary**  \n",
    "   - Extract key points, definitions, practical notes, and gaps\n",
    "\n",
    "3) **Grounded Reasoning**  \n",
    "   - Provide short reasoning steps tied to evidence  \n",
    "   - Avoid outside knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51cd4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_STRICT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are a RAG assistant.\\n\"\n",
    "        \"You MUST answer using ONLY the provided CONTEXT.\\n\"\n",
    "        \"If the answer is not in the context, reply exactly:\\n\"\n",
    "        \"\\\"I don't know based on the provided context.\\\"\\n\\n\"\n",
    "        \"CONTEXT:\\n{context}\\n\\n\"\n",
    "        \"QUESTION:\\n{question}\\n\\n\"\n",
    "        \"RULES:\\n\"\n",
    "        \"- No outside knowledge.\\n\"\n",
    "        \"- Be concise.\\n\"\n",
    "        \"- Cite chunks like [Chunk 2].\\n\\n\"\n",
    "        \"ANSWER:\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "SUMMARY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"topic\"],\n",
    "    template=(\n",
    "        \"Summarize the provided context about: {topic}\\n\\n\"\n",
    "        \"CONTEXT:\\n{context}\\n\\n\"\n",
    "        \"OUTPUT FORMAT:\\n\"\n",
    "        \"- Key points (3–7 bullets)\\n\"\n",
    "        \"- Definitions (if any)\\n\"\n",
    "        \"- Practical notes (if any)\\n\"\n",
    "        \"- Missing information / open questions (if any)\\n\\n\"\n",
    "        \"SUMMARY:\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "REASONING_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are an analyst. Use ONLY the provided context.\\n\"\n",
    "        \"Do not add outside knowledge.\\n\\n\"\n",
    "        \"CONTEXT:\\n{context}\\n\\n\"\n",
    "        \"QUESTION:\\n{question}\\n\\n\"\n",
    "        \"OUTPUT FORMAT:\\n\"\n",
    "        \"1) Answer (1–3 sentences)\\n\"\n",
    "        \"2) Evidence: cite chunks like [Chunk 1]\\n\"\n",
    "        \"3) Reasoning: 3–6 short bullet steps tied to evidence\\n\\n\"\n",
    "        \"RESPONSE:\\n\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a964e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_retrieval_row(question: str, rows: list[dict]) -> dict:\n",
    "    # exact match first\n",
    "    for r in rows:\n",
    "        if r[\"query\"].strip() == question.strip():\n",
    "            return r\n",
    "    # fallback: return first row (still allows prompt behavior testing)\n",
    "    return rows[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc2cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_id(*parts) -> str:\n",
    "    s = \"||\".join(map(str, parts))\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def run_generation(\n",
    "    question: str,\n",
    "    retrieval_row: dict,\n",
    "    prompt: PromptTemplate,\n",
    "    template_name: str,\n",
    "    temperature: float,\n",
    "    topic: str = \"RAG notes\"\n",
    ") -> dict:\n",
    "    llm = build_llm(temperature)\n",
    "\n",
    "    context = format_context(retrieval_row[\"chunks\"], retrieval_row[\"scores\"])\n",
    "    kwargs = {\"context\": context}\n",
    "\n",
    "    if \"question\" in prompt.input_variables:\n",
    "        kwargs[\"question\"] = question\n",
    "    if \"topic\" in prompt.input_variables:\n",
    "        kwargs[\"topic\"] = topic\n",
    "\n",
    "    prompt_text = prompt.format(**kwargs)\n",
    "\n",
    "    t0 = time.time()\n",
    "    answer = llm.invoke(prompt_text)\n",
    "    latency = round(time.time() - t0, 3)\n",
    "\n",
    "    answer = (answer or \"\").strip()\n",
    "\n",
    "    return {\n",
    "        \"run_id\": stable_id(question, template_name, MODEL_NAME, temperature),\n",
    "        \"question\": question,\n",
    "        \"template\": template_name,\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"temperature\": temperature,\n",
    "        \"latency_s\": latency,\n",
    "        \"prompt_chars\": len(prompt_text),\n",
    "        \"context_chars\": len(context),\n",
    "        \"n_chunks\": len(retrieval_row[\"chunks\"]),\n",
    "        \"retrieval_query_used\": retrieval_row[\"query\"],\n",
    "        \"answer\": answer,\n",
    "        \"idk_flag\": (\"i don't know based on the provided context\" in answer.lower()),\n",
    "        \"has_citation_flag\": (\"[chunk\" in answer.lower()),  # lightweight heuristic\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e57dc",
   "metadata": {},
   "source": [
    "## Test Set\n",
    "\n",
    "A small, diverse set of questions is used to compare prompt behavior:\n",
    "- definitions (low risk)\n",
    "- “why” questions (higher hallucination risk)\n",
    "- comparison questions\n",
    "- pipeline-level questions\n",
    "\n",
    "This is a controlled prompt experiment, not a full benchmark yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9dc1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"What is Retrieval-Augmented Generation (RAG)?\",\n",
    "    \"Why do we use chunk overlap in retrieval systems?\",\n",
    "    \"Explain cosine similarity vs dot product similarity for embeddings.\",\n",
    "    \"What is the role of FAISS in a RAG pipeline?\",\n",
    "    \"When would you increase chunk size and why?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95e98a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>question</th>\n",
       "      <th>template</th>\n",
       "      <th>model</th>\n",
       "      <th>temperature</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>prompt_chars</th>\n",
       "      <th>context_chars</th>\n",
       "      <th>n_chunks</th>\n",
       "      <th>retrieval_query_used</th>\n",
       "      <th>answer</th>\n",
       "      <th>idk_flag</th>\n",
       "      <th>has_citation_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>773148cfcd0d205f</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>qa_strict</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.294</td>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explain retrieval augmented generation in simp...</td>\n",
       "      <td>I don't know based on the provided context.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ed2bea5911a9d13d</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>qa_strict</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.496</td>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explain retrieval augmented generation in simp...</td>\n",
       "      <td>I don't know based on the provided context.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>223dbe53cb99f7e7</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>summary</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.585</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explain retrieval augmented generation in simp...</td>\n",
       "      <td>Please provide the context about RAG/retrieval...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ec596e6f77b05533</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>summary</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.440</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explain retrieval augmented generation in simp...</td>\n",
       "      <td>Please provide the context about RAG/retrieval...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83b5662850e2bfeb</td>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG)?</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>gemma3:4b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.530</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explain retrieval augmented generation in simp...</td>\n",
       "      <td>1) Retrieval-Augmented Generation (RAG) is a t...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             run_id                                       question   template  \\\n",
       "0  773148cfcd0d205f  What is Retrieval-Augmented Generation (RAG)?  qa_strict   \n",
       "1  ed2bea5911a9d13d  What is Retrieval-Augmented Generation (RAG)?  qa_strict   \n",
       "2  223dbe53cb99f7e7  What is Retrieval-Augmented Generation (RAG)?    summary   \n",
       "3  ec596e6f77b05533  What is Retrieval-Augmented Generation (RAG)?    summary   \n",
       "4  83b5662850e2bfeb  What is Retrieval-Augmented Generation (RAG)?  reasoning   \n",
       "\n",
       "       model  temperature  latency_s  prompt_chars  context_chars  n_chunks  \\\n",
       "0  gemma3:4b          0.0      1.294           325              0         0   \n",
       "1  gemma3:4b          0.2      0.496           325              0         0   \n",
       "2  gemma3:4b          0.0      1.585           229              0         0   \n",
       "3  gemma3:4b          0.2      1.440           229              0         0   \n",
       "4  gemma3:4b          0.0      7.530           296              0         0   \n",
       "\n",
       "                                retrieval_query_used  \\\n",
       "0  Explain retrieval augmented generation in simp...   \n",
       "1  Explain retrieval augmented generation in simp...   \n",
       "2  Explain retrieval augmented generation in simp...   \n",
       "3  Explain retrieval augmented generation in simp...   \n",
       "4  Explain retrieval augmented generation in simp...   \n",
       "\n",
       "                                              answer  idk_flag  \\\n",
       "0        I don't know based on the provided context.      True   \n",
       "1        I don't know based on the provided context.      True   \n",
       "2  Please provide the context about RAG/retrieval...     False   \n",
       "3  Please provide the context about RAG/retrieval...     False   \n",
       "4  1) Retrieval-Augmented Generation (RAG) is a t...     False   \n",
       "\n",
       "   has_citation_flag  \n",
       "0              False  \n",
       "1              False  \n",
       "2              False  \n",
       "3              False  \n",
       "4               True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates = [\n",
    "    (\"qa_strict\", QA_STRICT_PROMPT),\n",
    "    (\"summary\", SUMMARY_PROMPT),\n",
    "    (\"reasoning\", REASONING_PROMPT),\n",
    "]\n",
    "\n",
    "temperature_grid = [0.0, 0.2]  # compare deterministic vs slightly creative\n",
    "\n",
    "records = []\n",
    "for q in test_questions:\n",
    "    r = find_retrieval_row(q, retrieval_rows)\n",
    "    for (name, tpl) in templates:\n",
    "        for temp in temperature_grid:\n",
    "            rec = run_generation(\n",
    "                question=q,\n",
    "                retrieval_row=r,\n",
    "                prompt=tpl,\n",
    "                template_name=name,\n",
    "                temperature=temp,\n",
    "                topic=\"RAG / retrieval fundamentals\"\n",
    "            )\n",
    "            records.append(rec)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f233eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/tkhamidulin/Desktop/First Project - RAG/artifacts/week3_prompt_experiments.csv\n",
      "Saved: /Users/tkhamidulin/Desktop/First Project - RAG/artifacts/week3_generations.jsonl\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(W3_EXPERIMENTS_CSV, index=False)\n",
    "\n",
    "with open(W3_GENERATIONS_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in records:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", W3_EXPERIMENTS_CSV)\n",
    "print(\"Saved:\", W3_GENERATIONS_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d96db46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>template</th>\n",
       "      <th>temperature</th>\n",
       "      <th>n</th>\n",
       "      <th>avg_latency_s</th>\n",
       "      <th>avg_answer_len</th>\n",
       "      <th>idk_rate</th>\n",
       "      <th>citation_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qa_strict</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6082</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa_strict</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4452</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.9418</td>\n",
       "      <td>1210.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>5.5042</td>\n",
       "      <td>1152.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.5362</td>\n",
       "      <td>267.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summary</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.4084</td>\n",
       "      <td>267.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    template  temperature  n  avg_latency_s  avg_answer_len  idk_rate  \\\n",
       "0  qa_strict          0.0  5         0.6082            43.0       1.0   \n",
       "1  qa_strict          0.2  5         0.4452            43.0       1.0   \n",
       "2  reasoning          0.0  5         5.9418          1210.2       0.0   \n",
       "3  reasoning          0.2  5         5.5042          1152.6       0.0   \n",
       "4    summary          0.0  5         1.5362           267.0       0.0   \n",
       "5    summary          0.2  5         1.4084           267.0       0.0   \n",
       "\n",
       "   citation_rate  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.8  \n",
       "3            0.8  \n",
       "4            0.0  \n",
       "5            0.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"answer_len\"] = df[\"answer\"].fillna(\"\").str.len()\n",
    "\n",
    "summary = (\n",
    "    df.groupby([\"template\", \"temperature\"])\n",
    "      .agg(\n",
    "          n=(\"run_id\", \"count\"),\n",
    "          avg_latency_s=(\"latency_s\", \"mean\"),\n",
    "          avg_answer_len=(\"answer_len\", \"mean\"),\n",
    "          idk_rate=(\"idk_flag\", \"mean\"),\n",
    "          citation_rate=(\"has_citation_flag\", \"mean\"),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values([\"template\", \"temperature\"])\n",
    ")\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a771e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>template</th>\n",
       "      <th>temperature</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qa_strict</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I don't know based on the provided context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa_strict</td>\n",
       "      <td>0.2</td>\n",
       "      <td>I don't know based on the provided context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Please provide the context about RAG/retrieval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summary</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Please provide the context about RAG/retrieval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1) Retrieval-Augmented Generation (RAG) is a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reasoning</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1) Retrieval-Augmented Generation (RAG) is a t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    template  temperature                                             answer\n",
       "0  qa_strict          0.0        I don't know based on the provided context.\n",
       "1  qa_strict          0.2        I don't know based on the provided context.\n",
       "2    summary          0.0  Please provide the context about RAG/retrieval...\n",
       "3    summary          0.2  Please provide the context about RAG/retrieval...\n",
       "4  reasoning          0.0  1) Retrieval-Augmented Generation (RAG) is a t...\n",
       "5  reasoning          0.2  1) Retrieval-Augmented Generation (RAG) is a t..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = test_questions[0]\n",
    "df[df[\"question\"] == q][[\"template\", \"temperature\", \"answer\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d483b19",
   "metadata": {},
   "source": [
    "## Conclusions (Week 3)\n",
    "\n",
    "**What changed when I changed the prompt:**\n",
    "- **Strict Q&A**: tends to be the most grounded and follows refusal rules more consistently.\n",
    "- **Summary**: produces the best structure/readability, but can omit details needed for precise Q&A.\n",
    "- **Grounded reasoning**: improves explanations, but is more sensitive to weak context and can drift if constraints are not strict enough.\n",
    "\n",
    "**Key takeaway:** prompt structure strongly controls:\n",
    "- refusal behavior (“I don’t know…”)\n",
    "- citation usage\n",
    "- verbosity vs conciseness\n",
    "- risk of hallucination (especially in reasoning-style prompts)\n",
    "\n",
    "**Next (Week 4):** implement guardrails and fallback logic when retrieval context is weak or irrelevant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4718d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/tkhamidulin/Desktop/First Project - RAG/artifacts/week3_summary.md\n"
     ]
    }
   ],
   "source": [
    "summary_text = f\"\"\"# Week 3 — Prompt Engineering Summary\n",
    "\n",
    "## Setup\n",
    "- LLM: Ollama via LangChain\n",
    "- Model: {MODEL_NAME}\n",
    "- Templates: qa_strict, summary, reasoning\n",
    "- Temperatures tested: {temperature_grid}\n",
    "\n",
    "## Quick metrics (proxy)\n",
    "{summary.to_string(index=False)}\n",
    "\n",
    "## Notes (fill after manual review)\n",
    "- qa_strict:\n",
    "- summary:\n",
    "- reasoning:\n",
    "\n",
    "## Observed failure modes\n",
    "- \n",
    "\n",
    "## Week 4 plan\n",
    "- add guardrails for weak retrieval\n",
    "- enforce minimum-evidence rules before answering\n",
    "- improve refusal behavior and citation enforcement\n",
    "\"\"\"\n",
    "\n",
    "W3_SUMMARY_MD.write_text(summary_text, encoding=\"utf-8\")\n",
    "print(\"Saved:\", W3_SUMMARY_MD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a363a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
