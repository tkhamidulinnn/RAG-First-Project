{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Week 2: Building the Local Retrieval Pipeline (FAISS)\n",
    "\n",
    "**Scope:** Build a complete retrieval pipeline using real PDF documents: loading → chunking → embeddings → FAISS index → retrieval → evaluation.\n",
    "\n",
    "**Data:** PDF documents from `data/` folder (RAG, GIT, GCP topics)\n",
    "\n",
    "**Models:** `all-MiniLM-L6-v2` (384d), `all-mpnet-base-v2` (768d)\n",
    "\n",
    "**Metrics:** Hit@K, Mean Reciprocal Rank (MRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS configured: Using CPU mode\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "faiss.omp_set_num_threads(4)\n",
    "print(\"FAISS configured: Using CPU mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## How FAISS Works\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) enables fast similarity search:\n",
    "\n",
    "1. **Indexing**: Store embeddings in an optimized data structure\n",
    "2. **Search**: Find K nearest neighbors to a query embedding\n",
    "3. **Scoring**: Return similarity scores for ranking\n",
    "\n",
    "### Index Types\n",
    "\n",
    "| Index | Description | Use Case |\n",
    "|-------|-------------|----------|\n",
    "| **IndexFlatIP** | Exact inner product search | Small datasets, highest accuracy |\n",
    "| **IndexFlatL2** | Exact L2 distance search | When using non-normalized embeddings |\n",
    "| **IndexIVF** | Approximate search with clustering | Large datasets |\n",
    "\n",
    "### Why IndexFlatIP with Normalized Embeddings?\n",
    "\n",
    "- **Normalized embeddings** have magnitude = 1.0\n",
    "- **Inner product = cosine similarity** for normalized vectors\n",
    "- Faster than computing cosine directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/tkhamidulin/Desktop/First Project - RAG\n",
      "Data dir: /Users/tkhamidulin/Desktop/First Project - RAG/data (exists: True)\n",
      "Artifacts dir: /Users/tkhamidulin/Desktop/First Project - RAG/artifacts\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data dir: {DATA_DIR} (exists: {DATA_DIR.exists()})\")\n",
    "print(f\"Artifacts dir: {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1: Load PDF Documents\n",
    "\n",
    "**Goal:** Load real PDF documents from the `data/` folder.\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "data/\n",
    "├── RAG/     (3 PDFs about RAG, HyDE, LangChain)\n",
    "├── GIT/     (3 PDFs about Git)\n",
    "└── GCP/     (3 PDFs about Google Cloud)\n",
    "```\n",
    "\n",
    "Each subfolder name becomes the **topic** for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GCP]\n",
      "  + gcloud-cheat-sheet.pdf (2 pages)\n",
      "  + google_security_wp.pdf (18 pages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 11 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  + A-Complete-Guide-to-the-Google-Cloud-Platform.pdf (53 pages)\n",
      "\n",
      "[GIT]\n",
      "  + GitGuide.pdf (8 pages)\n",
      "  + git-cheat-sheet-education.pdf (2 pages)\n",
      "  + How_to_Git.pdf (45 pages)\n",
      "\n",
      "[RAG]\n",
      "  + Advanced RAG — Improving retrieval using Hypothetical Document Embeddings(HyDE) _ by Plaban Nayak _ AI Planet.pdf (19 pages)\n",
      "  + Retrieval - Docs by LangChain.pdf (3 pages)\n",
      "  + Retrieval-Augmented Generation (RAG) _ Pinecone.pdf (4 pages)\n",
      "\n",
      "Loaded: 154 pages from 3 topics: ['GCP', 'GIT', 'RAG']\n"
     ]
    }
   ],
   "source": [
    "def load_pdfs(data_dir: Path) -> list:\n",
    "    \"\"\"\n",
    "    Load all PDFs from topic subfolders.\n",
    "    Adds metadata: topic (folder name), source (filename), page.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    topics_found = []\n",
    "    \n",
    "    for topic_dir in sorted(data_dir.iterdir()):\n",
    "        if not topic_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        topic = topic_dir.name\n",
    "        pdf_files = list(topic_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        if not pdf_files:\n",
    "            print(f\"  WARNING: No PDFs in {topic}/\")\n",
    "            continue\n",
    "        \n",
    "        topics_found.append(topic)\n",
    "        print(f\"\\n[{topic}]\")\n",
    "        \n",
    "        for pdf_path in pdf_files:\n",
    "            try:\n",
    "                loader = PyPDFLoader(str(pdf_path))\n",
    "                pdf_docs = loader.load()\n",
    "                for doc in pdf_docs:\n",
    "                    doc.metadata[\"topic\"] = topic\n",
    "                    doc.metadata[\"source\"] = pdf_path.name\n",
    "                    docs.append(doc)\n",
    "                print(f\"  + {pdf_path.name} ({len(pdf_docs)} pages)\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: {pdf_path.name} - {e}\")\n",
    "    \n",
    "    print(f\"\\nLoaded: {len(docs)} pages from {len(topics_found)} topics: {topics_found}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "# Load all PDFs\n",
    "raw_docs = load_pdfs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document:\n",
      "  Topic: GCP\n",
      "  Source: gcloud-cheat-sheet.pdf\n",
      "  Page: 0\n",
      "  Content preview: gcloud init\n",
      "I n i t i a l i z e ,  a u t h o r i z e ,  a n d  c o n fi g u r e  g c l o u d\n",
      "gcloud version\n",
      "D i s p l a y  ve r s i o n  a n d  i n s t a l l e d  c o m p o n e n t s\n",
      "gcloud components...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Sample document:\")\n",
    "print(f\"  Topic: {raw_docs[0].metadata['topic']}\")\n",
    "print(f\"  Source: {raw_docs[0].metadata['source']}\")\n",
    "print(f\"  Page: {raw_docs[0].metadata.get('page', 0)}\")\n",
    "print(f\"  Content preview: {raw_docs[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2: Implement Chunking with ONE Configuration\n",
    "\n",
    "**Goal:** Split documents into chunks using LangChain's RecursiveCharacterTextSplitter.\n",
    "\n",
    "**Why RecursiveCharacterTextSplitter?**\n",
    "- Tries to split on natural boundaries (paragraphs, sentences)\n",
    "- Falls back to character-level splits if needed\n",
    "- Preserves semantic coherence better than fixed-size splits\n",
    "\n",
    "**Parameters:**\n",
    "- `chunk_size`: Maximum characters per chunk\n",
    "- `chunk_overlap`: Characters shared between consecutive chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original documents: 154 pages\n",
      "After chunking: 663 chunks\n",
      "Chunk size: 500, Overlap: 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "\n",
    "print(f\"Original documents: {len(raw_docs)} pages\")\n",
    "print(f\"After chunking: {len(chunks)} chunks\")\n",
    "print(f\"Chunk size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks per topic:\n",
      "  GCP: 299 chunks\n",
      "  GIT: 91 chunks\n",
      "  RAG: 273 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topic_counts = {}\n",
    "for c in chunks:\n",
    "    topic = c.metadata[\"topic\"]\n",
    "    topic_counts[topic] = topic_counts.get(topic, 0) + 1\n",
    "\n",
    "print(\"Chunks per topic:\")\n",
    "for topic, count in sorted(topic_counts.items()):\n",
    "    print(f\"  {topic}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunk:\n",
      "  Topic: RAG\n",
      "  Source: Advanced RAG — Improving retrieval using Hypothetical Document Embeddings(HyDE) _ by Plaban Nayak _ AI Planet.pdf\n",
      "  Length: 471 chars\n",
      "  Content: significant influence on core managerial activities, notably each of the element\n",
      "organizational design.  In addition, of course, management must remain focused o\n",
      "creating value for customers in an efficient manner and on de veloping and  impl\n",
      "strategies to attract necessary inputs such as capital, t...\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample chunk\n",
    "sample_chunk = chunks[501]\n",
    "print(\"Sample chunk:\")\n",
    "print(f\"  Topic: {sample_chunk.metadata['topic']}\")\n",
    "print(f\"  Source: {sample_chunk.metadata['source']}\")\n",
    "print(f\"  Length: {len(sample_chunk.page_content)} chars\")\n",
    "print(f\"  Content: {sample_chunk.page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3: Build FAISS Index with ONE Model\n",
    "\n",
    "**Goal:** Create a working retrieval pipeline before comparing models.\n",
    "\n",
    "**Steps:**\n",
    "1. Extract text from chunks\n",
    "2. Generate embeddings with SentenceTransformer\n",
    "3. Build FAISS IndexFlatIP\n",
    "4. Test retrieval with a simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee9b99468e2462984bd453d085b6d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8447185b1acc4afe9f59fc31c690f6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embeddings shape: (663, 384)\n",
      "Embedding dimension: 384\n",
      "L2 norm of first embedding: 1.0000 (should be 1.0)\n"
     ]
    }
   ],
   "source": [
    "# Load ONE model first\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Extract texts\n",
    "texts = [c.page_content for c in chunks]\n",
    "topics = [c.metadata[\"topic\"] for c in chunks]\n",
    "\n",
    "# Generate embeddings (normalized for cosine similarity)\n",
    "# FIX: Add validation before encoding\n",
    "if not texts or len(texts) == 0:\n",
    "    raise ValueError(\"Cannot generate embeddings: texts list is empty\")\n",
    "\n",
    "embeddings = model.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
    "# FIX: Ensure proper data type and contiguity (required by FAISS)\n",
    "embeddings = np.ascontiguousarray(embeddings, dtype=np.float32)\n",
    "\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "if len(embeddings) > 0:\n",
    "    print(f\"L2 norm of first embedding: {np.linalg.norm(embeddings[0]):.4f} (should be 1.0)\")\n",
    "else:\n",
    "    print(\"WARNING: No embeddings generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built\n",
      "  Index type: IndexFlatIP\n",
      "  Dimension: 384\n",
      "  Total vectors: 663\n"
     ]
    }
   ],
   "source": [
    "# FIX: Add validation before creating index\n",
    "if len(embeddings) == 0:\n",
    "    raise ValueError(\"Cannot create index: embeddings array is empty\")\n",
    "\n",
    "if embeddings.shape[1] == 0:\n",
    "    raise ValueError(\"Cannot create index: embedding dimension is 0\")\n",
    "\n",
    "# Ensure embeddings are float32 and contiguous (required by FAISS)\n",
    "embeddings = np.ascontiguousarray(embeddings, dtype=np.float32)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product = Cosine for normalized vectors\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"FAISS index built\")\n",
    "print(f\"  Index type: IndexFlatIP\")\n",
    "print(f\"  Dimension: {dimension}\")\n",
    "print(f\"  Total vectors: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What is RAG?'\n",
      "\n",
      "Top 3 results:\n",
      "\n",
      "1. [RAG] score=0.6102\n",
      "   Source: Retrieval - Docs by LangChain.pdf\n",
      "   Building blocks\n",
      "RAG architectures\n",
      "RAG can be implemented in multiple ways, depending on your systemʼs needs. We\n",
      "outline each type in the sections belo...\n",
      "\n",
      "2. [RAG] score=0.5627\n",
      "   Source: Retrieval-Augmented Generation (RAG) _ Pinecone.pdf\n",
      "   Traditional RAG\n",
      "By combining relevant data from an external data source with the user’s query and providing it to the\n",
      "model as context for the generat...\n",
      "\n",
      "3. [RAG] score=0.5067\n",
      "   Source: Advanced RAG — Improving retrieval using Hypothetical Document Embeddings(HyDE) _ by Plaban Nayak _ AI Planet.pdf\n",
      "   Mar 13, 2025\n",
      "In by\n",
      "Implement RAG with Knowledge\n",
      "Graph and Llama-Index\n",
      "Introduction\n",
      "Dec 3, 2023\n",
      "In by\n",
      "Semantic Chunking for RAG\n",
      "What is Chunking ?\n",
      "Apr ...\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval with ONE query\n",
    "query = \"What is RAG?\"\n",
    "query_embedding = model.encode([query], normalize_embeddings=True)\n",
    "query_embedding = np.array(query_embedding, dtype=\"float32\")\n",
    "\n",
    "# Search top 3\n",
    "k = 3\n",
    "scores, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"\\nTop {k} results:\")\n",
    "for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):\n",
    "    chunk = chunks[idx]\n",
    "    print(f\"\\n{rank}. [{chunk.metadata['topic']}] score={score:.4f}\")\n",
    "    print(f\"   Source: {chunk.metadata['source']}\")\n",
    "    print(f\"   {chunk.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ad0cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What is GIT?'\n",
      "\n",
      "Top 3 results:\n",
      "\n",
      "1. [GIT] score=0.7412\n",
      "   Source: GitGuide.pdf\n",
      "   Beginner’s   Guide   to   using   Git  \n",
      "Written   By:   Kyle   Swygert  \n",
      "Voiland   College   Tutoring   Services  \n",
      "WSU   CptS   223   and   onward   \n",
      "...\n",
      "\n",
      "2. [GIT] score=0.7135\n",
      "   Source: How_to_Git.pdf\n",
      "   Version control tool that tracks file change history (like track\n",
      "changes for word but much more sophisticated)\n",
      "Popular among software developers\n",
      "GitHu...\n",
      "\n",
      "3. [GIT] score=0.6573\n",
      "   Source: How_to_Git.pdf\n",
      "   Introduction to Git\n",
      "Basic Git commands\n",
      "Collaboration with Git and GitHub\n",
      "3\n",
      "Objectives...\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval with ONE query\n",
    "query = \"What is GIT?\"\n",
    "query_embedding = model.encode([query], normalize_embeddings=True)\n",
    "query_embedding = np.array(query_embedding, dtype=\"float32\")\n",
    "\n",
    "# Search top 3\n",
    "k = 3\n",
    "scores, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"\\nTop {k} results:\")\n",
    "for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):\n",
    "    chunk = chunks[idx]\n",
    "    print(f\"\\n{rank}. [{chunk.metadata['topic']}] score={score:.4f}\")\n",
    "    print(f\"   Source: {chunk.metadata['source']}\")\n",
    "    print(f\"   {chunk.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07640fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What is GCP?'\n",
      "\n",
      "Top 3 results:\n",
      "\n",
      "1. [GCP] score=0.6503\n",
      "   Source: A-Complete-Guide-to-the-Google-Cloud-Platform.pdf\n",
      "   enterprise network. But since the cloud \n",
      "operates web scale its failures often \n",
      "get ampliﬁed. \n",
      "Comparing the \n",
      "Networking Services of \n",
      "Google and Amazo...\n",
      "\n",
      "2. [GCP] score=0.5649\n",
      "   Source: A-Complete-Guide-to-the-Google-Cloud-Platform.pdf\n",
      "   IV. BONUS! Live Migration in\n",
      "GCP\n",
      "Another advantage of working with \n",
      "GCP networking is the availability of \n",
      "live migration. Hardware failures \n",
      "happen i...\n",
      "\n",
      "3. [GCP] score=0.5511\n",
      "   Source: A-Complete-Guide-to-the-Google-Cloud-Platform.pdf\n",
      "   based on Google’s Andromeda. This \n",
      "architecture allows the creation of \n",
      "networking elements at any level and \n",
      "supports customization of the network\n",
      "to...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"What is GCP?\"\n",
    "query_embedding = model.encode([query], normalize_embeddings=True)\n",
    "query_embedding = np.array(query_embedding, dtype=\"float32\")\n",
    "\n",
    "# Search top 3\n",
    "k = 3\n",
    "scores, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"\\nTop {k} results:\")\n",
    "for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):\n",
    "    chunk = chunks[idx]\n",
    "    print(f\"\\n{rank}. [{chunk.metadata['topic']}] score={score:.4f}\")\n",
    "    print(f\"   Source: {chunk.metadata['source']}\")\n",
    "    print(f\"   {chunk.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 4: Compare Chunk Sizes\n",
    "\n",
    "**Goal:** Understand how chunk size affects retrieval quality.\n",
    "\n",
    "**Configurations:**\n",
    "- Small: 300 chars, 30 overlap → more precise, less context\n",
    "- Medium: 500 chars, 50 overlap → balanced\n",
    "- Large: 800 chars, 80 overlap → more context, may include noise\n",
    "\n",
    "**Expected behavior:**\n",
    "- Small chunks → more chunks, precise matches\n",
    "- Large chunks → fewer chunks, broader context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size comparison:\n",
      "==================================================\n",
      "\n",
      "Chunk size: 300, Overlap: 30\n",
      "  Total chunks: 1097\n",
      "  Avg chunk length: 248 chars\n",
      "\n",
      "Chunk size: 500, Overlap: 50\n",
      "  Total chunks: 662\n",
      "  Avg chunk length: 418 chars\n",
      "\n",
      "Chunk size: 800, Overlap: 80\n",
      "  Total chunks: 435\n",
      "  Avg chunk length: 640 chars\n"
     ]
    }
   ],
   "source": [
    "chunk_configs = [\n",
    "    {\"chunk_size\": 300, \"overlap\": 30},\n",
    "    {\"chunk_size\": 500, \"overlap\": 50},\n",
    "    {\"chunk_size\": 800, \"overlap\": 80},\n",
    "]\n",
    "\n",
    "print(\"Chunk size comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for cfg in chunk_configs:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=cfg[\"chunk_size\"],\n",
    "        chunk_overlap=cfg[\"overlap\"],\n",
    "    )\n",
    "    chunks_temp = splitter.split_documents(raw_docs)\n",
    "    \n",
    "    avg_len = np.mean([len(c.page_content) for c in chunks_temp])\n",
    "    \n",
    "    print(f\"\\nChunk size: {cfg['chunk_size']}, Overlap: {cfg['overlap']}\")\n",
    "    print(f\"  Total chunks: {len(chunks_temp)}\")\n",
    "    print(f\"  Avg chunk length: {avg_len:.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 5: Compare Embedding Models\n",
    "\n",
    "**Goal:** Compare MiniLM vs MPNet on the same retrieval task.\n",
    "\n",
    "| Model | Dimensions | Speed | Quality |\n",
    "|-------|------------|-------|--------|\n",
    "| all-MiniLM-L6-v2 | 384 | Fast | Good |\n",
    "| all-mpnet-base-v2 | 768 | Slower | Better |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How to create a git branch?'\n",
      "==================================================\n",
      "\n",
      "NOTE: Models are tested in separate cells to prevent memory overload.\n",
      "Run the cells below one at a time.\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"MiniLM\": \"all-MiniLM-L6-v2\",\n",
    "    \"MPNet\": \"all-mpnet-base-v2\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_index_with_model(texts: list[str], model: SentenceTransformer):\n",
    "    \"\"\"Build FAISS index with pre-loaded model (prevents memory issues).\"\"\"\n",
    "    # FIX: Add validation to prevent crashes\n",
    "    if not texts or len(texts) == 0:\n",
    "        raise ValueError(f\"Cannot build index: texts list is empty\")\n",
    "    \n",
    "    # FIX: Use batch processing for large datasets to prevent memory issues\n",
    "    batch_size = 32  # Process in smaller batches\n",
    "    embeddings_list = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        emb_batch = model.encode(batch, normalize_embeddings=True, show_progress_bar=False)\n",
    "        embeddings_list.append(emb_batch)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    emb = np.vstack(embeddings_list)\n",
    "    \n",
    "    # FIX: Ensure proper data type and contiguity (required by FAISS)\n",
    "    emb = np.ascontiguousarray(emb, dtype=np.float32)\n",
    "    \n",
    "    if emb.shape[0] == 0 or emb.shape[1] == 0:\n",
    "        raise ValueError(f\"Cannot build index: embeddings have invalid shape {emb.shape}\")\n",
    "    \n",
    "    index = faiss.IndexFlatIP(emb.shape[1])\n",
    "    index.add(emb)\n",
    "    return index\n",
    "\n",
    "\n",
    "def build_index(texts: list[str], model_name: str):\n",
    "    \"\"\"\n",
    "    Build FAISS index with given model name (backward compatibility).\n",
    "    \n",
    "    FIX: This function loads the model each time, which can cause memory issues.\n",
    "    For better performance, use build_index_with_model() with a pre-loaded model.\n",
    "    \"\"\"\n",
    "    # FIX: Add validation to prevent crashes\n",
    "    if not texts or len(texts) == 0:\n",
    "        raise ValueError(f\"Cannot build index: texts list is empty\")\n",
    "    \n",
    "    # Load model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # FIX: Use batch processing for large datasets to prevent memory issues\n",
    "    batch_size = 32\n",
    "    embeddings_list = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        emb_batch = model.encode(batch, normalize_embeddings=True, show_progress_bar=False)\n",
    "        embeddings_list.append(emb_batch)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    emb = np.vstack(embeddings_list)\n",
    "    \n",
    "    # FIX: Ensure proper data type and contiguity (required by FAISS)\n",
    "    emb = np.ascontiguousarray(emb, dtype=np.float32)\n",
    "    \n",
    "    if emb.shape[0] == 0 or emb.shape[1] == 0:\n",
    "        raise ValueError(f\"Cannot build index: embeddings have invalid shape {emb.shape}\")\n",
    "    \n",
    "    index = faiss.IndexFlatIP(emb.shape[1])\n",
    "    index.add(emb)\n",
    "    return model, index\n",
    "\n",
    "\n",
    "def retrieve(model, index, query: str, top_k: int = 3):\n",
    "    \"\"\"Retrieve top-k results for a query.\"\"\"\n",
    "    # FIX: Add validation\n",
    "    if not query or len(query.strip()) == 0:\n",
    "        raise ValueError(\"Query cannot be empty\")\n",
    "    \n",
    "    if index.ntotal == 0:\n",
    "        raise ValueError(\"Index is empty, cannot retrieve\")\n",
    "    \n",
    "    q = model.encode([query], normalize_embeddings=True)\n",
    "    q = np.ascontiguousarray(q, dtype=np.float32)\n",
    "    \n",
    "    # FIX: Ensure top_k doesn't exceed available vectors\n",
    "    actual_k = min(top_k, index.ntotal)\n",
    "    scores, indices = index.search(q, actual_k)\n",
    "    return scores[0], indices[0]\n",
    "\n",
    "\n",
    "# Test query for model comparison\n",
    "query = \"How to create a git branch?\"\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nNOTE: Models are tested in separate cells to prevent memory overload.\")\n",
    "print(\"Run the cells below one at a time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4815ab7",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Comparison: MiniLM\n",
    "\n",
    "**Goal:** Test MiniLM model (384d embeddings) on the query.\n",
    "\n",
    "**Why separate cells?** Each model is tested in its own cell to prevent memory overload and crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e47793c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MiniLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3546d2b52f0a47c59e59521070df69f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MiniLM loaded successfully\n",
      "\n",
      "Building index...\n",
      "\n",
      "✗ ERROR with MiniLM: name 'build_index_with_model' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/z8/cqzr9q312nzfqkq1hrzrwqnr0000gp/T/ipykernel_28878/4041347915.py\", line 11, in <module>\n",
      "    idx = build_index_with_model(texts, model)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'build_index_with_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Test MiniLM model\n",
    "model_name = \"MiniLM\"\n",
    "model_id = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "try:\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    model = SentenceTransformer(model_id)\n",
    "    print(f\"✓ {model_name} loaded successfully\\n\")\n",
    "    \n",
    "    print(\"Building index...\")\n",
    "    idx = build_index_with_model(texts, model)\n",
    "    print(f\"✓ Index built: {idx.ntotal} vectors\\n\")\n",
    "    \n",
    "    print(\"Retrieving...\")\n",
    "    scores, indices = retrieve(model, idx, query)\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for rank, (score, i) in enumerate(zip(scores, indices), 1):\n",
    "        print(f\"{rank}. [{topics[i]}] score={score:.4f} - {chunks[i].metadata['source'][:30]}\")\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del idx\n",
    "    del model\n",
    "    print(f\"\\n✓ {model_name} completed and memory freed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ ERROR with {model_name}: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7cdcf3",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Comparison: MPNet\n",
    "\n",
    "**Goal:** Test MPNet model (768d embeddings) on the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99f5f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MPNet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f72c2096884f6fb345783d6a8c6f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MPNetModel LOAD REPORT from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test MPNet model\n",
    "model_name = \"MPNet\"\n",
    "model_id = \"all-mpnet-base-v2\"\n",
    "\n",
    "try:\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    model = SentenceTransformer(model_id)\n",
    "    print(f\"✓ {model_name} loaded successfully\\n\")\n",
    "    \n",
    "    print(\"Building index...\")\n",
    "    idx = build_index_with_model(texts, model)\n",
    "    print(f\"✓ Index built: {idx.ntotal} vectors\\n\")\n",
    "    \n",
    "    print(\"Retrieving...\")\n",
    "    scores, indices = retrieve(model, idx, query)\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for rank, (score, i) in enumerate(zip(scores, indices), 1):\n",
    "        print(f\"{rank}. [{topics[i]}] score={score:.4f} - {chunks[i].metadata['source'][:30]}\")\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del idx\n",
    "    del model\n",
    "    print(f\"\\n✓ {model_name} completed and memory freed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ ERROR with {model_name}: {str(e)}\")\n",
    "    print(\"This may indicate insufficient memory. Try:\")\n",
    "    print(\"  1. Restart kernel and run only this cell\")\n",
    "    print(\"  2. Close other applications to free memory\")\n",
    "    print(\"  3. Use MiniLM instead (smaller model)\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face1ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
